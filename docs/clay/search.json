[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fastmath documentation",
    "section": "",
    "text": "Preface\nDocumentation work in progress\nStatuses:\n✓ - done\n+ - partially done\n⇾ - wip\n. - awaiting\n\n\n\n\n\nnamespace\nclay docs\ndocstrings\nincluded in book\nnotes\n\n\n\n\nfastmath.core\n✓\n✓\n✓\n\n\n\nfastmath.vector\n.\n.\n✓\n\n\n\nfastmath.matrix\n.\n.\n✓\n\n\n\nfastmath.random\n+\n.\n✓\nnew functions added\n\n\nfastmath.stats\n✓\n✓\n✓\nexperimental, LLM based\n\n\nfastmath.stats.bootstrap\n✓\n✓\n✓\nexperimental, LLM based\n\n\nfastmath.polynomials\n.\n.\n✓\n\n\n\nfastmath.special\n✓\n.\n✓\n\n\n\nfastmath.calculus\n+\n.\n✓\nClerk version exists\n\n\nfastmath.solver\n.\n.\n+\n\n\n\nfastmath.interpolation\n⇾\n.\n✓\n\n\n\nfastmath.kernel\n.\n.\n+\n\n\n\nfastmath.optimization\n.\n.\n✓\n\n\n\nfastmath.transform\n⇾\n.\n✓\n\n\n\nfastmath.signal\n.\n.\n.\nrefactor required\n\n\nfastmath.ml.regression\n.\n.\n✓\n\n\n\nfastmath.ml.clustering\n.\n.\n✓\n\n\n\nfastmath.complex\n✓\n✓\n✓\n\n\n\nfastmath.quaternions\n✓\n✓\n✓\n\n\n\nfastmath.distance\n.\n.\n.\n\n\n\nfastmath.easings\n.\n.\n✓\n\n\n\nfastmath.grid\n.\n.\n.\n\n\n\nfastmath.fields\n.\n.\n✓\n\n\n\nfastmath.curves\n.\n.\n.\n\n\n\nfastmath.efloat\n.\n.\n✓\n\n\n\n\n\n\n\nsource: clay/index.clj",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core",
    "section": "",
    "text": "Basic operations\nCollection of type hinted math macros and functions. Partially backed by Java static functions and exposed as macros. They are prepared to accept primitive long or double arguments and return long or double only.\nThere is a possibility to replace clojure.core functions with a selection of fastmath.core macros. Call:\nBe aware that there are some differences and fastmath.core versions shoudn’t be treated as a drop-in replacement for clojure.core versions. Also, since Clojure 1.12, always call unuse-primitive-operators at the end of the namespace.\nHere is the complete list of replaced functions:\nBasic math operations.\nWhen used in an expression oprations are inlined and can accept mixture of long and double values. If all values are of long primitive type, long is returned, double otherwise.\nWhen used in higher order function, double is returned always. To operate on long primitive type, reach for long- versions.",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#basic-operations",
    "href": "core.html#basic-operations",
    "title": "Core",
    "section": "",
    "text": "Defined functions\n\n\n\n\n+, -, *, /, quot\ninc, dec\nmin, max, smooth-max, constrain\nrem, mod, remainder, wrap\nabs\n\nlong versions\n\nlong-add, long-sub, long-mult, long-div, long-quot\nlong-inc, long-dec\nlong-min, long-max\nlong-rem, long-mod\nlong-abs\n\n\n\n\nArithmetics\n\naddition, incrementation\nsubtraction, decrementation\nmultiplication\ndivision\nabsolute value\n\n\n\n\n\n\n\nDivision differences\n\n\n\nPlease note that there some differences between division in fastmath and clojure.core\n\nwhen called with one argument (double or long) m// always returns reciprocal (clojure.core// returns a ratio)\nwhen called on long arguments, m// is a long division (clojure.core// returns a ratio)\nm// for two long arguments is equivalent to m/quot\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\nAddition\n\n(m/+) ;; =&gt; 0.0\n(m/+ 1 2 3 4) ;; =&gt; 10\n(m/+ 1.0 2.5 3 4) ;; =&gt; 10.5\n(reduce m/+ [1 2 3]) ;; =&gt; 6.0\n\n\n(m/long-add) ;; =&gt; 0\n(m/long-add 1 2 3 4) ;; =&gt; 10\n(m/long-add 1.0 2.5 3 4) ;; =&gt; 10\n(reduce m/long-add [1 2 3.5]) ;; =&gt; 6\n\nSubtraction\n\n[(m/- 1) (m/- 1.0)] ;; =&gt; [-1 -1.0]\n(m/- 1 2 3 4) ;; =&gt; -8\n(m/- 1.0 2.5 3 4) ;; =&gt; -8.5\n(reduce m/- [1 2 3]) ;; =&gt; -4.0\n\n\n(m/long-sub 1) ;; =&gt; -1\n(m/long-sub 1 2 3 4) ;; =&gt; -8\n(m/long-sub 1.0 2.5 3 4) ;; =&gt; -8\n(reduce m/long-sub [1 2 3.5]) ;; =&gt; -4\n\nMultiplication\n\n(m/*) ;; =&gt; 1.0\n(m/* 1 2 3 4) ;; =&gt; 24\n(m/* 1.0 2.5 3 4) ;; =&gt; 30.0\n(reduce m/* [1 2 3]) ;; =&gt; 6.0\n\n\n(m/long-mult) ;; =&gt; 1\n(m/long-mult 1 2 3 4) ;; =&gt; 24\n(m/long-mult 1.0 2.5 3 4) ;; =&gt; 24\n(reduce m/long-mult [1 2 3.5]) ;; =&gt; 6\n\nDivision\n\n[(m// 2) (m// 2) (/ 2)] ;; =&gt; [0.5 0.5 1/2]\n(m// 1 2 3 4) ;; =&gt; 0\n(m// 1.0 2.5 3 4) ;; =&gt; 0.03333333333333333\n(reduce m// [1 2 3]) ;; =&gt; 0.16666666666666666\n(m/quot 10.5 -3) ;; =&gt; -3.0\n\n\n(m/long-div 2) ;; =&gt; 0.5\n(m/long-div 100 5 3) ;; =&gt; 6\n(m/long-div 100.5 2.5 3) ;; =&gt; 16\n(reduce m/long-div [100 2 3.5]) ;; =&gt; 16\n(m/long-quot 10 -3) ;; =&gt; -3\n\nIncrement and decrement\n\n(m/inc 4) ;; =&gt; 5\n(m/inc 4.5) ;; =&gt; 5.5\n(m/dec 4) ;; =&gt; 3\n(m/dec 4.5) ;; =&gt; 3.5\n(map m/inc [1 2 3.5 4.5]) ;; =&gt; (2.0 3.0 4.5 5.5)\n\n\n(m/long-inc 4) ;; =&gt; 5\n(m/long-inc 4.5) ;; =&gt; 5\n(m/long-dec 4) ;; =&gt; 3\n(m/long-dec 4.5) ;; =&gt; 3\n(map m/long-inc [1 2 3.5 4.5]) ;; =&gt; (2 3 4 5)\n\nAbsolute value\n\n(m/abs -3) ;; =&gt; 3\n(m/long-abs -3) ;; =&gt; 3\n(m/abs -3.5) ;; =&gt; 3.5\n(m/long-abs -3.5) ;; =&gt; 3\n\n\n\n\n\nRemainders\n\nrem and mod are the same as in clojure.core,\nremainder returns \\(dividend - divisor * n\\), where \\(n\\) is the mathematical integer closest to \\(\\frac{dividend}{divisor}\\). Returned value is inside the \\([\\frac{-|divisor|}{2},\\frac{|divisor|}{2}]\\) range.\nwrap wraps the value to be within given interval (right open) \\([a,b)\\) `\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/mod 10 4) ;; =&gt; 2\n(m/mod -10.25 4.0) ;; =&gt; 1.75\n(m/mod 10.25 -4.0) ;; =&gt; -1.75\n(m/mod -10.25 -4.0) ;; =&gt; -2.25\n(m/rem 10 4) ;; =&gt; 2\n(m/rem -10.25 4.0) ;; =&gt; -2.25\n(m/rem 10.25 -4.0) ;; =&gt; 2.25\n(m/rem -10.25 -4.0) ;; =&gt; -2.25\n(m/remainder 10 4) ;; =&gt; 2.0\n(m/remainder -10.25 4.0) ;; =&gt; 1.75\n(m/remainder 10.25 -4.0) ;; =&gt; -1.75\n(m/remainder -10.25 -4.0) ;; =&gt; 1.75\n(m/wrap -1.25 1.25 1.0) ;; =&gt; 1.0\n(m/wrap -1.25 1.25 1.35) ;; =&gt; -1.15\n(m/wrap -1.25 1.25 -1.25) ;; =&gt; -1.25\n(m/wrap -1.25 1.25 1.25) ;; =&gt; -1.25\n(m/wrap [-1.25 1.25] -1.35) ;; =&gt; 1.15\n\n\n\n\n\nMin, max, constrain\nConstrain is a macro which is equivalent to (max (min value mx) mn)\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/min 1 2 -3) ;; =&gt; -3\n(m/min 1.0 2 -3) ;; =&gt; -3.0\n(m/max 1 2 -3) ;; =&gt; 2\n(m/max 1.0 2 -3) ;; =&gt; 2.0\n(m/constrain 10 -1 1) ;; =&gt; 1\n(m/constrain -10 -1 1) ;; =&gt; -1\n(m/constrain 0 -1 1) ;; =&gt; 0\n\n\n\n\nSmooth maximum\nSmooth maximum is a family of functions \\(\\max_\\alpha(xs)\\) for which \\(\\lim_{\\alpha\\to\\infty}\\max_\\alpha(xs)=\\max(xs)\\).\nFive types of smooth maximum are defined (see wikipedia for formulas):\n\n:lse - LogSumExp (default)\n:boltzmann - Boltzmann operator, works for small alpha values\n:mellowmax\n:p-norm\n:smu - smooth maximum unit, \\(\\epsilon=\\frac{1}{\\alpha}\\)\n\n:lse, :boltzmann and :mellowmax are also smooth minimum for negative \\(\\alpha\\) values.\nThe following plots show value of the smooth max for different \\(\\alpha\\) and set of the numbers equal to [-3.5 -2 -1 0.1 3 4]. Blue dashed horizontal lines are minimum (-3.5) and maximum values (4.0).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following plots are defined only for positive \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/smooth-max [-3.5 -2 -1 0.1 3 4] 4.0 :lse) ;; =&gt; 4.004537523710555\n(m/smooth-max [-3.5 -2 -1 0.1 3 4] -4.0 :lse) ;; =&gt; -3.500630381944282\n(m/smooth-max [-3.5 -2 -1 0.1 3 4] 4.0 :boltzmann) ;; =&gt; 3.9820131397304284\n(m/smooth-max [-3.5 -2 -1 0.1 3 4] -4.0 :boltzmann) ;; =&gt; -3.496176019710726\n(m/smooth-max [-3.5 -2 -1 0.1 3 4] 4.0 :mellowmax) ;; =&gt; 3.5565976564035413\n(m/smooth-max [-3.5 -2 -1 0.1 3 4] -4.0 :mellowmax) ;; =&gt; -3.0526905146372685\n(m/smooth-max [-3.5 -2 -1 0.1 3 4] 4.0 :p-norm) ;; =&gt; 4.738284340366858\n(m/smooth-max [-3.5 -2 -1 0.1 3 4] 4.0 :smu) ;; =&gt; 4.060190281957045\n\n\n\n\n\n\nfma\nFused multiply-add \\(fma(a,b,c)=a+bc\\) is the operation implemented with better accuracy in Java 9+ and as one instruction (see more here and here). When Java 8 is used fma is replaced with direct a+bc formula.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nfma, muladd, negmuladd\ndifference-of-products, sum-of-products\n\n\n\n\\[\\operatorname{fma}(a,b,c)=\\operatorname{muladd}(a,b,c)=a+bc\\] \\[\\operatorname{negmuladd}(a,b,c)=\\operatorname{fma}(-a,b,c)\\]\ndifference-of-products (dop) and sum-of-products (sop) are using Kahan’s algorithm to avoid catastrophic cancellation.\n\\[\\operatorname{dop}(a,b,c,d)=ab-cd=\\operatorname{fma}(a,b,-cd)+\\operatorname{fma}(-c,d,cd)\\] \\[\\operatorname{sop}(a,b,c,d)=ab+cd=\\operatorname{fma}(a,b,cd)+\\operatorname{fma}(c,d,-cd)\\]\nThe following example shows that \\(x^2-y^2\\) differs from the best floating point approximation which is equal 1.8626451518330422e-9.\n\n(let [x (m/inc (m/pow 2 -29))\n      y (m/inc (m/pow 2 -30))]\n  {:proper-value (m/difference-of-products x x y y)\n   :wrong-value (m/- (m/* x x) (m/* y y))})\n\n\n{:proper-value 1.8626451518330422E-9, :wrong-value 1.862645149230957E-9}\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/fma 3 4 5) ;; =&gt; 17.0\n(m/muladd 3 4 5) ;; =&gt; 17.0\n(m/negmuladd 3 4 5) ;; =&gt; -7.0\n(m/difference-of-products 3 3 4 4) ;; =&gt; -7.0\n(m/sum-of-products 3 3 4 4) ;; =&gt; 25.0",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#rounding",
    "href": "core.html#rounding",
    "title": "Core",
    "section": "Rounding",
    "text": "Rounding\nVarious rounding functions.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nfloor, ceil\nround, round-even, rint, approx, trunc, itrunc\nqfloor, qceil, qround\nfrac, sfrac\nround-up-pow2\n\n\n\n\nfloor, ceil and rint accept additional argument, scale, which allows to round to the nearest multiple of scale.\nround returns long while rint returns double\nround-even performs IEEE / IEC rounding (even-odd or bankers’ rounding)\napprox rounds number to the given number of digits, uses bigdec\ntrunc returns integer part of a number, frac returns fractional part\ntrunc returns double while itrunc returns long\nsfrac keeps sign of the argument\nqfloor, qceil and qround are implemented using casting to long\nround-up-pow2 rounds to the lowest power of 2 greater than an argument, \\(2^{\\left\\lceil{\\log_2{x}}\\right\\rceil}\\), returns long.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(map m/floor [-10.5 10.5]) ;; =&gt; (-11.0 10.0)\n(m/floor 10.5 4.0) ;; =&gt; 8.0\n(map m/ceil [-10.5 10.5]) ;; =&gt; (-10.0 11.0)\n(m/ceil 10.5 4.0) ;; =&gt; 12.0\n(map m/rint [-10.51 -10.5 -10.49 10.49 10.5 10.51]) ;; =&gt; (-11.0 -10.0 -10.0 10.0 10.0 11.0)\n(m/rint 10.5 4.0) ;; =&gt; 12.0\n(m/rint 10.591 0.1) ;; =&gt; 10.600000000000001\n(map m/round [-10.51 -10.5 -10.49 10.49 10.5 10.51]) ;; =&gt; (-11 -10 -10 10 11 11)\n(map m/round-even [-10.51 -10.5 -10.49 10.49 10.5 10.51]) ;; =&gt; (-11 -10 -10 10 10 11)\n(map m/qfloor [-10.5 10.5]) ;; =&gt; (-11 10)\n(map m/qceil [-10.5 10.5]) ;; =&gt; (-10 11)\n(map m/qround [-10.51 -10.5 -10.49 10.49 10.5 10.51]) ;; =&gt; (-11 -11 -10 10 11 11)\n(map m/trunc [-10.591 10.591]) ;; =&gt; (-10.0 10.0)\n(map m/itrunc [-10.591 10.591]) ;; =&gt; (-10 10)\n(m/approx 10.591) ;; =&gt; 10.59\n(m/approx 10.591 1) ;; =&gt; 10.6\n(m/approx 10.591 0) ;; =&gt; 11.0\n(m/approx -10.591) ;; =&gt; -10.59\n(m/approx -10.591 1) ;; =&gt; -10.6\n(m/approx -10.591 0) ;; =&gt; -11.0\n(map m/frac [-10.591 10.591]) ;; =&gt; (0.5909999999999993 0.5909999999999993)\n(map m/sfrac [-10.591 10.591]) ;; =&gt; (-0.5909999999999993 0.5909999999999993)\n(map m/round-up-pow2 (range 10)) ;; =&gt; (0 1 2 4 4 8 8 8 8 16)\n\nThe difference between rint and round. round is bounded by minimum and maximum long values.\n\n(m/rint 1.23456789E30) ;; =&gt; 1.23456789E30\n(m/round 1.23456789E30) ;; =&gt; 9223372036854775807",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#sign",
    "href": "core.html#sign",
    "title": "Core",
    "section": "Sign",
    "text": "Sign\nSign of the number.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nsignum and sgn\ncopy-sign\n\n\n\n\\[\\operatorname{signum}(x)=\\begin{cases}\n-1 & x&lt;0 \\\\\n1 & x&gt;0 \\\\\n0 & x=0\n\\end{cases}\\]\n\\[\\operatorname{sgn}(x)=\\begin{cases}\n-1 & x&lt;0 \\\\\n1 & x\\geq 0\n\\end{cases}\\]\ncopy-sign sets the sign of the second argument to the first. Please note that -0.0 is negative and 0.0 is positive.\n\\[\\operatorname{copy-sign}(x,y)=\\begin{cases}\n|x| & y&gt;0 \\lor y=0.0\\\\\n-|x| & y&lt;0 \\lor y=-0.0\n\\end{cases}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/signum -2.5) ;; =&gt; -1.0\n(m/signum 2.5) ;; =&gt; 1.0\n(m/sgn -2.5) ;; =&gt; -1.0\n(m/sgn 2.5) ;; =&gt; 1.0\n(m/signum 0) ;; =&gt; 0.0\n(m/sgn 0) ;; =&gt; 1.0\n(m/copy-sign 123 -10) ;; =&gt; -123.0\n(m/copy-sign -123 10) ;; =&gt; 123.0\n(m/copy-sign 123 -0.0) ;; =&gt; -123.0\n(m/copy-sign -123 0.0) ;; =&gt; 123.0",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#comparison-and-predicates",
    "href": "core.html#comparison-and-predicates",
    "title": "Core",
    "section": "Comparison and Predicates",
    "text": "Comparison and Predicates\nVarious predicates and comparison functions\n\n\n\n\n\n\nDefined functions\n\n\n\n\n==, eq, not==, &lt;, &gt;, &lt;=, &gt;=\napprox-eq, approx=, delta-eq, delta=\nzero?, negative-zero?, near-zero?, one?\nneg?, pos?, not-neg?, not-pos?\neven?, odd?\ninteger?\nnan?, inf?, pos-inf?, neg-inf?, invalid-double?, valid-double?\nbetween?, between-?\n\n\n\n\nComparison\nComparison functions operate on primitive values and can handle multiple arguments, chaining the comparison (e.g., (m/&lt; 1 2 3) is equivalent to (and (m/&lt; 1 2) (m/&lt; 2 3))).\nStandard comparisons:\n\n==, eq: Primitive equality. Note that (m/== 1.0 1) is true. Multi-arity checks if the first argument is equal to all subsequent arguments.\nnot==: Primitive inequality. Multi-arity checks if all arguments are pairwise unique.\n&lt;, &gt;, &lt;=, &gt;=: Standard primitive inequalities. Multi-arity checks if the values are monotonically increasing/decreasing.\n\nApproximate equality:\n\napprox-eq, approx=: Checks for equality after rounding to a specified number of decimal digits. Can be inaccurate.\ndelta-eq, delta=: Checks if the absolute difference between two numbers is within a given tolerance (absolute and/or relative). This is the recommended way to compare floating-point numbers for near equality\n\nWith absolute tolerance: \\(|a - b| &lt; \\text{abs-tol}\\)\nWith absolute and relative tolerance: \\(|a - b| &lt; \\max(\\text{abs-tol}, \\text{rel-tol} \\cdot \\max(|a|, |b|))\\)\n\n\nRange checks:\n\nbetween?: Checks if a value is within a closed interval \\([a, b]\\), i.e., \\(a \\le \\text{value} \\le b\\).\nbetween-?: Checks if a value is within a half-open interval \\((a, b]\\), i.e., \\(a &lt; \\text{value} \\le b\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/== 1.0 1) ;; =&gt; true\n(m/== 1.0 2) ;; =&gt; false\n(m/== 1 1 2 3 4) ;; =&gt; false\n(m/eq 1.0 1) ;; =&gt; true\n(m/not== 1.0 1) ;; =&gt; false\n(m/not== 1.0 2) ;; =&gt; true\n(m/not== 1 2 3 4) ;; =&gt; true\n(m/not== 1 2 3 1 4) ;; =&gt; false\n(m/&lt; 1.0 1) ;; =&gt; false\n(m/&lt; 1.0 2) ;; =&gt; true\n(m/&lt; 2 1.0) ;; =&gt; false\n(m/&lt; 1 2 3 4 10) ;; =&gt; true\n(m/&lt; 10 4 3 2 1) ;; =&gt; false\n(m/&lt;= 1.0 1) ;; =&gt; true\n(m/&lt;= 1.0 2) ;; =&gt; true\n(m/&lt;= 2 1.0) ;; =&gt; false\n(m/&gt; 1.0 1) ;; =&gt; false\n(m/&gt; 1.0 2) ;; =&gt; false\n(m/&gt; 2 1.0) ;; =&gt; true\n(m/&gt; 1 2 3 4 10) ;; =&gt; false\n(m/&gt; 10 4 3 2 1) ;; =&gt; true\n(m/&gt;= 1.0 1) ;; =&gt; true\n(m/&gt;= 1.0 2) ;; =&gt; false\n(m/&gt;= 2 1.0) ;; =&gt; true\n(m/between? -1 1 -1) ;; =&gt; true\n(m/between? -1 1 0) ;; =&gt; true\n(m/between? -1 1 1) ;; =&gt; true\n(m/between-? -1 1 -1) ;; =&gt; false\n(m/between-? -1 1 0) ;; =&gt; true\n(m/between-? -1 1 1) ;; =&gt; true\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/approx-eq 10 10.01) ;; =&gt; false\n(m/approx-eq 10 10.001) ;; =&gt; true\n(m/approx-eq 10 10.01 1) ;; =&gt; true\n(m/approx-eq 10 10.001 5) ;; =&gt; false\n(m/delta-eq 10 10.01) ;; =&gt; false\n(m/delta-eq 10 10.01 0.1) ;; =&gt; true\n(m/delta-eq 1.0E-6 1.01E-6) ;; =&gt; true\n(m/delta-eq 1.0E-6 1.01E-6 1.0E-8) ;; =&gt; false\n(m/delta-eq 1.0E-6 1.01E-6 1.0E-8 0.01) ;; =&gt; true\n\n\n\n\n\nPredicates\nBasic predicates for common number properties:\n\nzero?: Checks if value is 0 or 0.0.\nnegative-zero?: Checks specifically for the floating point -0.0.\nnear-zero?: Checks if absolute value is within absolute and/or relative tolerance of zero: \\(|x| &lt; \\text{abs-tol}\\) or \\(|x| &lt; \\max(\\text{abs-tol}, \\text{rel-tol} \\cdot |x|)\\).\none?: Checks if value is 1 or 1.0.\nneg?: Checks if value is \\(&lt; 0\\).\npos?: Checks if value is \\(&gt; 0\\).\nnot-neg?: Checks if value is \\(\\ge 0\\).\nnot-pos?: Checks if value is \\(\\le 0\\).\neven?: Checks if a long is even.\nodd?: Checks if a long is odd.\ninteger?: Checks if a number (long or double) has a zero fractional part.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/zero? 0) ;; =&gt; true\n(m/zero? 0.0) ;; =&gt; true\n(m/zero? -0.0) ;; =&gt; true\n(m/zero? 1) ;; =&gt; false\n(m/negative-zero? 0.0) ;; =&gt; false\n(m/negative-zero? -0.0) ;; =&gt; true\n(m/one? 1) ;; =&gt; true\n(m/one? 1.0) ;; =&gt; true\n(m/neg? -1) ;; =&gt; true\n(m/neg? 0) ;; =&gt; false\n(m/neg? 1.0) ;; =&gt; false\n(m/pos? -1) ;; =&gt; false\n(m/pos? 0) ;; =&gt; false\n(m/pos? 1.0) ;; =&gt; true\n(m/not-neg? -1) ;; =&gt; false\n(m/not-neg? 0) ;; =&gt; true\n(m/not-neg? 1.0) ;; =&gt; true\n(m/not-pos? -1) ;; =&gt; true\n(m/not-pos? 0) ;; =&gt; true\n(m/not-pos? 1.0) ;; =&gt; false\n(m/even? 0) ;; =&gt; true\n(m/even? 1) ;; =&gt; false\n(m/even? 2) ;; =&gt; true\n(m/odd? 0) ;; =&gt; false\n(m/odd? 1) ;; =&gt; true\n(m/odd? 2) ;; =&gt; false\n(m/integer? 1) ;; =&gt; true\n(m/integer? 1.0) ;; =&gt; true\n(m/integer? 1.1) ;; =&gt; false\n\n\n\nPredicates for floating point special values:\n\nnan?: Checks if value is Not-a-Number (NaN).\ninf?: Checks if value is positive or negative infinity (Inf or -Inf).\npos-inf?: Checks if value is positive infinity (Inf).\nneg-inf?: Checks if value is negative infinity (-Inf).\ninvalid-double?: Checks if value is not a finite double (NaN or ±Inf).\nvalid-double?: Checks if value is a finite double (not NaN or ±Inf).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/nan? ##NaN) ;; =&gt; true\n(m/nan? ##Inf) ;; =&gt; false\n(m/nan? ##-Inf) ;; =&gt; false\n(m/nan? 1) ;; =&gt; false\n(m/inf? ##NaN) ;; =&gt; false\n(m/inf? ##Inf) ;; =&gt; true\n(m/inf? ##-Inf) ;; =&gt; true\n(m/inf? 1) ;; =&gt; false\n(m/pos-inf? ##NaN) ;; =&gt; false\n(m/pos-inf? ##Inf) ;; =&gt; true\n(m/pos-inf? ##-Inf) ;; =&gt; false\n(m/pos-inf? 1) ;; =&gt; false\n(m/neg-inf? ##NaN) ;; =&gt; false\n(m/neg-inf? ##Inf) ;; =&gt; false\n(m/neg-inf? ##-Inf) ;; =&gt; true\n(m/neg-inf? 1) ;; =&gt; false\n(m/valid-double? ##NaN) ;; =&gt; false\n(m/valid-double? ##Inf) ;; =&gt; false\n(m/valid-double? ##-Inf) ;; =&gt; false\n(m/valid-double? 1) ;; =&gt; true\n(m/invalid-double? ##NaN) ;; =&gt; true\n(m/invalid-double? ##Inf) ;; =&gt; true\n(m/invalid-double? ##-Inf) ;; =&gt; true\n(m/invalid-double? 1) ;; =&gt; false",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#trigonometry",
    "href": "core.html#trigonometry",
    "title": "Core",
    "section": "Trigonometry",
    "text": "Trigonometry\nTrigonometric (with historical variants) and hyperbolic functions.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nradians, degrees\nsin, cos, tan, cot, sec, csc\nqsin, qcos\nsinpi, cospi, tanpi, cotpi, secpi, cscpi\nasin, acos, atan, atan2, acot, asec, acsc\nsinh, cosh, tanh, coth, sech, scsh\nasinh, acosh, atanh, acoth, asech, ascsh\ncrd, acrd\nversin, coversin, vercos, covercos\naversin, acoversin, avercos, acovercos\nhaversin, hacoversin, havercos, hacovercos\nahaversin, ahacoversin, ahavercos, ahacovercos\nexsec, excsc\naexsec, aexcsc\nsinc\n\n\n\n\nAngle conversion\nConvert between radians and degrees\n\nradians(deg): Converts an angle from degrees to radians. \\[\\text{radians} = \\text{degrees} \\cdot \\frac{\\pi}{180^\\circ}\\]\ndegrees(rad): Converts an angle from radians to degrees. \\[\\text{degrees} = \\text{radians} \\cdot \\frac{180^\\circ}{\\pi}\\]\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/radians 180) ;; =&gt; 3.141592653589793\n(m/degrees m/PI) ;; =&gt; 180.0\n(m/radians 90) ;; =&gt; 1.5707963267948966\n(m/degrees m/HALF_PI) ;; =&gt; 90.0\n\n\n\n\n\nTrigonometric\nStandard trigonometric functions:\n\nsin(x): Sine of x.\ncos(x): Cosine of x.\ntan(x): Tangent of x, \\(\\tan(x) = \\frac{\\sin(x)}{\\cos(x)}\\).\ncot(x): Cotangent of x, \\(\\cot(x) = \\frac{1}{\\tan(x)}\\).\nsec(x): Secant of x, \\(\\sec(x) = \\frac{1}{\\cos(x)}\\).\ncsc(x): Cosecant of x, \\(\\csc(x) = \\frac{1}{\\sin(x)}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/sin m/HALF_PI) ;; =&gt; 1.0\n(m/cos m/PI) ;; =&gt; -1.0\n(m/tan m/QUARTER_PI) ;; =&gt; 0.9999999999999997\n(m/cot m/HALF_PI) ;; =&gt; 6.12323399538461E-17\n(m/sec 0.0) ;; =&gt; 1.0\n(m/csc m/HALF_PI) ;; =&gt; 1.0\n\n\n\nQuick trigonometric functions:\n\nqsin(x): Fast, less accurate sine.\nqcos(x): Fast, less accurate cosine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/qsin 0.1) ;; =&gt; 0.10106986275482788\n(m/sin 0.1) ;; =&gt; 0.09983341664682818\n(m/qcos 0.1) ;; =&gt; 0.9948793307948056\n(m/cos 0.1) ;; =&gt; 0.9950041652780257\n\n\n\nPi-scaled trigonometric functions:\n\nsinpi(x): Sine of \\(\\pi x\\), \\(\\sin(\\pi x)\\).\ncospi(x): Cosine of \\(\\pi x\\), \\(\\cos(\\pi x)\\).\ntanpi(x): Tangent of \\(\\pi x\\), \\(\\tan(\\pi x)\\).\ncotpi(x): Cotangent of \\(\\pi x\\), \\(\\cot(\\pi x)\\).\nsecpi(x): Secant of \\(\\pi x\\), \\(\\sec(\\pi x)\\).\ncscpi(x): Cosecant of \\(\\pi x\\), \\(\\csc(\\pi x)\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/sinpi 0.5) ;; =&gt; 1.0\n(m/cospi 1) ;; =&gt; -1.0\n(m/tanpi 0.25) ;; =&gt; 0.9999999999999997\n(m/cotpi 0.5) ;; =&gt; 6.12323399538461E-17\n(m/secpi 0) ;; =&gt; 1.0\n(m/cscpi 0.5) ;; =&gt; 1.0\n\n\n\n\n\nInverse trigonometric\nInverse trigonometric functions:\n\nasin(x): Arcsine of x, \\(\\arcsin(x)\\).\nacos(x): Arccosine of x, \\(\\arccos(x)\\).\natan(x): Arctangent of x, \\(\\arctan(x)\\).\natan2(y, x): Arctangent of \\(\\frac{y}{x}\\), returning the angle in the correct quadrant.\nacot(x): Arccotangent of x, \\(\\operatorname{arccot}(x) = \\frac{\\pi}{2} - \\arctan(x)\\).\nasec(x): Arcsecant of x, \\(\\operatorname{arcsec}(x) = \\arccos(\\frac{1}{x})\\).\nacsc(x): Arccosecant of x, \\(\\operatorname{arccsc}(x) = \\arcsin(\\frac{1}{x})\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/asin 1) ;; =&gt; 1.5707963267948966\n(m/acos -1) ;; =&gt; 3.141592653589793\n(m/atan 1) ;; =&gt; 0.7853981633974483\n(m/acot 0) ;; =&gt; 1.5707963267948966\n(m/asec 1) ;; =&gt; 0.0\n(m/acsc 1) ;; =&gt; 1.5707963267948966\n(m/atan2 1 1) ;; =&gt; 0.7853981633974483\n\n\n\n\n\nSpecial\nsinc(x): Sinc function: \\(\\operatorname{sinc}(x) = \\frac{\\sin(\\pi x)}{\\pi x}\\) for \\(x \\ne 0\\), and \\(1\\) for \\(x=0\\).\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/sinc 0.0) ;; =&gt; 1.0\n(m/sinc 1.0) ;; =&gt; 3.898171832295186E-17\n\n\n\n\n\n\n\n\nHyperbolic\nHyperbolic functions\n\nsinh(x): Hyperbolic sine, \\(\\sinh(x) = \\frac{e^x - e^{-x}}{2}\\).\ncosh(x): Hyperbolic cosine, \\(\\cosh(x) = \\frac{e^x + e^{-x}}{2}\\).\ntanh(x): Hyperbolic tangent, \\(\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)}\\).\ncoth(x): Hyperbolic cotangent, \\(\\coth(x) = \\frac{1}{\\tanh(x)}\\).\nsech(x): Hyperbolic secant, \\(\\operatorname{sech}(x) = \\frac{1}{\\cosh(x)}\\).\ncsch(x): Hyperbolic cosecant, \\(\\operatorname{csch}(x) = \\frac{1}{\\sinh(x)}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(m/sinh 0) ;; =&gt; 0.0\n(m/cosh 0) ;; =&gt; 1.0\n(m/tanh 0) ;; =&gt; 0.0\n(m/coth 1) ;; =&gt; 1.3130352854993315\n(m/sech 0) ;; =&gt; 1.0\n(m/csch 1) ;; =&gt; 0.8509181282393214\n\n\n\nInverse hyperbolic\nInverse hyperbolic functions:\n\nasinh(x): Area hyperbolic sine, \\(\\operatorname{arsinh}(x) = \\ln(x + \\sqrt{x^2 + 1})\\).\nacosh(x): Area hyperbolic cosine, \\(\\operatorname{arcosh}(x) = \\ln(x + \\sqrt{x^2 - 1})\\) for \\(x \\ge 1\\).\natanh(x): Area hyperbolic tangent, \\(\\operatorname{artanh}(x) = \\frac{1}{2} \\ln(\\frac{1 + x}{1 - x})\\) for \\(-1 &lt; x &lt; 1\\).\nacoth(x): Area hyperbolic cotangent, \\(\\operatorname{arcoth}(x) = \\operatorname{artanh}(\\frac{1}{x})\\).\nasech(x): Area hyperbolic secant, \\(\\operatorname{arsech}(x) = \\operatorname{arcosh}(\\frac{1}{x})\\).\nacsch(x): Area hyperbolic cosecant, \\(\\operatorname{arcsch}(x) = \\operatorname{arsinh}(\\frac{1}{x})\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/asinh 0) ;; =&gt; 0.0\n(m/acosh 1) ;; =&gt; 0.0\n(m/atanh 0) ;; =&gt; 0.0\n(m/acoth 2) ;; =&gt; 0.5493061443340548\n(m/asech 1) ;; =&gt; 0.0\n(m/acsch 1) ;; =&gt; 0.8813735870195429\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical\nHistorical/Specific trigonometric functions:\n\ncrd(x): Chord, \\(\\operatorname{crd}(x) = 2 \\sin(\\frac{x}{2})\\).\nacrd(x): Inverse chord, \\(\\operatorname{acrd}(x) = 2 \\arcsin(\\frac{x}{2})\\).\nversin(x): Versine, \\(\\operatorname{versin}(x) = 1 - \\cos(x)\\).\ncoversin(x): Coversine, \\(\\operatorname{coversin}(x) = 1 - \\sin(x)\\).\nvercos(x): Vercosine, \\(\\operatorname{vercos}(x) = 1 + \\cos(x)\\).\ncovercos(x): Covercosine, \\(\\operatorname{covercos}(x) = 1 + \\sin(x)\\).\nhaversin(x) / haversine(x): Haversine, \\(\\operatorname{hav}(x) = \\sin^2(\\frac{x}{2}) = \\frac{1 - \\cos(x)}{2}\\). Also computes the haversine value for pairs of geographic coordinates.\nhacoversin(x): Hacoversine, \\(\\operatorname{hacov}(x) = \\frac{1 - \\sin(x)}{2}\\).\nhavercos(x): Havercosine, \\(\\operatorname{haver}(x) = \\frac{1 + \\cos(x)}{2}\\).\nhacovercos(x): Hacovercosine, \\(\\operatorname{hacover}(x) = \\frac{1 + \\sin(x)}{2}\\).\nexsec(x): Exsecant, \\(\\operatorname{exsec}(x) = \\sec(x) - 1\\).\nexcsc(x): Excosecant, \\(\\operatorname{excsc}(x) = \\csc(x) - 1\\).\naversin(x): Arc versine, \\(\\operatorname{aversin}(x) = \\arccos(1 - x)\\).\nacoversin(x): Arc coversine, \\(\\operatorname{acoversin}(x) = \\arcsin(1 - x)\\).\navercos(x): Arc vercosine, \\(\\operatorname{avercos}(x) = \\arccos(x - 1)\\).\nacovercos(x): Arc covercosine, \\(\\operatorname{acovercos}(x) = \\arcsin(x - 1)\\).\nahaversin(x): Arc haversine, \\(\\operatorname{ahav}(x) = \\arccos(1 - 2x)\\).\nahacoversin(x): Arc hacoversine, \\(\\operatorname{ahacov}(x) = \\arcsin(1 - 2x)\\).\nahavercos(x): Arc havercosine, \\(\\operatorname{ahaver}(x) = \\arccos(2x - 1)\\).\nahacovercos(x): Arc hacovercosine, \\(\\operatorname{ahacover}(x) = \\arcsin(2x - 1)\\).\naexsec(x): Arc exsecant, \\(\\operatorname{aexsec}(x) = \\operatorname{arcsec}(1 + x)\\).\naexcsc(x): Arc excosecant, \\(\\operatorname{aexcsc}(x) = \\operatorname{arccsc}(1 + x)\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/crd m/HALF_PI) ;; =&gt; 1.414213562373095\n(m/versin m/HALF_PI) ;; =&gt; 0.9999999999999999\n(m/coversin m/PI) ;; =&gt; 0.9999999999999999\n(m/vercos m/HALF_PI) ;; =&gt; 1.0\n(m/covercos m/PI) ;; =&gt; 1.0000000000000002\n(m/haversin m/HALF_PI) ;; =&gt; 0.49999999999999994\n(m/hacoversin m/PI) ;; =&gt; 0.49999999999999994\n(m/havercos m/HALF_PI) ;; =&gt; 0.5\n(m/hacovercos m/PI) ;; =&gt; 0.5000000000000001\n(m/exsec m/PI) ;; =&gt; -2.0\n(m/excsc m/HALF_PI) ;; =&gt; 0.0\n(m/acrd 2.0) ;; =&gt; 3.141592653589793\n(m/aversin 1.0) ;; =&gt; 1.5707963267948966\n(m/acoversin 0.0) ;; =&gt; 1.5707963267948966\n(m/avercos 1.0) ;; =&gt; 1.5707963267948966\n(m/acovercos 0.0) ;; =&gt; -1.5707963267948966\n(m/ahaversin 1.0) ;; =&gt; 3.141592653589793\n(m/ahacoversin 0.0) ;; =&gt; 1.5707963267948966\n(m/ahavercos 0.0) ;; =&gt; 3.141592653589793\n(m/ahacovercos 1.0) ;; =&gt; 1.5707963267948966\n(m/aexsec -2.0) ;; =&gt; 3.141592653589793\n(m/aexcsc 0.0) ;; =&gt; 1.5707963267948966\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe haversine formula for calculating the square of half the chord length between two points \\((\\phi_1, \\lambda_1)\\) and \\((\\phi_2, \\lambda_2)\\) on a sphere (where \\(\\phi\\) is latitude and \\(\\lambda\\) is longitude, in radians) is:\n\\[ a = \\sin^2\\left(\\frac{\\phi_2 - \\phi_1}{2}\\right) + \\cos(\\phi_1) \\cos(\\phi_2) \\sin^2\\left(\\frac{\\lambda_2 - \\lambda_1}{2}\\right) \\]\nIn this formula: * \\(a\\) is the square of half the chord length of the great-circle arc. * \\(\\phi_1, \\phi_2\\) are the latitudes of the two points. * \\(\\lambda_1, \\lambda_2\\) are the longitudes of the two points.\nThe actual great-circle distance \\(d\\) between the points on a sphere of radius \\(R\\) is then given by: \\[ d = R \\cdot 2 \\arcsin(\\sqrt{a}) \\]\nThe fastmath.core/haversin function with four arguments [lat1 lon1 lat2 lon2] calculates the value \\(a\\). The fastmath.core/haversine-dist function then calculates the distance \\(d\\) assuming \\(R=1\\).\nLet’s calculate haversin value for two lat/lon points in degrees [38.898N, 77.037E] (White House) and [48.858N, 2.294W] (Eiffel Tower).\n\n(m/haversin (m/radians 38.898) (m/radians 77.037) (m/radians 48.858) (m/radians -2.294))\n\n\n0.2161581789280517\n\nWhich is equivalent to a distance in km on Earth:\n\n(* 6371.2\n   (m/haversine-dist (m/radians 38.898) (m/radians 77.037) (m/radians 48.858) (m/radians -2.294)))\n\n\n6161.63145620435",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#power-and-logarithms",
    "href": "core.html#power-and-logarithms",
    "title": "Core",
    "section": "Power and logarithms",
    "text": "Power and logarithms\nThis section covers exponential, logarithmic, and power functions, including various specialized and numerically stable variants.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nexp, exp2, exp10, qexp\nln, log, logb, log2, log10, qlog\nexpm1, exprel, xexpx, xexpy, cexpexp, expexp\nlog1p, log1pexp, log1mexp, log2mexp, log1psq, logexpm1,log1pmx, logmxp1\nxlogx, xlogy, xlog1py, cloglog, loglog, logcosh\nlogaddexp, logsubexp, logsumexp, log2int\nsigmoid, logit\nsqrt, cbrt, sq, cb\nsafe-sqrt, qsqrt, rqsqrt\npow, spow, fpow, qpow, mpow, pow2, pow3, pow10\nlow-2-exp, high-2-exp, low-exp, high-exp\n\n\n\n\nExponents\n\nexp(x): Natural exponential function \\(e^x\\).\nexp2(x): Base-2 exponential function \\(2^x\\).\nexp10(x): Base-10 exponential function \\(10^x\\).\nqexp(x): Fast, less accurate version of `exp(x)$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/exp 1) ;; =&gt; 2.7182818284590455\n(m/exp2 3) ;; =&gt; 8.0\n(m/exp10 2) ;; =&gt; 100.0\n(m/qexp 1.0) ;; =&gt; 2.799318313598633\n\n\n\n\n\nLogarithms\n\nln(x): Natural logarithm \\(\\ln(x)\\). Alias for log(x) with one argument.\nlog(x): Natural logarithm \\(\\ln(x)\\). With two arguments, computes \\(\\log_b(x)\\).\nlogb(b, x): Logarithm of \\(x\\) with base \\(b\\), \\(\\log_b(x)\\).\nlog2(x): Base-2 logarithm \\(\\log_2(x)\\).\nlog2int(x): Integer base-2 logarithm, related to the exponent of the floating point representation. Returns long.\nlog10(x): Base-10 logarithm \\(\\log_{10}(x)\\).\nqlog(x): Fast, less accurate version of `log(x)$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/ln 10) ;; =&gt; 2.302585092994046\n(m/log 10) ;; =&gt; 2.302585092994046\n(m/log 2 8) ;; =&gt; 3.0\n(m/logb 2 8) ;; =&gt; 3.0\n(m/log2 8) ;; =&gt; 2.9999999999999996\n(m/log2int 8) ;; =&gt; 3\n(m/log2int 7.1) ;; =&gt; 3\n(m/log10 100) ;; =&gt; 2.0\n(m/qlog 10.0) ;; =&gt; 2.3025850929940455\n\n\n\n\n\nSpecialized Log/Exp functions\nThese functions provide numerically stable computations for expressions involving exp and log especially for small or large arguments.\n\nexpm1(x): \\(e^x - 1\\), computed accurately for small \\(x\\).\nexprel(x): \\((e^x - 1)/x\\), computed accurately for small \\(x\\). Returns 1 for \\(x=0\\).\nxexpx(x): \\(x e^x\\).\nxexpy(x,y): \\(x e^y\\).\ncexpexp(x): \\(1-e^{-e^x}\\), inverse of cloglog.\nexpexp(x): \\(e^{-e^{-x}}\\), inverse of loglog.\nlog1p(x): \\(\\ln(1+x)\\), computed accurately for small \\(x\\).\nlog1pexp(x): \\(\\ln(1+e^x)\\).\nlog1mexp(x): \\(\\ln(1-e^x)\\), for \\(x &lt; 0\\).\nlog2mexp(x): \\(\\ln(2-e^x)\\).\nlog1psq(x): \\(\\ln(1+x^2)\\), computed accurately for small \\(x\\).\nlogexpm1(x): \\(\\ln(e^x - 1)\\).\nlog1pmx(x): \\(\\ln(1+x)-x\\), computed accurately for small \\(x\\).\nlogmxp1(x): \\(\\ln(x)-x+1\\), computed accurately for \\(x\\) near 1.\nxlogx(x): \\(x \\ln(x)\\). Returns 0 for \\(x=0\\).\nxlogy(x, y): \\(x \\ln(y)\\). Returns 0 for \\(x=0\\).\nxlog1py(x, y): \\(x \\ln(1+y)\\). Returns 0 for \\(x=0\\).\ncloglog(x): \\(\\ln(-\\ln(1-x))\\). Used in complementary log-log models.\nloglog(x): \\(-\\ln(-\\ln(x))\\).\nlogcosh(x): \\(\\ln(\\cosh(x))\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/expm1 1.0E-9) ;; =&gt; 1.0000000005000001E-9\n(m/exprel 1.0E-9) ;; =&gt; 1.0000000005000001E-9\n(m/xexpx 0.5) ;; =&gt; 0.8243606353500641\n(m/xexpy 0.5 -0.5) ;; =&gt; 0.3032653298563167\n(m/cexpexp 1.0) ;; =&gt; 0.9340119641546875\n(m/expexp 1.0) ;; =&gt; 0.6922006275553464\n(m/log1p 1.0E-9) ;; =&gt; 9.999999995E-10\n(m/log1pexp 0) ;; =&gt; 0.6931471805599453\n(m/log1mexp -1) ;; =&gt; -0.458675145387082\n(m/log2mexp -1) ;; =&gt; 0.4898801256447501\n(m/log1psq 1.0E-5) ;; =&gt; 9.999999999500002E-11\n(m/logexpm1 1) ;; =&gt; 0.5413248546129182\n(m/log1pmx 1) ;; =&gt; -0.3068528194400547\n(m/xlogx 2) ;; =&gt; 1.3862943611198906\n(m/xlogy 2 5) ;; =&gt; 3.2188758248682006\n(m/xlog1py 0.5 -0.5) ;; =&gt; -0.34657359027997264\n(m/cloglog 0.5) ;; =&gt; -0.36651292058166435\n(m/loglog 0.5) ;; =&gt; 0.36651292058166435\n(m/logcosh 1) ;; =&gt; 0.4337808304830272\n\n\n\n\n\nLog-sum-exp\nThese functions are used for numerically stable computation of sums and differences of exponents.\n\nlogaddexp(x, y): \\(\\ln(e^x + e^y)\\).\nlogsubexp(x, y): \\(\\ln(|e^x - e^y|)\\).\nlogsumexp(xs): \\(\\ln(\\sum_{i} e^{x_i})\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/logaddexp 0 0) ;; =&gt; 0.6931471805599453\n(m/logaddexp 100 100) ;; =&gt; 100.69314718055995\n(m/logsubexp 0 0) ;; =&gt; ##-Inf\n(m/logsubexp 100 99) ;; =&gt; 99.54132485461292\n(m/logsumexp [0 0 0]) ;; =&gt; 1.0986122886681098\n(m/logsumexp [100 100 100]) ;; =&gt; 101.09861228866811\n\n\n\n\n\nSigmoid and Logit\nFunctions used in statistics and machine learning.\n\nsigmoid(x): Sigmoid function \\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\). Also known as the logistic function.\nlogit(x): Logit function \\(\\operatorname{logit}(x) = \\ln(\\frac{x}{1-x})\\) for \\(0 &lt; x &lt; 1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/sigmoid 0) ;; =&gt; 0.5\n(m/sigmoid 10) ;; =&gt; 0.9999546021312976\n(m/sigmoid -10) ;; =&gt; 4.5397868702434395E-5\n(m/logit 0.5) ;; =&gt; 0.0\n(m/logit 0.9) ;; =&gt; 2.1972245773362196\n(m/logit 0.1) ;; =&gt; -2.197224577336219\n\n\n\n\n\nRoots and Powers\nBasic root and power functions.\n\nsqrt(x): Square root \\(\\sqrt{x}\\).\ncbrt(x): Cubic root \\(\\sqrt[3]{x}\\).\nsq(x) or pow2(x): Square \\(x^2\\).\ncb(x) or pow3(x): Cube \\(x^3\\).\npow10(x): \\(x^{10}\\).\npow(x, exponent): \\(x^{\\text{exponent}}\\).\nspow(x, exponent): Symmetric power of \\(x\\), keeping the sign: \\(\\operatorname{sgn}(x) |x|^{\\text{exponent}}\\).\nfpow(x, exponent): Fast integer power \\(x^n\\), where \\(n\\) is an integer.\nmpow(x, exponent, modulus): Modular exponentiation \\((x^e \\\\pmod m)\\).\nqsqrt(x): Fast, less accurate square root.\nrqsqrt(x): Fast, less accurate reciprocal square root \\(1/\\sqrt{x}\\).\nsafe-sqrt(x): Square root, returning 0 for \\(x \\le 0\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/sqrt 9) ;; =&gt; 3.0\n(m/qsqrt 9) ;; =&gt; 3.053203231465716\n(m/rqsqrt 9) ;; =&gt; 0.3424875211975356\n(m/cbrt 27) ;; =&gt; 3.0\n(m/sq 3) ;; =&gt; 9.0\n(m/cb 3) ;; =&gt; 27.0\n(m/pow 2 3) ;; =&gt; 8.0\n(m/pow 9 0.5) ;; =&gt; 3.0\n(m/pow 0.2 0.3) ;; =&gt; 0.6170338627200097\n(m/qpow 0.2 0.3) ;; =&gt; 0.61701691483573\n(m/spow -8 1/3) ;; =&gt; -2.0\n(m/fpow 2 10) ;; =&gt; 1024.0\n(m/mpow 30 112 74) ;; =&gt; 70\n(m/safe-sqrt -4) ;; =&gt; 0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExponent/Log Utilities\nFunctions related to powers of 2 and general bases.\n\nlow-2-exp(x): Finds the largest integer \\(n\\) such that \\(2^n \\le |x|\\). Returns long.\nhigh-2-exp(x): Finds the smallest integer \\(n\\) such that \\(2^n \\ge |x|\\). Returns long.\nlow-exp(b, x): Finds the largest integer \\(n\\) such that \\(b^n \\le |x|\\). Returns long.\nhigh-exp(b, x): Finds the smallest integer \\(n\\) such that \\(b^n \\ge |x|\\). Returns long.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/low-2-exp 10) ;; =&gt; 3\n(m/high-2-exp 10) ;; =&gt; 4\n(m/low-exp 10 150) ;; =&gt; 2\n(m/high-exp 10 150) ;; =&gt; 3",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#bitwise-operations",
    "href": "core.html#bitwise-operations",
    "title": "Core",
    "section": "Bitwise operations",
    "text": "Bitwise operations\nFunctions for performing bitwise operations on long primitive types.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nbit-and, bit-or, bit-xor,\nbit-not, bit-nand, bit-nor, bit-xnor, bit-and-not\nbit-set, bit-clear, bit-flip, bit-test\n&lt;&lt;, bit-shift-left, &gt;&gt;, bit-shift-right, &gt;&gt;&gt;, unsigned-bit-shift-right\n\n\n\n\nLogical Bitwise Operations\nThese functions perform standard logical operations on the individual bits of their long arguments. They are inlined and accept one or more arguments for multi-arity operations.\n\nbit-and: Bitwise AND (\\(\\land\\)).\nbit-or: Bitwise OR (\\(\\lor\\)).\nbit-xor: Bitwise XOR (\\(\\oplus\\)).\nbit-not: Bitwise NOT (\\(\\sim\\)).\nbit-nand: Bitwise NAND (\\(\\neg (x \\land y)\\)).\nbit-nor: Bitwise NOR (\\(\\neg (x \\lor y)\\)).\nbit-xnor: Bitwise XNOR (\\(\\neg (x \\oplus y)\\)).\nbit-and-not: Bitwise AND with complement of the second argument (\\(x \\land \\sim y\\)).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/bit-and 12 10) ;; =&gt; 8\n(m/bit-or 12 10) ;; =&gt; 14\n(m/bit-xor 12 10) ;; =&gt; 6\n(m/bit-not 12) ;; =&gt; -13\n(m/bit-nand 12 10) ;; =&gt; -9\n(m/bit-nor 12 10) ;; =&gt; -15\n(m/bit-xnor 12 10) ;; =&gt; -7\n(m/bit-and-not 12 10) ;; =&gt; 4\n(kind/md \"Multi-arity examples\") ;; =&gt; [\"Multi-arity examples\"]\n(m/bit-and 15 12 10) ;; =&gt; 8\n(m/bit-or 0 12 10) ;; =&gt; 14\n(m/bit-xor 15 12 10) ;; =&gt; 9\n\n\n\n\n\nBit Shift Operations\nThese functions shift the bits of a long value left or right.\n\n&lt;&lt; / bit-shift-left: Signed left shift (\\(x \\ll \\text{shift}\\)). Bits shifted off the left are discarded, and zero bits are shifted in from the right.\n&gt;&gt; / bit-shift-right: Signed right shift (\\(x \\gg \\text{shift}\\)). Bits shifted off the right are discarded. The sign bit (the leftmost bit) is extended to fill in from the left, preserving the number’s sign.\n&gt;&gt;&gt; / unsigned-bit-shift-right: Unsigned right shift (\\(x \\ggg \\text{shift}\\)). Bits shifted off the right are discarded. Zero bits are shifted in from the left, regardless of the number’s sign.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/&lt;&lt; 12 2) ;; =&gt; 48\n(m/bit-shift-left 12 2) ;; =&gt; 48\n(m/&gt;&gt; 12 2) ;; =&gt; 3\n(m/bit-shift-right 12 2) ;; =&gt; 3\n(m/&gt;&gt; -12 2) ;; =&gt; -3\n(m/bit-shift-right -12 2) ;; =&gt; -3\n(m/&gt;&gt;&gt; 12 2) ;; =&gt; 3\n(m/unsigned-bit-shift-right 12 2) ;; =&gt; 3\n(m/&gt;&gt;&gt; -12 2) ;; =&gt; 4611686018427387901\n(m/unsigned-bit-shift-right -12 2) ;; =&gt; 4611686018427387901\n\n\n\n\n\nBit Manipulation\nFunctions to manipulate individual bits within a long value.\n\nbit-set: Sets a specific bit at the given index to 1.\nbit-clear: Clears a specific bit at the given index to 0.\nbit-flip: Flips the state of a specific bit at the given index (0 becomes 1, 1 becomes 0).\nbit-test: Tests the state of a specific bit at the given index. Returns true if the bit is 1, false if it is 0.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/bit-set 10 1) ;; =&gt; 10\n(m/bit-clear 10 3) ;; =&gt; 2\n(m/bit-flip 10 2) ;; =&gt; 14\n(m/bit-test 10 1) ;; =&gt; true\n(m/bit-test 10 0) ;; =&gt; false",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#floating-point",
    "href": "core.html#floating-point",
    "title": "Core",
    "section": "Floating point",
    "text": "Floating point\nFunctions for inspecting and manipulating the binary representation of floating-point numbers, and for finding adjacent floating-point values.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nnext-double, prev-double, ulp\ndouble-bits, double-high-bits, double-low-bits, bits-&gt;double\ndouble-exponent, double-significand\n\n\n\n\nnext-double(x): Returns the floating-point value adjacent to x in the direction of positive infinity. Can take an optional delta argument to find the value delta steps away.\nprev-double(x): Returns the floating-point value adjacent to x in the direction of negative infinity. Can take an optional delta argument to find the value delta steps away.\nulp(x): Returns the size of an ulp of x - the distance between this floating-point value and the floating-point value adjacent to it. Formally, it’s the spacing between floating-point numbers in the neighborhood of x.\ndouble-bits(x): Returns the 64-bit long integer representation of the double value x according to the IEEE 754 floating-point “double format” bit layout.\ndouble-high-bits(x): Returns the high 32 bits of the double value x’s IEEE 754 representation as a long.\ndouble-low-bits(x): Returns the low 32 bits of the double value x’s IEEE 754 representation as a long.\nbits-&gt;double(bits): Returns the double floating-point value corresponding to the given 64-bit long integer representation. This is the inverse of double-bits.\ndouble-exponent(x): Returns the unbiased exponent of the double value x.\ndouble-significand(x): Returns the significand (mantissa) of the double value x as a long. This is the 52 explicit bits of the significand for normalized numbers.\nlog2int(x): Returns an integer approximation of \\(\\log_2(|x|)\\). It’s closely related to double-exponent but includes an adjustment based on the significand to provide a more precise floor-like integer log.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/next-double 0.0) ;; =&gt; 4.9E-324\n(m/next-double -0.0) ;; =&gt; 4.9E-324\n(m/next-double 1.0) ;; =&gt; 1.0000000000000002\n(m/next-double 1.0 10) ;; =&gt; 1.0000000000000022\n(m/next-double 1.0E20) ;; =&gt; 1.0000000000000002E20\n(m/prev-double 0.0) ;; =&gt; -4.9E-324\n(m/prev-double 1.0) ;; =&gt; 0.9999999999999999\n(m/prev-double 1.0 10) ;; =&gt; 0.9999999999999989\n(m/prev-double 1.0E20) ;; =&gt; 9.999999999999998E19\n(m/ulp 1.0) ;; =&gt; 2.220446049250313E-16\n(m/ulp 2.0) ;; =&gt; 4.440892098500626E-16\n(m/ulp 1.0E20) ;; =&gt; 16384.0\n(m/log2int 8.0) ;; =&gt; 3\n(m/double-exponent 8.0) ;; =&gt; 3\n(m/log2int 7.1) ;; =&gt; 3\n(m/double-exponent 7.1) ;; =&gt; 2\n\n\n\nNow let’s convert 123.456 to internal representation.\n\n(let [d 123.456] {:double d, :bits (m/double-bits d), :high (m/double-high-bits d), :low (m/double-low-bits d), :exponent (m/double-exponent d), :significand (m/double-significand d)})\n\n\n{:bits 4638387860618067575,\n :double 123.456,\n :exponent 6,\n :high 1079958831,\n :low 446676599,\n :significand 4183844053827191}\n\nConvert back to a double:\n\n(m/bits-&gt;double 4638387860618067575)\n\n\n123.45600000000559",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#combinatorics",
    "href": "core.html#combinatorics",
    "title": "Core",
    "section": "Combinatorics",
    "text": "Combinatorics\nFunctions for common combinatorial calculations, including factorials and binomial coefficients.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nfactorial20, factorial, inv-factorial, log-factorial\nfalling-factorial, falling-factorial-int, rising-factorial, rising-factorial-int\ncombinations, log-combinations\n\n\n\n\nFactorials\nThe factorial of a non-negative integer \\(n\\), denoted by \\(n!\\), is the product of all positive integers less than or equal to \\(n\\). \\(0!\\) is defined as \\(1\\).\n\nfactorial20(n): Computes \\(n!\\) for \\(0 \\le n \\le 20\\) using a precomputed table. Returns long.\nfactorial(n): Computes \\(n!\\) for any non-negative integer \\(n\\). For \\(n &gt; 20\\), it uses the Gamma function: \\(n! = \\Gamma(n+1)\\). Returns double.\ninv-factorial(n): Computes the inverse factorial, \\(\\frac{1}{n!}\\). Returns double.\nlog-factorial(n): Computes the natural logarithm of the factorial, \\(\\ln(n!) = \\ln(\\Gamma(n+1))\\). Returns double.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/factorial 5) ;; =&gt; 120.0\n(m/factorial20 5) ;; =&gt; 120\n(m/factorial 21) ;; =&gt; 5.1090942171709776E19\n(m/inv-factorial 5) ;; =&gt; 0.008333333333333333\n(m/log-factorial 5) ;; =&gt; 4.787491742782046\n\n\n\n\n\nFactorial-like products\nThese functions generalize the factorial to falling (descending) and rising (Pochhammer) products.\n\nfalling-factorial-int(n, x): Computes the falling factorial \\(x^{\\underline{n}} = x(x-1)\\dots(x-n+1)\\) for integer \\(n \\ge 0\\).\nfalling-factorial(n, x): Computes the falling factorial for real \\(n\\), \\(x^{\\underline{n}} = \\frac{\\Gamma(x+1)}{\\Gamma(x-n+1)}\\).\nrising-factorial-int(n, x): Computes the rising factorial (Pochhammer symbol) \\(x^{\\overline{n}} = x(x+1)\\dots(x+n-1)\\) for integer \\(n \\ge 0\\).\nrising-factorial(n, x): Computes the rising factorial for real \\(n\\), \\(x^{\\overline{n}} = \\frac{\\Gamma(x+n)}{\\Gamma(x)}\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/falling-factorial-int 3 10) ;; =&gt; 720.0\n(m/falling-factorial 3.5 10.0) ;; =&gt; 1939.2340147230293\n(m/rising-factorial-int 3 10) ;; =&gt; 1320.0\n(m/rising-factorial 3.5 10.0) ;; =&gt; 4713.795382273956\n\n\n\n\n\nCombinations\nThe binomial coefficient \\(\\binom{n}{k}\\) represents the number of ways to choose \\(k\\) elements from a set of \\(n\\) distinct elements, without regard to the order of selection.\n\ncombinations(n, k): Computes the binomial coefficient \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\). Returns double.\nlog-combinations(n, k): Computes the natural logarithm of the binomial coefficient, \\(\\ln\\binom{n}{k}\\). Returns double.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/combinations 10 2) ;; =&gt; 45.0\n(m/combinations 10 8) ;; =&gt; 45.0\n(m/combinations 5 0) ;; =&gt; 1.0\n(m/combinations 5 6) ;; =&gt; 0.0\n(m/log-combinations 10 2) ;; =&gt; 3.8066624897703196\n(m/log-combinations 1000 500) ;; =&gt; 689.4672615678511",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#rank-and-order",
    "href": "core.html#rank-and-order",
    "title": "Core",
    "section": "Rank and order",
    "text": "Rank and order\nFunctions for determining the rank of elements within a collection and the order (indices) required to sort it.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nrank, rank1\norder\n\n\n\nrank computes the rank of each element in a collection. Ranks are 0-based indices indicating the position an element would have if the collection were sorted. It supports various tie-breaking strategies:\n\n:average: Assign the average rank to tied elements (default).\n:first: Assign ranks based on their original order for ties.\n:last: Assign ranks based on their original order (reverse of :first).\n:random: Assign random ranks for ties.\n:min: Assign the minimum rank to tied elements.\n:max: Assign the maximum rank to tied elements.\n:dense: Assign consecutive ranks without gaps for tied elements.\n\nIt also supports ascending (default) and descending order.\nrank1 is identical to rank but returns 1-based indices.\n\n(def data [5 1 8 1 5 1 1 1])\n\n\n\n\n\n\n\nExamples\n\n\n\nAscending\n\n(m/rank data) ;; =&gt; (5.5 2.0 7.0 2.0 5.5 2.0 2.0 2.0)\n(m/rank data :dense) ;; =&gt; (1 0 2 0 1 0 0 0)\n(m/rank data :min) ;; =&gt; (5 0 7 0 5 0 0 0)\n(m/rank data :max) ;; =&gt; (6 4 7 4 6 4 4 4)\n(m/rank data :random) ;; =&gt; [6 2 7 1 5 3 4 0]\n(m/rank data :random) ;; =&gt; [5 2 7 4 6 3 0 1]\n(m/rank data :first) ;; =&gt; [5 0 7 1 6 2 3 4]\n(m/rank data :last) ;; =&gt; [6 4 7 3 5 2 1 0]\n\nDescending\n\n(m/rank data :average true) ;; =&gt; (1.5 5.0 0.0 5.0 1.5 5.0 5.0 5.0)\n(m/rank data :dense true) ;; =&gt; (1 2 0 2 1 2 2 2)\n(m/rank data :min true) ;; =&gt; (1 3 0 3 1 3 3 3)\n(m/rank data :max true) ;; =&gt; (2 7 0 7 2 7 7 7)\n(m/rank data :random true) ;; =&gt; [2 5 0 3 1 4 7 6]\n(m/rank data :random true) ;; =&gt; [2 5 0 3 1 4 6 7]\n(m/rank data :first true) ;; =&gt; [1 3 0 4 2 5 6 7]\n(m/rank data :last true) ;; =&gt; [2 7 0 6 1 5 4 3]\n\n1-based indices\n\n(m/rank1 data) ;; =&gt; (6.5 3.0 8.0 3.0 6.5 3.0 3.0 3.0)\n\n\n\norder computes the indices that would sort the collection. Applying these indices to the original collection yields the sorted sequence.\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/order data) ;; =&gt; (1 3 5 6 7 0 4 2)\n(map data (m/order data)) ;; =&gt; (1 1 1 1 1 5 5 8)\n(m/order data true) ;; =&gt; (2 0 4 1 3 5 6 7)\n(map data (m/order data true)) ;; =&gt; (8 5 5 1 1 1 1 1)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#interpolation-and-mapping",
    "href": "core.html#interpolation-and-mapping",
    "title": "Core",
    "section": "Interpolation and mapping",
    "text": "Interpolation and mapping\nFunctions for mapping values between numerical ranges (normalization) and for various types of interpolation, smoothing the transition between values. These tools are useful for graphics, animation, signal processing, and data manipulation.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nnorm, mnorm, cnorm, make-norm\nlerp, mlerp\nsmoothstep\ncos-interpolation, smooth-interpolation, quad-interpolation\n\n\n\n\nMapping and Normalization\nThese functions map a value from one numerical range to another.\n\nnorm(v, start, stop): Maps v from the range \\([start, stop]\\) to \\([0, 1]\\). The formula is \\(\\frac{v - start}{stop - start}\\).\nnorm(v, start1, stop1, start2, stop2): Maps v from the range \\([start1, stop1]\\) to \\([start2, stop2]\\). The formula is \\(start2 + (stop2 - start2) \\frac{v - start1}{stop1 - start1}\\).\nmnorm: Macro version of norm for inlining.\ncnorm: Constrained version of norm. The result is clamped to the target range \\([0, 1]\\) or \\([start2, stop2]\\).\nmake-norm(start, stop, [dstart, dstop]): Creates a function that maps values from \\([start, stop]\\) to \\([0, 1]\\) or \\([dstart, dstop]\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/norm 5 0 10) ;; =&gt; 0.5\n(m/norm 15 0 10) ;; =&gt; 1.5\n(m/norm 5 0 10 100 200) ;; =&gt; 150.0\n(m/cnorm 5 0 10) ;; =&gt; 0.5\n(m/cnorm 15 0 10) ;; =&gt; 1.0\n(m/cnorm -5 0 10 100 200) ;; =&gt; 100.0\n(let [map-fn (m/make-norm 0 10 100 200)] (map-fn 5)) ;; =&gt; 150.0\n(let [map-fn (m/make-norm 0 10)] (map-fn 5 100 200)) ;; =&gt; 150.0\n\n\n\n\n\nInterpolation\nInterpolation functions find intermediate values between two points based on a blending factor, often called t (for time). The factor t typically ranges from 0 to 1, where t=0 corresponds to the start value and t=1 to the stop value.\n\nlerp(start, stop, t): Linear interpolation. Blends start and stop linearly based on t. The formula is \\(start + t \\cdot (stop - start)\\).\nmlerp: Macro version of lerp for inlining.\nsmoothstep(edge0, edge1, x): Smooth interpolation between 0 and 1. If \\(x \\le edge0\\), returns 0. If \\(x \\ge edge1\\), returns 1. Otherwise, it performs a cubic Hermite interpolation for \\(x\\) mapped from \\([edge0, edge1]\\) to \\([0, 1]\\). This provides a smooth transition with zero derivative at the edges. The formula for the interpolation step is \\(t^2 (3 - 2t)\\), where \\(t = \\operatorname{cnorm}(x, edge0, edge1)\\).\ncos-interpolation(start, stop, t): Cosine interpolation. Uses a cosine curve to smooth the transition between start and stop.\nsmooth-interpolation(start, stop, t): Uses the smoothstep interpolation curve (cubic \\(3t^2 - 2t^3\\)) to blend start and stop.\nquad-interpolation(start, stop, t): Quadratic interpolation. Uses a parabolic curve, giving a faster initial and slower final rate of change compared to linear.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/lerp 0 10 0.5) ;; =&gt; 5.0\n(m/lerp 0 10 0.0) ;; =&gt; 0.0\n(m/lerp 0 10 1.0) ;; =&gt; 10.0\n(m/smoothstep 2 8 1) ;; =&gt; 0.0\n(m/smoothstep 2 8 5) ;; =&gt; 0.5\n(m/smoothstep 2 8 9) ;; =&gt; 1.0\n(m/cos-interpolation 0 10 0.5) ;; =&gt; 4.999999999999999\n(m/smooth-interpolation 0 10 0.5) ;; =&gt; 5.0\n(m/quad-interpolation 0 10 0.5) ;; =&gt; 5.0",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#distance",
    "href": "core.html#distance",
    "title": "Core",
    "section": "Distance",
    "text": "Distance\nFunctions for calculating distances between points or the magnitude (Euclidean norm) of vectors, including specialized functions for geographic distances.\n\n\n\n\n\n\nDefined functions\n\n\n\n\ndist, qdist, hypot, hypot-sqrt\nhaversine-dist\n\n\n\n\ndist(x1, y1, x2, y2): Calculates the Euclidean distance between two 2D points \\((x_1, y_1)\\) and \\((x_2, y_2)\\). Also accepts pairs of coordinates [x1 y1] and [x2 y2]. \\[ \\operatorname{dist}((x_1, y_1), (x_2, y_2)) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \\]\nqdist: A faster, less accurate version of dist using [qsqrt] instead of [sqrt].\nhypot(x, y) and hypot(x, y, z): Calculates the Euclidean norm (distance from the origin) of a 2D or 3D vector \\(\\sqrt{x^2 + y^2}\\) or \\(\\sqrt{x^2 + y^2 + z^2}\\). This function uses a numerically stable algorithm to avoid potential overflow or underflow issues compared to a direct calculation. \\[ \\operatorname{hypot}(x, y) = \\sqrt{x^2 + y^2} \\] \\[ \\operatorname{hypot}(x, y, z) = \\sqrt{x^2 + y^2 + z^2} \\]\nhypot-sqrt(x, y) and hypot-sqrt(x, y, z): Calculates the Euclidean norm using the direct formula \\(\\sqrt{x^2+y^2}\\) or \\(\\sqrt{x^2+y^2+z^2}\\). This may be less numerically stable than [hypot] for inputs with vastly different magnitudes.\nhaversine-dist(lat1, lon1, lat2, lon2): Calculates the great-circle distance between two points on a sphere given their latitude and longitude (in radians), assuming a sphere with radius \\(R=1\\). This function uses the haversine formula component computed by [haversin] (described in the Trigonometry section) and the inverse haversine formula to find the angle, then scales by \\(R=1\\). Also accepts coordinate pairs [lat1 lon1] and [lat2 lon2]. The distance is \\(d = 2 \\arcsin(\\sqrt{a})\\), where \\(a\\) is the value computed by (haversin lat1 lon1 lat2 lon2).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/dist 0 0 3 4) ;; =&gt; 5.0\n(m/dist [0 0] [3 4]) ;; =&gt; 5.0\n(m/qdist 0 0 3 4) ;; =&gt; 4.981406462931432\n(m/hypot 3 4) ;; =&gt; 5.0\n(m/hypot 1.0E150 1.0E250) ;; =&gt; 1.0E250\n(m/hypot-sqrt 1.0E150 1.0E250) ;; =&gt; ##Inf\n(m/hypot 2 3 4) ;; =&gt; 5.385164807134504\n(m/haversine-dist (m/radians 38.898) (m/radians 77.037) (m/radians 48.858) (m/radians -2.294)) ;; =&gt; 0.9671068960642187\n(m/haversine-dist [(m/radians 38.898) (m/radians 77.037)] [(m/radians 48.858) (m/radians -2.294)]) ;; =&gt; 0.9671068960642187",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#intervals",
    "href": "core.html#intervals",
    "title": "Core",
    "section": "Intervals",
    "text": "Intervals\nFunctions for partitioning numerical ranges and grouping data into intervals. These are useful for data analysis, histogram creation, and data binning.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nslice-range, cut\nco-intervals, group-by-intervals\nsample\n\n\n\n\nslice-range(cnt) / slice-range(start, end, cnt): Generates a sequence of cnt evenly spaced double values between start and end (inclusive). If only cnt is provided, the range [0.0, 1.0] is used. If cnt is 1, returns the midpoint.\ncut(data, breaks) / cut(x1, x2, breaks): Divides a numerical range into breaks adjacent intervals. The range is either explicitly given by x1, x2 or determined by the min/max finite values in data. Returns a sequence of 2-element vectors [lower-bound upper-bound]. The intervals are effectively \\([min\\_value, p_1], (p_1, p_2], \\dots, (p_{breaks-1}, max\\_value]\\). The lower bound of the first interval is slightly adjusted downwards using [prev-double] to ensure the exact minimum is included.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/slice-range 5) ;; =&gt; (0.0 0.25 0.5 0.75 1.0)\n(m/slice-range -10 10 5) ;; =&gt; (-10.0 -5.0 0.0 5.0 10.0)\n(m/cut (range 10) 3) ;; =&gt; ((-4.9E-324 3.0) (3.0 6.0) (6.0 9.0))\n(m/cut 0 9 3) ;; =&gt; ((-4.9E-324 3.0) (3.0 6.0) (6.0 9.0))\n\n\n\n\nco-intervals(data, [number, overlap]): Generates number (default 6) overlapping intervals from the sorted finite values in data. Each interval aims to contain a similar number of data points. overlap (default 0.5) controls the proportion of overlap between consecutive intervals. Returns a sequence of 2-element vectors [lower-bound upper-bound].\ngroup-by-intervals(intervals, coll) / group-by-intervals(coll): Groups the values in coll into the provided intervals. Returns a map where keys are the interval vectors and values are sequences of numbers from coll falling into that interval (checked using [between-?] for \\((lower, upper]\\)). If no intervals are provided, it first computes them using [co-intervals] on coll.\n\n\n(def sample-data (repeatedly 20 #(+ (rand 10) (rand 10) (rand 10))))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/co-intervals sample-data 4) ;; =&gt; ([7.989745206346063 15.048277606627826] [13.542928416402676 17.196999210672697] [15.372698340572096 20.034417858707073] [18.252182592542226 26.10042034460932])\n(m/co-intervals sample-data 4 0.2) ;; =&gt; ([7.989745206346063 13.823468606513986] [13.753719366411461 17.127249970570173] [15.917328103457095 19.1774914572593] [19.107742217156776 26.10042034460932])\n\n\n\n\n(into (sorted-map) (m/group-by-intervals (m/co-intervals sample-data 4) sample-data))\n\n\n{[13.542928416402676 17.196999210672697] (17.09237535051891\n                                          17.162124590621435\n                                          15.952202723508357\n                                          13.788593986462724\n                                          13.577803036453938\n                                          14.515250826239509\n                                          15.407572960623359\n                                          15.013402986576564),\n [15.372698340572096 20.034417858707073] (17.09237535051891\n                                          18.28705721259349\n                                          18.479005327216264\n                                          17.162124590621435\n                                          19.14261683720804\n                                          15.952202723508357\n                                          15.407572960623359\n                                          19.99954323865581),\n [18.252182592542226 26.10042034460932] (18.28705721259349\n                                         18.479005327216264\n                                         26.065545724558056\n                                         19.14261683720804\n                                         22.231970177008506\n                                         22.544005141620858\n                                         20.832046880310664\n                                         19.99954323865581),\n [7.989745206346063 15.048277606627826] (10.08975067029497\n                                         8.024619826397325\n                                         11.774273981424557\n                                         13.788593986462724\n                                         11.670170068167565\n                                         13.577803036453938\n                                         14.515250826239509\n                                         15.013402986576564)}\n\n\nFunction sampling\nsample(f, number-of-values, [domain-min, domain-max, domain?]): Samples a function f by evaluating it at number-of-values evenly spaced points within the specified [domain-min, domain-max] range. If domain? is true, returns [x, (f x)] pairs; otherwise, returns just (f x) values. Defaults to sampling 6 values in the [0, 1] range.\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/sample m/sin 0 m/PI 5) ;; =&gt; (0.0 0.7071067811865475 1.0 0.7071067811865477 1.224646799076922E-16)\n(m/sample m/sin 0 m/PI 5 true) ;; =&gt; ([0.0 0.0] [0.7853981633974483 0.7071067811865475] [1.5707963267948966 1.0] [2.356194490192345 0.7071067811865477] [3.141592653589793 1.224646799076922E-16])",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#other",
    "href": "core.html#other",
    "title": "Core",
    "section": "Other",
    "text": "Other\nThis section contains a collection of miscellaneous utility functions that don’t fit neatly into the preceding categories, including integer number theory functions, boolean operations, data structure conversions, and error calculation utilities.\n\n\n\n\n\n\nDefined functions\n\n\n\n\ngcd, lcm\nbool-not, bool-xor, xor\nidentity-long, identity-double\nrelative-error, absolute-error\nseq-&gt;double-array, seq-&gt;double-double-array, double-array-&gt;seq, double-double-array-&gt;seq\nuse-primitive-operators, unuse-primitive-operators\n\n\n\n\nNumber Theory\nFunctions for calculating the greatest common divisor (GCD) and least common multiple (LCM) of integers.\n\ngcd(a, b): Computes the greatest common divisor of two long integers a and b. Uses the binary GCD algorithm (Stein’s algorithm), which is typically faster than the Euclidean algorithm for integers. \\[ \\operatorname{gcd}(a, b) = \\max \\{ d \\in \\mathbb{Z}^+ : d \\mid a \\land d \\mid b \\} \\]\nlcm(a, b): Computes the least common multiple of two long integers a and b. \\[ \\operatorname{lcm}(a, b) = \\frac{|a \\cdot b|}{\\operatorname{gcd}(a, b)} \\]\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/gcd 12 18) ;; =&gt; 6\n(m/gcd 35 49) ;; =&gt; 7\n(m/gcd 17 23) ;; =&gt; 1\n(m/lcm 12 18) ;; =&gt; 36\n(m/lcm 35 49) ;; =&gt; 245\n(m/lcm 17 23) ;; =&gt; 391\n\n\n\n\n\nBoolean and Identity Utilities\nBasic boolean operations and identity functions for primitive types.\n\nbool-not(x): Primitive boolean NOT. Returns true if x is logically false, false otherwise.\nbool-xor(x, y) / xor(x, y): Primitive boolean XOR (exclusive OR). Returns true if exactly one of x or y is logically true. Accepts multiple arguments, chaining the XOR operation.\nidentity-long(x): Returns its long argument unchanged. Useful for type hinting or guaranteeing a long primitive.\nidentity-double(x): Returns its double argument unchanged. Useful for type hinting or guaranteeing a double primitive.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/bool-not true) ;; =&gt; false\n(m/bool-not false) ;; =&gt; true\n(m/xor true false) ;; =&gt; true\n(m/xor true true) ;; =&gt; false\n(m/xor true false true) ;; =&gt; false\n(m/identity-long 5) ;; =&gt; 5\n(m/identity-double 5.0) ;; =&gt; 5.0\n\n\n\n\n\nError Calculation\nFunctions to compute the difference between a value and its approximation.\n\nabsolute-error(v, v-approx): Computes the absolute difference between a value v and its approximation v-approx. \\[ \\operatorname{absolute-error}(v, v_{\\text{approx}}) = |v - v_{\\text{approx}}| \\]\nrelative-error(v, v-approx): Computes the relative difference between a value v and its approximation v-approx. \\[ \\operatorname{relative-error}(v, v_{\\text{approx}}) = \\left| \\frac{v - v_{\\text{approx}}}{v} \\right| \\]\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/absolute-error 10.0 10.01) ;; =&gt; 0.009999999999999787\n(m/relative-error 10.0 10.01) ;; =&gt; 9.999999999999788E-4\n(m/absolute-error 1.0E-6 1.01E-6) ;; =&gt; 1.0000000000000116E-8\n(m/relative-error 1.0E-6 1.01E-6) ;; =&gt; 0.010000000000000116\n\n\n\n\n\nArray and Sequence Conversion\nUtilities for converting between Clojure sequences and primitive Java arrays (double[] and double[][]). These are often necessary for interoperation with Java libraries or for performance-critical operations.\n\nseq-&gt;double-array(vs): Converts a sequence vs into a double[] array. If vs is already a double[], it is returned directly. If vs is a single number, returns a double[] of size 1 containing that number.\ndouble-array-&gt;seq(res): Converts a double[] array res into a sequence. This is an alias for Clojure’s built-in seq function which works correctly for Java arrays.\nseq-&gt;double-double-array(vss): Converts a sequence of sequences vss into a double[][] array. If vss is already a double[][], it is returned directly.\ndouble-double-array-&gt;seq(res): Converts a double[][] array res into a sequence of sequences.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(m/seq-&gt;double-array [1 2 3]) ;; =&gt; #object[\"[D\" 0x7ec7a807 \"[D@7ec7a807\"]\n(m/double-array-&gt;seq (double-array [1.0 2.0 3.0])) ;; =&gt; (1.0 2.0 3.0)\n(m/seq-&gt;double-double-array [[1 2] [3 4]]) ;; =&gt; #object[\"[[D\" 0xef3afa3 \"[[D@ef3afa3\"]\n(m/double-double-array-&gt;seq (into-array (map double-array [[1 2] [3 4]]))) ;; =&gt; ((1.0 2.0) (3.0 4.0))\n\n\n\n\n\nPrimitive Operators Toggle\nMacros to replace or restore core Clojure math functions with fastmath.core primitive-specialized versions. This can improve performance but should be used carefully as the fastmath.core versions have specific type and return value behaviors.\n\nuse-primitive-operators: Replaces a select set of clojure.core functions (like +, -, *, /, comparison operators, bitwise operators, inc, dec, predicates) with their primitive-specialized fastmath.core macro equivalents within the current namespace.\nunuse-primitive-operators: Reverts the changes made by use-primitive-operators, restoring the original clojure.core functions in the current namespace. Recommended practice is to call this at the end of any namespace that calls use-primitive-operators (especially important in Clojure 1.12+).\n\nExample usage (typically done at the top or bottom of a namespace):\n(ns my-namespace (:require [fastmath.core :as m]))\n(m/use-primitive-operators)\n... your code using primitive math ...\n(m/unuse-primitive-operators) ;; or at the end of the file",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#constants",
    "href": "core.html#constants",
    "title": "Core",
    "section": "Constants",
    "text": "Constants\n\n\n\n\n\nConstant symbol\nValue\nDescription\n\n\n\n\n-E\n-2.718281828459045\nValue of \\(-\\mathrm{e}\\)\n\n\n-HALF_PI\n-1.5707963267948966\nValue of \\(\\frac{\\pi}{2}\\)\n\n\n-PI\n-3.141592653589793\nValue of \\(-\\pi\\)\n\n\n-QUARTER_PI\n-0.7853981633974483\nValue of \\(\\frac{\\pi}{4}\\)\n\n\n-TAU\n-6.283185307179586\nValue of \\(-2\\pi\\)\n\n\n-THIRD_PI\n1.0471975511965976\nValue of \\(\\frac{\\pi}{3}\\)\n\n\n-TWO_PI\n-6.283185307179586\nValue of \\(-2\\pi\\)\n\n\nCATALAN_G\n0.915965594177219\nCatalan G\n\n\nE\n2.718281828459045\nValue of \\(\\mathrm{e}\\)\n\n\nEPSILON\n1.0E-10\n\\(\\varepsilon\\), a small number\n\n\nFOUR_INV_PI\n1.2732395447351628\nValue of \\(\\frac{4}{\\pi}\\)\n\n\nGAMMA\n0.5772156649015329\n\\(\\gamma\\), Euler-Mascheroni constant\n\n\nHALF_PI\n1.5707963267948966\nValue of \\(\\frac{\\pi}{2}\\)\n\n\nINV_FOUR_PI\n0.07957747154594767\nValue of \\(\\frac{2}{2\\pi}\\)\n\n\nINV_LN2\n1.4426950408889634\nValue of \\(\\frac{1}{\\ln{2}}\\)\n\n\nINV_LOG_HALF\n-1.4426950408889634\nValue of \\(\\frac{1}{\\ln{\\frac{1}{2}}}\\)\n\n\nINV_PI\n0.3183098861837907\nValue of \\(\\frac{1}{\\pi}\\)\n\n\nINV_SQRT2PI\n0.3989422804014327\nValue of \\(\\frac{1}{\\sqrt{2\\pi}}\\)\n\n\nINV_SQRTPI\n0.5641895835477563\nValue of \\(\\frac{1}{\\sqrt\\pi}\\)\n\n\nINV_SQRT_2\n0.7071067811865475\nValue of \\(\\frac{1}{\\sqrt{2}}\\)\n\n\nINV_TWO_PI\n0.15915494309189535\nValue of \\(\\frac{1}{2\\pi}\\)\n\n\nLANCZOS_G\n4.7421875\nLanchos approximation of g constant\n\n\nLN10\n2.302585092994046\nValue of \\(\\ln{10}\\)\n\n\nLN2\n0.6931471805599453\nValue of \\(\\ln{2}\\)\n\n\nLN2_2\n0.34657359027997264\nValue of \\(\\frac{\\ln{2}}{2}\\)\n\n\nLOG10E\n0.4342944819032518\n\\(\\log_{10}{\\mathrm{e}}\\)\n\n\nLOG2E\n1.4426950408889634\n\\(\\log_{2}{\\mathrm{e}}\\)\n\n\nLOG_HALF\n-0.6931471805599453\nValue of \\(\\ln{\\frac{1}{2}}\\)\n\n\nLOG_PI\n1.1447298858494002\nValue of \\(\\ln{\\pi}\\)\n\n\nLOG_TWO_PI\n1.8378770664093453\nValue of \\(\\ln{2\\pi}\\)\n\n\nMACHINE-EPSILON\n1.1102230246251565E-16\nulp(1)/2\n\n\nMACHINE-EPSILON10\n1.1102230246251565E-15\n5ulp(1)\n\n\nM_1_PI\n0.3183098861837907\nValue of \\(\\frac{1}{\\pi}\\)\n\n\nM_2_PI\n0.6366197723675814\nValue of \\(\\frac{2}{\\pi}\\)\n\n\nM_2_SQRTPI\n1.1283791670955126\nValue of \\(\\frac{2}{\\sqrt\\pi}\\)\n\n\nM_3PI_4\n2.356194490192345\nValue of \\(\\frac{3\\pi}{4}\\)\n\n\nM_E\n2.718281828459045\nValue of \\(\\mathrm{e}\\)\n\n\nM_INVLN2\n1.4426950408889634\nValue of \\(\\frac{1}{\\ln{2}}\\)\n\n\nM_IVLN10\n0.43429448190325176\nValue of \\(\\frac{1}{\\ln{10}}\\)\n\n\nM_LN10\n2.302585092994046\nValue of \\(\\ln{10}\\)\n\n\nM_LN2\n0.6931471805599453\nValue of \\(\\ln{2}\\)\n\n\nM_LOG10E\n0.4342944819032518\nValue of \\(\\log_{10}{e}\\)\n\n\nM_LOG2E\n1.4426950408889634\nValue of \\(\\log_{2}{e}\\)\n\n\nM_LOG2_E\n0.6931471805599453\nValue of \\(\\ln{2}\\)\n\n\nM_PI\n3.141592653589793\nValue of \\(\\pi\\)\n\n\nM_PI_2\n1.5707963267948966\nValue of \\(\\frac{\\pi}{2}\\)\n\n\nM_PI_4\n0.7853981633974483\nValue of \\(\\frac{\\pi}{4}\\)\n\n\nM_SQRT1_2\n0.7071067811865475\nValue of \\(\\frac{1}{\\sqrt{2}}\\)\n\n\nM_SQRT2\n1.4142135623730951\nValue of \\(\\sqrt{2}\\)\n\n\nM_SQRT3\n1.7320508075688772\nValue of \\(\\sqrt{3}\\)\n\n\nM_SQRT_PI\n1.7724538509055159\nValue of \\(\\sqrt\\pi\\)\n\n\nM_TWOPI\n6.283185307179586\nValue of \\(2\\pi\\)\n\n\nONE_SIXTH\n0.16666666666666666\nValue of \\(\\frac{1}{6}\\)\n\n\nONE_THIRD\n0.3333333333333333\nValue of \\(\\frac{1}{3}\\)\n\n\nPHI\n1.618033988749895\nGolden ratio \\(\\phi\\)\n\n\nPI\n3.141592653589793\nValue of \\(\\pi\\)\n\n\nQUARTER_PI\n0.7853981633974483\nValue of \\(\\frac{\\pi}{4}\\)\n\n\nSILVER\n2.414213562373095\nSilver ratio \\(\\delta_S\\)\n\n\nSIXTH\n0.16666666666666666\nValue of \\(\\frac{1}{6}\\)\n\n\nSQRT2\n1.4142135623730951\nValue of \\(\\sqrt{2}\\)\n\n\nSQRT2PI\n2.5066282746310002\nValue of \\(\\sqrt{2\\pi}\\)\n\n\nSQRT2_2\n0.7071067811865476\nValue of \\(\\frac{\\sqrt{2}}{2}\\)\n\n\nSQRT3\n1.7320508075688772\nValue of \\(\\sqrt{3}\\)\n\n\nSQRT3_2\n0.8660254037844386\nValue of \\(\\frac{\\sqrt{3}}{2}\\)\n\n\nSQRT3_3\n0.5773502691896257\nValue of \\(\\frac{\\sqrt{3}}{3}\\)\n\n\nSQRT3_4\n0.4330127018922193\nValue of \\(\\frac{\\sqrt{3}}{4}\\)\n\n\nSQRT5\n2.23606797749979\nValue of \\(\\sqrt{5}\\)\n\n\nSQRTPI\n1.7724538509055159\nValue of \\(\\sqrt{\\pi}\\)\n\n\nSQRT_2_PI\n0.7978845608028654\nValue of \\(\\sqrt{\\frac{2}{\\pi}}\\)\n\n\nSQRT_HALFPI\n1.2533141373155001\nValue of \\(\\sqrt{\\frac{1}{2}\\pi}\\)\n\n\nTAU\n6.283185307179586\nValue of \\(2\\pi\\)\n\n\nTHIRD\n0.3333333333333333\nValue of \\(\\frac{1}{3}\\)\n\n\nTHIRD_PI\n1.0471975511965976\nValue of \\(\\frac{\\pi}{3}\\)\n\n\nTWO_INV_PI\n0.6366197723675814\nValue of \\(\\frac{2}{\\pi}\\)\n\n\nTWO_PI\n6.283185307179586\nValue of \\(2\\pi\\)\n\n\nTWO_THIRD\n0.6666666666666666\nValue of \\(\\frac{2}{3}\\)\n\n\nTWO_THIRDS\n0.6666666666666666\nValue of \\(\\frac{2}{3}\\)\n\n\ndeg-in-rad\n0.017453292519943295\n\\(\\frac{\\pi}{180}\\)\n\n\ndouble-one-minus-epsilon\n0.9999999999999999\nValue of 0x1.fffffffffffffp-1d = 0.(9)\n\n\nrad-in-deg\n57.29577951308232\n\\(\\frac{180}{\\pi}\\)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#reference",
    "href": "core.html#reference",
    "title": "Core",
    "section": "Reference",
    "text": "Reference\n\nfastmath.core\nHigh-performance mathematical functions and constants for Clojure, optimized for primitive double and long types.\nKey features:\n\nMost functions are specialized for primitive types (double and long) and are inlined for performance.\nPrimarily backed by the FastMath library and custom primitive implementations.\nProvides an option to replace clojure.core’s standard numerical operators with primitive-specialized macros.\n\nThis namespace contains functions for:\n\nBasic arithmetic (+, -, *, /, etc.)\nComparisons and predicates (==, &lt;, zero?, pos?, etc.)\nBitwise operations\nTrigonometric and hyperbolic functions\nExponents, logarithms, and powers\nFloating-point specific operations (ulp, bits manipulation)\nCombinatorics (factorial, combinations)\nDistance calculations\nInterpolation and mapping\nUtility functions (gcd, lcm, error calculation, etc.)\n\nPrimitive Math Operators:\nA set of inlined macros designed to replace selected clojure.core arithmetic, comparison, and bitwise operators for potential performance gains with primitive arguments. These macros operate on double and long primitives and generally return primitive values.\nReplaced operators:\n\n* + - / &gt; &lt; &gt;= &lt;= == rem quot mod\nbit-or bit-and bit-xor bit-not bit-and-not\nbit-shift-left bit-shift-right unsigned-bit-shift-right\nbit-set bit-clear bit-flip bit-test\ninc dec\nzero? neg? pos? even? odd?\nmin max\nabs\nAdditionally: &lt;&lt; &gt;&gt; &gt;&gt;&gt; not==\n\nTo enable these primitive operators in your namespace, call use-primitive-operators. To revert to the original clojure.core functions, call unuse-primitive-operators. Note that the fastmath.core versions are not a complete drop-in replacement due to their primitive-specific behavior (e.g., return types), and calling unuse-primitive-operators at the end of the namespace is recommended, especially in Clojure 1.12+.\n\n\n*\n\n(*)\n(* a)\n(* a b)\n(* a b c)\n(* a b c d)\n(* a b c d & r)\n\nPrimitive and inlined *.\nsource\n\n\n\n+\n\n(+)\n(+ a)\n(+ a b)\n(+ a b c)\n(+ a b c d)\n(+ a b c d & r)\n\nPrimitive and inlined +.\nsource\n\n\n\n-\n\n(- a)\n(- a b)\n(- a b c)\n(- a b c d)\n(- a b c d & r)\n\nPrimitive and inlined -.\nsource\n\n\n\n-E CONST\n-E = -2.718281828459045\nValue of \\(-\\mathrm{e}\\)\nsource\n\n\n\n-HALF_PI CONST\n-HALF_PI = -1.5707963267948966\nValue of \\(\\frac{\\pi}{2}\\)\nsource\n\n\n\n-PI CONST\n-PI = -3.141592653589793\nValue of \\(-\\pi\\)\nsource\n\n\n\n-QUARTER_PI CONST\n-QUARTER_PI = -0.7853981633974483\nValue of \\(\\frac{\\pi}{4}\\)\nsource\n\n\n\n-TAU CONST\n-TAU = -6.283185307179586\nValue of \\(-2\\pi\\)\nsource\n\n\n\n-THIRD_PI CONST\n-THIRD_PI = 1.0471975511965976\nValue of \\(\\frac{\\pi}{3}\\)\nsource\n\n\n\n-TWO_PI CONST\n-TWO_PI = -6.283185307179586\nValue of \\(-2\\pi\\)\nsource\n\n\n\n/\n\n(/ a)\n(/ a b)\n(/ a b c)\n(/ a b c d)\n(/ a b c d & r)\n\nPrimitive and inlined /.\nsource\n\n\n\n&lt;\n\n(&lt; _)\n(&lt; a b)\n(&lt; a b & r)\n\nPrimitive math less-then function.\nsource\n\n\n\n&lt;&lt;\n\n(&lt;&lt; x shift)\n\nShift bits left\nsource\n\n\n\n&lt;=\n\n(&lt;= _)\n(&lt;= a b)\n(&lt;= a b & r)\n\nPrimitive math less-and-equal function.\nsource\n\n\n\n==\n\n(== _)\n(== a b)\n(== a b & r)\n\nPrimitive math equality function.\nsource\n\n\n\n&gt;\n\n(&gt; _)\n(&gt; a b)\n(&gt; a b & r)\n\nPrimitive math greater-than function.\nsource\n\n\n\n&gt;=\n\n(&gt;= _)\n(&gt;= a b)\n(&gt;= a b & r)\n\nPrimitive math greater-and-equal function.\nsource\n\n\n\n&gt;&gt;\n\n(&gt;&gt; x shift)\n\nShift bits right and keep most significant bit unchanged\nsource\n\n\n\n&gt;&gt;&gt;\n\n(&gt;&gt;&gt; x shift)\n\nShift bits right and set most significant bit to 0\nsource\n\n\n\nCATALAN_G CONST\nCATALAN_G = 0.915965594177219\nCatalan G\nsource\n\n\n\nE CONST\nE = 2.718281828459045\nValue of \\(\\mathrm{e}\\)\nsource\n\n\n\nEPSILON CONST\nEPSILON = 1.0E-10\n\\(\\varepsilon\\), a small number\nsource\n\n\n\nFOUR_INV_PI CONST\nFOUR_INV_PI = 1.2732395447351628\nValue of \\(\\frac{4}{\\pi}\\)\nsource\n\n\n\nGAMMA CONST\nGAMMA = 0.5772156649015329\n\\(\\gamma\\), Euler-Mascheroni constant\nsource\n\n\n\nHALF_PI CONST\nHALF_PI = 1.5707963267948966\nValue of \\(\\frac{\\pi}{2}\\)\nsource\n\n\n\nINV_FOUR_PI CONST\nINV_FOUR_PI = 0.07957747154594767\nValue of \\(\\frac{2}{2\\pi}\\)\nsource\n\n\n\nINV_LN2 CONST\nINV_LN2 = 1.4426950408889634\nValue of \\(\\frac{1}{\\ln{2}}\\)\nsource\n\n\n\nINV_LOG_HALF CONST\nINV_LOG_HALF = -1.4426950408889634\nValue of \\(\\frac{1}{\\ln{\\frac{1}{2}}}\\)\nsource\n\n\n\nINV_PI CONST\nINV_PI = 0.3183098861837907\nValue of \\(\\frac{1}{\\pi}\\)\nsource\n\n\n\nINV_SQRT2PI CONST\nINV_SQRT2PI = 0.3989422804014327\nValue of \\(\\frac{1}{\\sqrt{2\\pi}}\\)\nsource\n\n\n\nINV_SQRTPI CONST\nINV_SQRTPI = 0.5641895835477563\nValue of \\(\\frac{1}{\\sqrt\\pi}\\)\nsource\n\n\n\nINV_SQRT_2 CONST\nINV_SQRT_2 = 0.7071067811865475\nValue of \\(\\frac{1}{\\sqrt{2}}\\)\nsource\n\n\n\nINV_TWO_PI CONST\nINV_TWO_PI = 0.15915494309189535\nValue of \\(\\frac{1}{2\\pi}\\)\nsource\n\n\n\nLANCZOS_G CONST\nLANCZOS_G = 4.7421875\nLanchos approximation of g constant\nsource\n\n\n\nLN10 CONST\nLN10 = 2.302585092994046\nValue of \\(\\ln{10}\\)\nsource\n\n\n\nLN2 CONST\nLN2 = 0.6931471805599453\nValue of \\(\\ln{2}\\)\nsource\n\n\n\nLN2_2 CONST\nLN2_2 = 0.34657359027997264\nValue of \\(\\frac{\\ln{2}}{2}\\)\nsource\n\n\n\nLOG10E CONST\nLOG10E = 0.4342944819032518\n\\(\\log_{10}{\\mathrm{e}}\\)\nsource\n\n\n\nLOG2E CONST\nLOG2E = 1.4426950408889634\n\\(\\log_{2}{\\mathrm{e}}\\)\nsource\n\n\n\nLOG_HALF CONST\nLOG_HALF = -0.6931471805599453\nValue of \\(\\ln{\\frac{1}{2}}\\)\nsource\n\n\n\nLOG_PI CONST\nLOG_PI = 1.1447298858494002\nValue of \\(\\ln{\\pi}\\)\nsource\n\n\n\nLOG_TWO_PI CONST\nLOG_TWO_PI = 1.8378770664093453\nValue of \\(\\ln{2\\pi}\\)\nsource\n\n\n\nMACHINE-EPSILON CONST\nMACHINE-EPSILON = 1.1102230246251565E-16\nulp(1)/2\nsource\n\n\n\nMACHINE-EPSILON10 CONST\nMACHINE-EPSILON10 = 1.1102230246251565E-15\n5ulp(1)\nsource\n\n\n\nM_1_PI CONST\nM_1_PI = 0.3183098861837907\nValue of \\(\\frac{1}{\\pi}\\)\nsource\n\n\n\nM_2_PI CONST\nM_2_PI = 0.6366197723675814\nValue of \\(\\frac{2}{\\pi}\\)\nsource\n\n\n\nM_2_SQRTPI CONST\nM_2_SQRTPI = 1.1283791670955126\nValue of \\(\\frac{2}{\\sqrt\\pi}\\)\nsource\n\n\n\nM_3PI_4 CONST\nM_3PI_4 = 2.356194490192345\nValue of \\(\\frac{3\\pi}{4}\\)\nsource\n\n\n\nM_E CONST\nM_E = 2.718281828459045\nValue of \\(\\mathrm{e}\\)\nsource\n\n\n\nM_INVLN2 CONST\nM_INVLN2 = 1.4426950408889634\nValue of \\(\\frac{1}{\\ln{2}}\\)\nsource\n\n\n\nM_IVLN10 CONST\nM_IVLN10 = 0.43429448190325176\nValue of \\(\\frac{1}{\\ln{10}}\\)\nsource\n\n\n\nM_LN10 CONST\nM_LN10 = 2.302585092994046\nValue of \\(\\ln{10}\\)\nsource\n\n\n\nM_LN2 CONST\nM_LN2 = 0.6931471805599453\nValue of \\(\\ln{2}\\)\nsource\n\n\n\nM_LOG10E CONST\nM_LOG10E = 0.4342944819032518\nValue of \\(\\log_{10}{e}\\)\nsource\n\n\n\nM_LOG2E CONST\nM_LOG2E = 1.4426950408889634\nValue of \\(\\log_{2}{e}\\)\nsource\n\n\n\nM_LOG2_E CONST\nM_LOG2_E = 0.6931471805599453\nValue of \\(\\ln{2}\\)\nsource\n\n\n\nM_PI CONST\nM_PI = 3.141592653589793\nValue of \\(\\pi\\)\nsource\n\n\n\nM_PI_2 CONST\nM_PI_2 = 1.5707963267948966\nValue of \\(\\frac{\\pi}{2}\\)\nsource\n\n\n\nM_PI_4 CONST\nM_PI_4 = 0.7853981633974483\nValue of \\(\\frac{\\pi}{4}\\)\nsource\n\n\n\nM_SQRT1_2 CONST\nM_SQRT1_2 = 0.7071067811865475\nValue of \\(\\frac{1}{\\sqrt{2}}\\)\nsource\n\n\n\nM_SQRT2 CONST\nM_SQRT2 = 1.4142135623730951\nValue of \\(\\sqrt{2}\\)\nsource\n\n\n\nM_SQRT3 CONST\nM_SQRT3 = 1.7320508075688772\nValue of \\(\\sqrt{3}\\)\nsource\n\n\n\nM_SQRT_PI CONST\nM_SQRT_PI = 1.7724538509055159\nValue of \\(\\sqrt\\pi\\)\nsource\n\n\n\nM_TWOPI CONST\nM_TWOPI = 6.283185307179586\nValue of \\(2\\pi\\)\nsource\n\n\n\nONE_SIXTH CONST\nONE_SIXTH = 0.16666666666666666\nValue of \\(\\frac{1}{6}\\)\nsource\n\n\n\nONE_THIRD CONST\nONE_THIRD = 0.3333333333333333\nValue of \\(\\frac{1}{3}\\)\nsource\n\n\n\nPHI CONST\nPHI = 1.618033988749895\nGolden ratio \\(\\phi\\)\nsource\n\n\n\nPI CONST\nPI = 3.141592653589793\nValue of \\(\\pi\\)\nsource\n\n\n\nQUARTER_PI CONST\nQUARTER_PI = 0.7853981633974483\nValue of \\(\\frac{\\pi}{4}\\)\nsource\n\n\n\nSILVER CONST\nSILVER = 2.414213562373095\nSilver ratio \\(\\delta_S\\)\nsource\n\n\n\nSIXTH CONST\nSIXTH = 0.16666666666666666\nValue of \\(\\frac{1}{6}\\)\nsource\n\n\n\nSQRT2 CONST\nSQRT2 = 1.4142135623730951\nValue of \\(\\sqrt{2}\\)\nsource\n\n\n\nSQRT2PI CONST\nSQRT2PI = 2.5066282746310002\nValue of \\(\\sqrt{2\\pi}\\)\nsource\n\n\n\nSQRT2_2 CONST\nSQRT2_2 = 0.7071067811865476\nValue of \\(\\frac{\\sqrt{2}}{2}\\)\nsource\n\n\n\nSQRT3 CONST\nSQRT3 = 1.7320508075688772\nValue of \\(\\sqrt{3}\\)\nsource\n\n\n\nSQRT3_2 CONST\nSQRT3_2 = 0.8660254037844386\nValue of \\(\\frac{\\sqrt{3}}{2}\\)\nsource\n\n\n\nSQRT3_3 CONST\nSQRT3_3 = 0.5773502691896257\nValue of \\(\\frac{\\sqrt{3}}{3}\\)\nsource\n\n\n\nSQRT3_4 CONST\nSQRT3_4 = 0.4330127018922193\nValue of \\(\\frac{\\sqrt{3}}{4}\\)\nsource\n\n\n\nSQRT5 CONST\nSQRT5 = 2.23606797749979\nValue of \\(\\sqrt{5}\\)\nsource\n\n\n\nSQRTPI CONST\nSQRTPI = 1.7724538509055159\nValue of \\(\\sqrt{\\pi}\\)\nsource\n\n\n\nSQRT_2_PI CONST\nSQRT_2_PI = 0.7978845608028654\nValue of \\(\\sqrt{\\frac{2}{\\pi}}\\)\nsource\n\n\n\nSQRT_HALFPI CONST\nSQRT_HALFPI = 1.2533141373155001\nValue of \\(\\sqrt{\\frac{1}{2}\\pi}\\)\nsource\n\n\n\nTAU CONST\nTAU = 6.283185307179586\nValue of \\(2\\pi\\)\nsource\n\n\n\nTHIRD CONST\nTHIRD = 0.3333333333333333\nValue of \\(\\frac{1}{3}\\)\nsource\n\n\n\nTHIRD_PI CONST\nTHIRD_PI = 1.0471975511965976\nValue of \\(\\frac{\\pi}{3}\\)\nsource\n\n\n\nTWO_INV_PI CONST\nTWO_INV_PI = 0.6366197723675814\nValue of \\(\\frac{2}{\\pi}\\)\nsource\n\n\n\nTWO_PI CONST\nTWO_PI = 6.283185307179586\nValue of \\(2\\pi\\)\nsource\n\n\n\nTWO_THIRD CONST\nTWO_THIRD = 0.6666666666666666\nValue of \\(\\frac{2}{3}\\)\nsource\n\n\n\nTWO_THIRDS CONST\nTWO_THIRDS = 0.6666666666666666\nValue of \\(\\frac{2}{3}\\)\nsource\n\n\n\nabs\n\n(abs x)\n\nAbsolute value.\nsource\n\n\n\nabsolute-error\n\n(absolute-error v v-approx)\n\nAbsolute error between two values\nsource\n\n\n\nacos\n\n(acos x)\n\nacos(x)\nsource\n\n\n\nacosh\n\n(acosh x)\n\nacosh(x)\nsource\n\n\n\nacot\n\n(acot x)\n\nacot(x)\nsource\n\n\n\nacoth\n\n(acoth x)\n\nArea hyperbolic cotangent\nsource\n\n\n\nacovercos\n\n(acovercos x)\n\nArc covercosine\nsource\n\n\n\nacoversin\n\n(acoversin x)\n\nArc coversine\nsource\n\n\n\nacrd\n\n(acrd x)\n\nInverse chord\nsource\n\n\n\nacsc\n\n(acsc x)\n\nacsc(x)\nsource\n\n\n\nacsch\n\n(acsch v)\n\nArea hyperbolic cosecant\nsource\n\n\n\naexcsc\n\n(aexcsc x)\n\nArc excosecant\nsource\n\n\n\naexsec\n\n(aexsec x)\n\nArc exsecant\nsource\n\n\n\nahacovercos\n\n(ahacovercos x)\n\nArc hacovercosine\nsource\n\n\n\nahacoversin\n\n(ahacoversin x)\n\nArc hacoversine\nsource\n\n\n\nahavercos\n\n(ahavercos x)\n\nArc havecosine\nsource\n\n\n\nahaversin\n\n(ahaversin x)\n\nArc haversine\nsource\n\n\n\napprox\n\n(approx v)\n(approx v digits)\n\nRound v to specified (default: 2) decimal places. Be aware of floating point number accuracy.\nsource\n\n\n\napprox-eq\n\n(approx-eq a b)\n(approx-eq a b digits)\n\nChecks equality approximately up to selected number of digits (2 by default).\nIt can be innacurate due to the algorithm used. Use delta-eq instead.\nSee approx.\nsource\n\n\n\napprox=\nAlias for approx-eq\nsource\n\n\n\nasec\n\n(asec x)\n\nasec(x)\nsource\n\n\n\nasech\n\n(asech x)\n\nArea hyperbolic secant\nsource\n\n\n\nasin\n\n(asin x)\n\nasin(x)\nsource\n\n\n\nasinh\n\n(asinh x)\n\nasinh(x)\nsource\n\n\n\natan\n\n(atan x)\n\natan(x)\nsource\n\n\n\natan2\n\n(atan2 x y)\n\natan2(x,y)\nsource\n\n\n\natanh\n\n(atanh x)\n\natanh(x)\nsource\n\n\n\navercos\n\n(avercos x)\n\nArc vecosine\nsource\n\n\n\naversin\n\n(aversin x)\n\nArc versine\nsource\n\n\n\nbetween-?\n\n(between-? [x y] v)\n(between-? x y v)\n\nCheck if given number is within the range (x,y].\nsource\n\n\n\nbetween?\n\n(between? [x y] v)\n(between? x y v)\n\nCheck if given number is within the range [x,y].\nsource\n\n\n\nbit-and\n\n(bit-and x)\n(bit-and x y)\n(bit-and x y & r)\n\nx ∧ y - bitwise AND\nsource\n\n\n\nbit-and-not\n\n(bit-and-not x)\n(bit-and-not x y)\n(bit-and-not x y & r)\n\nx ∧ ~y - bitwise AND (with complement second argument)\nsource\n\n\n\nbit-clear\n\n(bit-clear x bit)\n\nClear bit (set to 0).\nsource\n\n\n\nbit-flip\n\n(bit-flip x bit)\n\nFlip bit (set to 0 when 1 or to 1 when 0).\nsource\n\n\n\nbit-nand\n\n(bit-nand x)\n(bit-nand x y)\n(bit-nand x y & r)\n\n~(x ∧ y) - bitwise NAND\nsource\n\n\n\nbit-nor\n\n(bit-nor x)\n(bit-nor x y)\n(bit-nor x y & r)\n\n~(x ∨ y) - bitwise NOR\nsource\n\n\n\nbit-not\n\n(bit-not x)\n\n~x - bitwise NOT\nsource\n\n\n\nbit-or\n\n(bit-or x)\n(bit-or x y)\n(bit-or x y & r)\n\nx ∨ y - bitwise OR\nsource\n\n\n\nbit-set\n\n(bit-set x bit)\n\nSet bit (set to 1).\nsource\n\n\n\nbit-shift-left\n\n(bit-shift-left x shift)\n\nShift bits left\nsource\n\n\n\nbit-shift-right\n\n(bit-shift-right x shift)\n\nShift bits right and keep most significant bit unchanged\nsource\n\n\n\nbit-test\n\n(bit-test x bit)\n\nTest bit (return to true when 1 or false when 0).\nsource\n\n\n\nbit-xnor\n\n(bit-xnor x)\n(bit-xnor x y)\n(bit-xnor x y & r)\n\n~(x⊕y) - bitwise XNOR\nsource\n\n\n\nbit-xor\n\n(bit-xor x)\n(bit-xor x y)\n(bit-xor x y & r)\n\nx⊕y - bitwise XOR\nsource\n\n\n\nbits-&gt;double\n\n(bits-&gt;double v)\n\nConvert 64 bits to double\nsource\n\n\n\nbool-not\n\n(bool-not x)\n\nPrimitive boolean not\nsource\n\n\n\nbool-xor\n\n(bool-xor x y)\n(bool-xor x y & r)\n\nPrimitive boolean xor\nsource\n\n\n\ncb\n\n(cb x)\n\nx^3\nsource\n\n\n\ncbrt\n\n(cbrt x)\n\ncubic root, cbrt(x)\nsource\n\n\n\nceil\n\n(ceil x)\n(ceil x scale)\n\nCalculates the ceiling of a number.\nWith a scale argument, rounds to the nearest multiple of scale towards positive infinity.\nSee also: qceil.\nsource\n\n\n\ncexpexp\n\n(cexpexp x)\n\n1-exp(-exp(x))\nsource\n\n\n\ncloglog\n\n(cloglog x)\n\nlog(-log(1-x))\nsource\n\n\n\ncnorm\n\n(cnorm v start1 stop1 start2 stop2)\n(cnorm v start stop)\n\nConstrained version of norm. Result of norm is applied to constrain to [0,1] or [start2,stop2] ranges.\nsource\n\n\n\nco-intervals\n\n(co-intervals data)\n(co-intervals data number)\n(co-intervals data number overlap)\n\nDivides a sequence of numerical data into number (default 6) overlapping intervals.\nThe intervals are constructed such that each contains a similar number of values from the sorted data, aiming to replicate the behavior of R’s co.intervals() function. Invalid doubles (NaN, infinite) are removed from the input data before processing.\nArguments: - data: A collection of numbers. - number: The desired number of intervals (default 6, long). - overlap: The desired overlap proportion between consecutive intervals (default 0.5, double).\nReturns a sequence of intervals, where each interval is represented as a 2-element vector [lower-bound upper-bound].\nsource\n\n\n\ncombinations\n\n(combinations n k)\n\nBinomial coefficient (n choose k)\nsource\n\n\n\nconstrain MACRO\n\n(constrain value mn mx)\n\nClamp value to the range [mn,mx].\nsource\n\n\n\ncopy-sign\n\n(copy-sign magnitude sign)\n\nReturns a value with a magnitude of first argument and sign of second.\nsource\n\n\n\ncos\n\n(cos x)\n\ncos(x)\nsource\n\n\n\ncos-interpolation\n\n(cos-interpolation start stop t)\n\noF interpolateCosine interpolation. See also lerp/mlerp, quad-interpolation or smooth-interpolation.\nsource\n\n\n\ncosh\n\n(cosh x)\n\ncosh(x)\nsource\n\n\n\ncospi\n\n(cospi x)\n\ncos(pi*x)\nsource\n\n\n\ncot\n\n(cot x)\n\ncot(x)\nsource\n\n\n\ncoth\n\n(coth x)\n\nHyperbolic cotangent\nsource\n\n\n\ncotpi\n\n(cotpi x)\n\ncot(pi*x)\nsource\n\n\n\ncovercos\n\n(covercos x)\n\nCovercosine\nsource\n\n\n\ncoversin\n\n(coversin x)\n\nCoversine\nsource\n\n\n\ncrd\n\n(crd x)\n\nChord\nsource\n\n\n\ncsc\n\n(csc x)\n\ncsc(x)\nsource\n\n\n\ncsch\n\n(csch x)\n\nHyperbolic cosecant\nsource\n\n\n\ncscpi\n\n(cscpi x)\n\ncsc(pi*x)\nsource\n\n\n\ncut\n\n(cut data breaks)\n(cut x1 x2 breaks)\n\nDivides a numerical range or a sequence of numbers into a specified number of equally spaced intervals.\nGiven data and breaks, the range is determined by the minimum and maximum finite values in data. Invalid doubles (NaN, infinite) are ignored. Given x1, x2, and breaks, the range is explicitly [x1, x2].\nThe function generates breaks + 1 equally spaced points within the range using slice-range, and then forms breaks intervals from these points.\nThe intervals are returned as a sequence of 2-element vectors [lower-bound upper-bound]. Specifically, if the generated points are p0, p1, ..., p_breaks, the intervals are formed as: [(prev-double p0), p1], [p1, p2], ..., [p_breaks-1, p_breaks](#LOS-(prev-double p0), p1], [p1, p2], ..., [p_breaks-1, p_breaks).\nThis means intervals are generally closed on the right, [a, b], with the first interval’s lower bound slightly adjusted downwards to ensure inclusion of the exact minimum value if necessary due to floating point precision.\nArguments: - data: A collection of numbers (used to determine range). - x1, x2: The start and end points of the range. - breaks: The desired number of intervals (a positive long).\nsource\n\n\n\ndec\n\n(dec x)\n\nPrimitive and inlined dec\nsource\n\n\n\ndeg-in-rad CONST\ndeg-in-rad = 0.017453292519943295\n\\(\\frac{\\pi}{180}\\)\nsource\n\n\n\ndegrees\n\n(degrees rad)\n\nConvert radians into degrees.\nsource\n\n\n\ndelta-eq\n\n(delta-eq a b)\n(delta-eq a b accuracy)\n(delta-eq a b abs-tol rel-tol)\n\nChecks if two floating-point numbers a and b are approximately equal within given tolerances.\nThe check returns true if abs(a - b) is less than a combined tolerance, or if (== a b).\n\n2-arity (delta-eq a b): Uses a default absolute tolerance of 1.0e-6.\n3-arity (delta-eq a b abs-tol): Uses the provided absolute tolerance abs-tol.\n4-arity (delta-eq a b abs-tol rel-tol): Uses both absolute and relative tolerances. The combined tolerance is max(abs-tol, rel-tol * max(abs(a), abs(b))).\n\nThis function is useful for comparing floating-point numbers where exact equality checks (==) may fail due to precision issues.\nsource\n\n\n\ndelta=\nAlias for delta-eq\nsource\n\n\n\ndifference-of-products\n\n(difference-of-products a b c d)\n\nKahan’s algorithm for (ab)-(cd) to avoid catastrophic cancellation.\nsource\n\n\n\ndist\n\n(dist [x1 y1] [x2 y2])\n(dist x1 y1 x2 y2)\n\nEuclidean distance between points (x1,y1) and (x2,y2).\nsource\n\n\n\ndouble-array-&gt;seq\nConvert double array into sequence.\nAlias for seq.\nsource\n\n\n\ndouble-array-type\nsource\n\n\n\ndouble-bits\n\n(double-bits v)\n\nReturns double as 64-bits (long)\nsource\n\n\n\ndouble-double-array-&gt;seq\n\n(double-double-array-&gt;seq res)\n\nConvert double array of double arrays into sequence of sequences.\nsource\n\n\n\ndouble-double-array-type\nsource\n\n\n\ndouble-exponent\n\n(double-exponent v)\n\nExtract exponent information from double\nsource\n\n\n\ndouble-high-bits\n\n(double-high-bits v)\n\nReturns high word from double as bits\nsource\n\n\n\ndouble-low-bits\n\n(double-low-bits v)\n\nReturns low word from double as bits\nsource\n\n\n\ndouble-one-minus-epsilon CONST\ndouble-one-minus-epsilon = 0.9999999999999999\nValue of 0x1.fffffffffffffp-1d = 0.(9)\nsource\n\n\n\ndouble-significand\n\n(double-significand v)\n\nExtract significand from double\nsource\n\n\n\neq\n\n(eq _)\n(eq a b)\n(eq a b & r)\n\nPrimitive math equality function.\nsource\n\n\n\neven?\n\n(even? x)\n\nPrimitive and inlined even?\nsource\n\n\n\nexcsc\n\n(excsc x)\n\nExcosecant\nsource\n\n\n\nexp\n\n(exp x)\n\nexp(x) = e^x\nsource\n\n\n\nexp10\n\n(exp10 x)\n\nexp10(x) = 10^x\nsource\n\n\n\nexp2\n\n(exp2 x)\n\nexp2(x) = 2^x\nsource\n\n\n\nexpexp\n\n(expexp x)\n\nexp(-exp(-x))\nsource\n\n\n\nexpm1\n\n(expm1 x)\n\nexp(x)-1 for small x\nsource\n\n\n\nexprel\n\n(exprel x)\n\n(exp(x)-1)/x\nsource\n\n\n\nexsec\n\n(exsec x)\n\nExsecant\nsource\n\n\n\nfactorial\n\n(factorial n)\n\nFactorial\nsource\n\n\n\nfactorial20\n\n(factorial20 n)\n\nFactorial table up to 20!\nsource\n\n\n\nfalling-factorial\n\n(falling-factorial n x)\n\nFalling (descending) factorial.\nsource\n\n\n\nfalling-factorial-int\n\n(falling-factorial-int n x)\n\nFalling (descending) factorial for integer n.\nsource\n\n\n\nfast* DEPRECATED\nDeprecated: Use * instead\n\n(fast*)\n(fast* a)\n(fast* a b)\n(fast* a b & r)\n\nPrimitive and inlined * as a function\nsource\n\n\n\nfast+ DEPRECATED\nDeprecated: Use + instead\n\n(fast+)\n(fast+ a)\n(fast+ a b)\n(fast+ a b & r)\n\nPrimitive and inlined + as a function\nsource\n\n\n\nfast- DEPRECATED\nDeprecated: Use - instead\n\n(fast-)\n(fast- a)\n(fast- a b)\n(fast- a b & r)\n\nPrimitive and inlined - as a function\nsource\n\n\n\nfast-div DEPRECATED\nDeprecated: Use / instead\n\n(fast-div)\n(fast-div a)\n(fast-div a b)\n(fast-div a b & r)\n\nPrimitive and inlined / as a function\nsource\n\n\n\nfast-identity DEPRECATED\nDeprecated: Use identity-double instead\n\n(fast-identity a)\n\nIdentity on double.\nsource\n\n\n\nfast-max DEPRECATED\nDeprecated: Use max instead\n\n(fast-max a)\n(fast-max a b)\n(fast-max a b & r)\n\nPrimitive and inlined max as a function\nsource\n\n\n\nfast-min DEPRECATED\nDeprecated: Use min instead\n\n(fast-min a)\n(fast-min a b)\n(fast-min a b & r)\n\nPrimitive and inlined min as a function\nsource\n\n\n\nfloor\n\n(floor x)\n(floor x scale)\n\nCalculates the floor of a number.\nWith a scale argument, rounds to the nearest multiple of scale towards negative infinity.\nSee also: qfloor.\nsource\n\n\n\nfma\n\n(fma x y z)\n\n(x y z) -&gt; (+ z (* x y)) or Math/fma for java 9+\nsource\n\n\n\nfpow\n\n(fpow x exponent)\n\nFast version of pow where exponent is integer.\nsource\n\n\n\nfrac\n\n(frac v)\n\nFractional part, always returns values from 0.0 to 1.0 (exclusive). See sfrac for signed version.\nsource\n\n\n\ngcd\n\n(gcd a b)\n\nFast binary greatest common divisor (Stein’s algorithm)\nsource\n\n\n\ngroup-by-intervals\n\n(group-by-intervals coll)\n(group-by-intervals intervals coll)\n\nGroups values from a sequence coll into specified intervals.\nThe function partitions the values in coll based on which interval they fall into. Each interval is a 2-element vector [lower upper]. Values are included in an interval if they are strictly greater than the lower bound and less than or equal to the upper bound, using between-?.\nArguments: - intervals: A sequence of 2-element vectors representing the intervals [lower upper]. - coll: A collection of numerical values to be grouped.\nIf intervals are not provided, the function first calculates overlapping intervals using co-intervals from the values in coll, and then groups the values into these generated intervals.\nReturns a map where keys are the interval vectors and values are sequences of the numbers from coll that fall within that interval.\nsource\n\n\n\nhacovercos\n\n(hacovercos x)\n\nHacovercosine\nsource\n\n\n\nhacoversin\n\n(hacoversin x)\n\nHacoversine\nsource\n\n\n\nhavercos\n\n(havercos x)\n\nHavercosine\nsource\n\n\n\nhaversin\n\n(haversin x)\n(haversin [lat1 lon1] [lat2 lon2])\n(haversin lat1 lon1 lat2 lon2)\n\nHaversine formula for value or lattitude and longitude pairs.\nsource\n\n\n\nhaversine\nHaversine (haversin alias)\nsource\n\n\n\nhaversine-dist\n\n(haversine-dist [lat1 lon1] [lat2 lon2])\n(haversine-dist lat1 lon1 lat2 lon2)\n\nHaversine distance d for r=1\nsource\n\n\n\nhigh-2-exp\n\n(high-2-exp x)\n\nFinds the smallest integer n such that 2^n &gt;= |x|. See low-2-exp.\nsource\n\n\n\nhigh-exp\n\n(high-exp b x)\n\nFinds the smallest integer n such that b^n &gt;= |x|. See also low-exp.\nsource\n\n\n\nhypot\n\n(hypot x y)\n(hypot x y z)\n\nCalculates the Euclidean norm (distance from the origin) for 2 or 3 arguments.\nIt uses a numerically stable algorithm to avoid intermediate overflow or underflow.\nSee also hypot-sqrt for the direct calculation.\nsource\n\n\n\nhypot-sqrt\n\n(hypot-sqrt x y)\n(hypot-sqrt x y z)\n\nCalculates the Euclidean norm (distance from the origin) using a direct sqrt of sum of squares.\nNote: This method can be less numerically stable than hypot for inputs with vastly different magnitudes.\nsource\n\n\n\niabs DEPRECATED\nDeprecated: Use long-abs.\n\n(iabs x)\n\nAbsolute value, long version. See abs.\nsource\n\n\n\nidentity-double\n\n(identity-double a)\n\nIdentity on double.\nsource\n\n\n\nidentity-long\n\n(identity-long a)\n\nIdentity on long.\nsource\n\n\n\ninc\n\n(inc x)\n\nPrimitive and inlined inc\nsource\n\n\n\ninf?\n\n(inf? v)\n\nCheck if a number is an infinite (positive or negative).\nsource\n\n\n\ninteger?\n\n(integer? v)\n\nCheck if given real number is an integer.\nsource\n\n\n\ninv-factorial\n\n(inv-factorial n)\n\nInverse of factorial, 1/n!\nsource\n\n\n\ninvalid-double?\n\n(invalid-double? v)\n\nCheck if a number is not finite double (NaN or ±Inf).\nsource\n\n\n\nitrunc\n\n(itrunc v)\n\nTruncate fractional part, keep sign. Returns long.\nsource\n\n\n\njvm-version CONST\njvm-version = 21\nsource\n\n\n\nlcm\n\n(lcm a b)\n\nFast binary least common multiplier.\nsource\n\n\n\nlerp\n\n(lerp start stop t)\n\nLinear interpolation between start and stop for amount t. See also mlerp, cos-interpolation, quad-interpolation or smooth-interpolation.\nsource\n\n\n\nln\n\n(ln x)\n\nlog(x)=ln(x)\nsource\n\n\n\nlog\n\n(log x)\n(log base x)\n\nlog(x)=ln(x) or logarithm with given base.\nsource\n\n\n\nlog-combinations\n\n(log-combinations n k)\n\nLog of binomial coefficient (n choose k)\nsource\n\n\n\nlog-factorial\n\n(log-factorial x)\n\nLog factorial, alias to log-gamma\nsource\n\n\n\nlog10\n\n(log10 x)\n\nlog_10(x)\nsource\n\n\n\nlog1mexp\n\n(log1mexp x)\n\nlog(1-exp(x)), x&lt;0\nsource\n\n\n\nlog1p\n\n(log1p x)\n\nlog(1+x) for small x\nsource\n\n\n\nlog1pexp\n\n(log1pexp x)\n\nlog(1+exp(x))\nsource\n\n\n\nlog1pmx\n\n(log1pmx x)\n\nlog(1+x)-x\nsource\n\n\n\nlog1psq\n\n(log1psq x)\n\nlog(1+x^2))\nsource\n\n\n\nlog2\n\n(log2 x)\n\nLogarithm with base 2.\n\\(\\ln_2{x}\\)\nsource\n\n\n\nlog2int\n\n(log2int v)\n\nFast and integer version of log2, returns long\nsource\n\n\n\nlog2mexp\n\n(log2mexp x)\n\nlog(2-exp(x))\nsource\n\n\n\nlogaddexp\n\n(logaddexp x y)\n\nlog(exp(x)+exp(y))\nsource\n\n\n\nlogb\n\n(logb b x)\n\nLogarithm with base b.\n\\(\\ln_b{x}\\)\nsource\n\n\n\nlogcosh\n\n(logcosh x)\n\nlog(cosh(x))\nsource\n\n\n\nlogexpm1\n\n(logexpm1 x)\n\nlog(exp(x)-1))\nsource\n\n\n\nlogistic\nAlias for sigmoid\nsource\n\n\n\nlogit\n\n(logit x)\n\nLogit function\nsource\n\n\n\nloglog\n\n(loglog x)\n\n-log(-log(x))\nsource\n\n\n\nlogmxp1\n\n(logmxp1 x)\n\nlog(x)-x+1\nsource\n\n\n\nlogsubexp\n\n(logsubexp x y)\n\nlog(abs(exp(x)-exp(y)))\nsource\n\n\n\nlogsumexp\n\n(logsumexp xs)\n\nlog(exp(x1)+…+exp(xn))\nsource\n\n\n\nlong-abs\n\n(long-abs x)\n\nAbsolut value, long version. See abs.\nsource\n\n\n\nlong-add\n\n(long-add)\n(long-add a)\n(long-add a b)\n(long-add a b c)\n(long-add a b c d)\n(long-add a b c d & r)\n\nPrimitive and inlined +. Coerces arguments and returned value to a long.\nsource\n\n\n\nlong-dec\n\n(long-dec x)\n\nPrimitive and inlined dec coerced to a long\nsource\n\n\n\nlong-div\n\n(long-div a)\n(long-div a b)\n(long-div a b c)\n(long-div a b c d)\n(long-div a b c d & r)\n\nPrimitive and inlined /. Coerces to arguments and returned value to a long.\nsource\n\n\n\nlong-inc\n\n(long-inc x)\n\nPrimitive and inlined inc coerced to a long\nsource\n\n\n\nlong-max\n\n(long-max a)\n(long-max a b)\n(long-max a b c)\n(long-max a b c d)\n(long-max a b c d & r)\n\nPrimitive and inlined max. Coerces arguments and returned values to longs.\nsource\n\n\n\nlong-min\n\n(long-min a)\n(long-min a b)\n(long-min a b c)\n(long-min a b c d)\n(long-min a b c d & r)\n\nPrimitive and inlined min. Coerces arguments and returned values to longs.\nsource\n\n\n\nlong-mod\n\n(long-mod x y)\n\nPrimitive and inlined mod coerced to longs\nsource\n\n\n\nlong-mult\n\n(long-mult)\n(long-mult a)\n(long-mult a b)\n(long-mult a b c)\n(long-mult a b c d)\n(long-mult a b c d & r)\n\nPrimitive and inlined *. Coerces arguments and returned value to a long.\nsource\n\n\n\nlong-quot\n\n(long-quot x y)\n\nPrimitive and inlined quot coerced to longs\nsource\n\n\n\nlong-rem\n\n(long-rem x y)\n\nPrimitive and inlined rem coerced to longs\nsource\n\n\n\nlong-sub\n\n(long-sub a)\n(long-sub a b)\n(long-sub a b c)\n(long-sub a b c d)\n(long-sub a b c d & r)\n\nPrimitive and inlined -. Coerces arguments and returned value to a long.\nsource\n\n\n\nlow-2-exp\n\n(low-2-exp x)\n\nFinds the greatest integer n such that 2^n &lt;= |x|. See high-2-exp.\nsource\n\n\n\nlow-exp\n\n(low-exp b x)\n\nFinds the greatest integer n such that b^n &lt;= |x|. See also high-exp.\nsource\n\n\n\nmake-norm\n\n(make-norm start stop)\n(make-norm start stop dstart dstop)\n\nMake norm function for given range. Resulting function accepts double value (with optional target [dstart,dstop] range) and returns double.\nsource\n\n\n\nmax\n\n(max a)\n(max a b)\n(max a b c)\n(max a b c d)\n(max a b c d & r)\n\nPrimitive and inlined max.\nsource\n\n\n\nmin\n\n(min a)\n(min a b)\n(min a b c)\n(min a b c d)\n(min a b c d & r)\n\nPrimitive and inlined min.\nsource\n\n\n\nmlerp MACRO\n\n(mlerp start stop t)\n\nlerp as macro. For inline code. See also lerp, cos-interpolation, quad-interpolation or smooth-interpolation.\nsource\n\n\n\nmnorm MACRO\n\n(mnorm v start stop)\n(mnorm v start1 stop1 start2 stop2)\n\nMacro version of norm.\nsource\n\n\n\nmod\n\n(mod x y)\n\nPrimitive and inlined mod\nsource\n\n\n\nmpow\n\n(mpow x e m)\n\nCalculates the modular exponentiation \\((x^e \\pmod m)\\).\nComputes the remainder of x raised to the power of e, when divided by m. Uses binary exponentiation.\nx: Base (long). e: Exponent (long, non-negative). m: Modulus (long, positive).\nsource\n\n\n\nmuladd\n\n(muladd x y z)\n\n(x y z) -&gt; (+ z (* x y)) or Math/fma for java 9+\nsource\n\n\n\nnan?\n\n(nan? v)\n\nCheck if a number is a NaN\nsource\n\n\n\nnear-zero?\n\n(near-zero? x)\n(near-zero? x abs-tol)\n(near-zero? x abs-tol rel-tol)\n\nChecks if given value is near zero with absolute (default: 1.0e-6) and/or relative (default 0.0) tolerance.\nsource\n\n\n\nneg-inf?\n\n(neg-inf? v)\n\nCheck if a number is negatively infinite.\nsource\n\n\n\nneg?\n\n(neg? x)\n\nPrimitive and inlined neg?\nsource\n\n\n\nnegative-zero?\n\n(negative-zero? x)\n\nCheck if zero is negative, ie. -0.0\nsource\n\n\n\nnegmuladd\n\n(negmuladd x y z)\n\n(x y z) -&gt; (+ z (* -x y)) or Math/fma for java 9+\nsource\n\n\n\nnext-double\n\n(next-double v)\n(next-double v delta)\n\nNext double value. Optional value delta sets step amount.\nsource\n\n\n\nnorm\n\n(norm v start stop)\n(norm v start1 stop1 start2 stop2)\n\nNormalize v from the range [start,stop] to the range [0,1] or map v from the range [start1,stop1] to the range [start2,stop2]. See also make-norm.\nsource\n\n\n\nnot-neg?\n\n(not-neg? x)\n\nPrimitive and inlined not-neg? (x&gt;=0.0)\nsource\n\n\n\nnot-pos?\n\n(not-pos? x)\n\nPrimitive and inlined not-pos? (x&lt;=0.0)\nsource\n\n\n\nnot-zero?\n\n(not-zero? x)\n\nPrimitive and inlined x&lt;&gt;0.0\nsource\n\n\n\nnot==\n\n(not== _)\n(not== a b)\n(not== a b & r)\n\nNot equality. For more than two arguments, returns true when all values are unique.\n(not== 1 2 1) === (and (not= 1 1) (not= 1 2))\nsource\n\n\n\nodd?\n\n(odd? x)\n\nPrimitive and inlined odd?\nsource\n\n\n\none?\n\n(one? x)\n\nPrimitive and inlined one? (x==1.0)\nsource\n\n\n\norder\n\n(order vs)\n(order vs decreasing?)\n\nComputes the permutation of indices that would sort the input collection vs.\nThe result is a sequence of 0-based indices such that applying them to the original collection using (map #(nth vs %) result) yields a sorted sequence.\nArguments:\n\nvs: A collection of comparable values.\ndecreasing?: Optional boolean (default false). If true, the indices permute for a descending sort.\n\nReturns: A sequence of 0-based indices.\nsource\n\n\n\npos-inf?\n\n(pos-inf? v)\n\nCheck if a number is positively infinite.\nsource\n\n\n\npos?\n\n(pos? x)\n\nPrimitive and inlined pos?\nsource\n\n\n\npow\n\n(pow x exponent)\n\nPower of a number\nsource\n\n\n\npow10\n\n(pow10 x)\n\nx^10\nsource\n\n\n\npow2\n\n(pow2 x)\n\nx^2, x*x\nsource\n\n\n\npow3\n\n(pow3 x)\n\nx^3\nsource\n\n\n\nprev-double\n\n(prev-double v)\n(prev-double v delta)\n\nNext double value. Optional value delta sets step amount.\nsource\n\n\n\nqceil\n\n(qceil x)\n\nFast version of ceil. Returns long.\nsource\n\n\n\nqcos\n\n(qcos x)\n\nFast and less accurate cos(x)\nsource\n\n\n\nqdist\n\n(qdist [x1 y1] [x2 y2])\n(qdist x1 y1 x2 y2)\n\nQuick version of Euclidean distance between points. qsqrt is used instead of sqrt.\nsource\n\n\n\nqexp\n\n(qexp x)\n\nQuick and less accurate version of exp.\nsource\n\n\n\nqfloor\n\n(qfloor x)\n\nFast version of floor. Returns long.\nsource\n\n\n\nqlog\n\n(qlog x)\n\nFast and less accurate version of log.\nsource\n\n\n\nqpow\n\n(qpow x exponent)\n\nFast and less accurate version of pow.\nsource\n\n\n\nqround\n\n(qround x)\n\nFast version of round. Returns long\nsource\n\n\n\nqsin\n\n(qsin x)\n\nFast and less accurate sin(x)\nsource\n\n\n\nqsqrt\n\n(qsqrt x)\n\nApproximated sqrt using binary operations with error 1.0E-2.\nsource\n\n\n\nquad-interpolation\n\n(quad-interpolation start stop t)\n\nQuad interpolation. See also lerp/mlerp, cos-interpolation or smooth-interpolation.\nsource\n\n\n\nquot\n\n(quot x y)\n\nPrimitive and inlined quot\nsource\n\n\n\nrad-in-deg CONST\nrad-in-deg = 57.29577951308232\n\\(\\frac{180}{\\pi}\\)\nsource\n\n\n\nradians\n\n(radians deg)\n\nConvert degrees into radians.\nsource\n\n\n\nrank\n\n(rank vs)\n(rank vs ties)\n(rank vs ties desc?)\n\nAssigns ranks to values in a collection, handling ties according to a specified strategy. Ranks are 0-based indices indicating the position of each element in the sorted collection.\nArguments:\n\nvs: A collection of comparable values.\nties: The tie-breaking strategy (keyword, optional, default :average). Supported strategies:\n\n:average: Assign the average rank to all tied values.\n:first: Assign ranks based on their appearance order in the input.\n:last: Assign ranks based on their appearance order in the input (reverse of :first).\n:random: Assign random ranks to tied values.\n:min: Assign the minimum rank to all tied values.\n:max: Assign the maximum rank to all tied values.\n:dense: Assign consecutive ranks without gaps (like data.table::frank in R).\n\ndesc?: If true, rank in descending order (boolean, optional, default false).\n\nReturns a sequence of rank values corresponding to the input elements.\nsource\n\n\n\nrank1\nsource\n\n\n\nrelative-error\n\n(relative-error v v-approx)\n\nRelative error between two values\nsource\n\n\n\nrem\n\n(rem x y)\n\nPrimitive and inlined rem\nsource\n\n\n\nremainder\n\n(remainder dividend divisor)\n\nFrom FastMath doc: returns dividend - divisor * n, where n is the mathematical integer closest to dividend/divisor. Returned value in [-|divisor|/2,|divisor|/2]\nsource\n\n\n\nrint\n\n(rint x)\n(rint x scale)\n\nRound to a double value. See round, qround.\nRounding is done to a multiply of scale value (when provided).\nsource\n\n\n\nrising-factorial\n\n(rising-factorial n x)\n\nRising (Pochhammer) factorial.\nsource\n\n\n\nrising-factorial-int\n\n(rising-factorial-int n x)\n\nRising (Pochhammer) factorial for integer n.\nsource\n\n\n\nround\n\n(round x)\n\nRound to a long value. See: rint, qround.\nsource\n\n\n\nround-even\n\n(round-even x)\n\nRound evenly, IEEE / IEC rounding\nsource\n\n\n\nround-up-pow2\n\n(round-up-pow2 v)\n\nRounds a positive long integer up to the smallest power of 2 greater than or equal to the input value.\nsource\n\n\n\nrqsqrt\n\n(rqsqrt x)\n\nReciprocal of qsqrt. Quick and less accurate.\nsource\n\n\n\nsafe-sqrt\n\n(safe-sqrt value)\n\nSafe sqrt, for value &lt;= 0 result is 0.\nsource\n\n\n\nsample\n\n(sample f number-of-values)\n(sample f number-of-values domain?)\n(sample f domain-min domain-max number-of-values)\n(sample f domain-min domain-max number-of-values domain?)\n\nSamples a function f by evaluating it at evenly spaced points within a numerical range.\nGenerates number-of-values points in the specified range [domain-min, domain-max] (inclusive) and applies f to each point.\nArguments:\n\nf: The function to sample. Should accept a single double argument.\nnumber-of-values: The total number of points to generate (a positive long).\ndomain-min: The lower bound of the sampling range (a double). Defaults to 0.0.\ndomain-max: The upper bound of the sampling range (a double). Defaults to 1.0.\ndomain?: A boolean flag. If true, returns pairs [x, (f x)]. If false (default), returns just (f x).\n\nArities:\n\n[f number-of-values]: Samples f in [0.0, 1.0]. Returns (f x) values.\n[f number-of-values domain?]: Samples f in [0.0, 1.0]. Returns [x, (f x)] pairs if domain? is true, otherwise (f x) values.\n[f domain-min domain-max number-of-values]: Samples f in [domain-min, domain-max]. Returns (f x) values.\n[f domain-min domain-max number-of-values domain?]: Samples f in [domain-min, domain-max]. Returns [x, (f x)] pairs if domain? is true, otherwise (f x) values.\n\nThe points are generated linearly from domain-min to domain-max. If number-of-values is 1, it samples only the midpoint of the range.\nReturns a sequence of double values or vectors [double, double] depending on domain?.\nsource\n\n\n\nsec\n\n(sec x)\n\nsec(x)\nsource\n\n\n\nsech\n\n(sech x)\n\nHyperbolic secant\nsource\n\n\n\nsecpi\n\n(secpi x)\n\nsec(pi*x)\nsource\n\n\n\nseq-&gt;double-array\n\n(seq-&gt;double-array vs)\n\nConvert sequence to double array. Returns input if vs is double array already.\nsource\n\n\n\nseq-&gt;double-double-array\n\n(seq-&gt;double-double-array vss)\n\nConvert sequence to double-array of double-arrays.\nIf sequence is double-array of double-arrays returns vss\nsource\n\n\n\nsfrac\n\n(sfrac v)\n\nFractional part, always returns values from -1.0 to 1.0 (exclusive). See frac for unsigned version.\nsource\n\n\n\nsgn\n\n(sgn value)\n\nReturns -1 when value is negative, 1 otherwise. See also signum.\nsource\n\n\n\nsigmoid\n\n(sigmoid x)\n\nSigmoid function\nsource\n\n\n\nsignum\n\n(signum value)\n\nReturns 1 if value is &gt; 0, 0 if it is 0, -1 otherwise. See also sgn.\nsource\n\n\n\nsin\n\n(sin x)\n\nsin(x)\nsource\n\n\n\nsinc\n\n(sinc v)\n\nSinc function.\nsource\n\n\n\nsinh\n\n(sinh x)\n\nsinh(x)\nsource\n\n\n\nsinpi\n\n(sinpi x)\n\nsin(pi*x)\nsource\n\n\n\nslice-range\n\n(slice-range start end cnt)\n(slice-range cnt)\n\nGenerates cnt evenly spaced points within a numerical range.\n\n(slice-range cnt): Generates cnt points in the range [0.0, 1.0].\n(slice-range start end cnt): Generates cnt points in the range [start, end].\n\nThe range is inclusive, meaning the first point is start and the last is end (unless cnt is 1). If cnt is 1, it returns a single value which is the midpoint (start + end) / 2 of the range. Returns a sequence of cnt double values.\nsource\n\n\n\nsmooth-interpolation\n\n(smooth-interpolation start stop t)\n\nSmoothstep based interpolation. See also lerp/mlerp, quad-interpolation or cos-interpolation.\nsource\n\n\n\nsmooth-max\n\n(smooth-max xs)\n(smooth-max xs alpha)\n(smooth-max xs alpha family)\n\nSmooth maximum function.\nA smooth function with alpha argument. When alpha goes to infinity, function returns maximum value of xs.\nFamily:\n\n:lse - LogSumExp (default)\n:boltzmann - Boltzmann operator, works for small alpha values\n:mellowmax\n:p-norm\n:smu - smooth maximum unit, epsilon = 1/alpha &gt; 0\n\nsource\n\n\n\nsmoothstep\n\n(smoothstep edge0 edge1 x)\n\nGL smoothstep.\nsource\n\n\n\nspow\n\n(spow x exponent)\n\nSymmetric power of a number (keeping a sign of the argument.\nsource\n\n\n\nsq\n\n(sq x)\n\nx^2, x*x\nsource\n\n\n\nsqrt\n\n(sqrt x)\n\nsquare root, sqrt(x)\nsource\n\n\n\nsum-of-products\n\n(sum-of-products a b c d)\n\nKahan’s algorithm for (ab)+(cd) to avoid catastrophic cancellation.\nsource\n\n\n\ntan\n\n(tan x)\n\ntan(x)\nsource\n\n\n\ntanh\n\n(tanh x)\n\ntanh(x)\nsource\n\n\n\ntanpi\n\n(tanpi x)\n\ntan(pi*x)\nsource\n\n\n\ntrunc\n\n(trunc v)\n\nTruncate fractional part, keep sign. Returns double.\nsource\n\n\n\nulp\n\n(ulp x)\n\nUnit in the Last Place, distance between next value larger than x and x\nsource\n\n\n\nunsigned-bit-shift-right\n\n(unsigned-bit-shift-right x shift)\n\nShift bits right and set most significant bit to 0\nsource\n\n\n\nunuse-primitive-operators\n\n(unuse-primitive-operators)\n(unuse-primitive-operators skip-set)\n\nUndoes the work of use-primitive-operators. This is idempotent.\nsource\n\n\n\nuse-primitive-operators\n\n(use-primitive-operators)\n(use-primitive-operators skip-set)\n\nReplaces Clojure’s arithmetic and number coercion functions with primitive equivalents. These are defined as macros, so they cannot be used as higher-order functions. This is an idempotent operation. Undo with unuse-primitive-operators.\nsource\n\n\n\nvalid-double?\n\n(valid-double? v)\n\nCheck if a number is finite double.\nsource\n\n\n\nvercos\n\n(vercos x)\n\nVercosine\nsource\n\n\n\nversin\n\n(versin x)\n\nVersine\nsource\n\n\n\nwrap\n\n(wrap [start stop] value)\n(wrap start stop value)\n\nWrap overflowed value into the range, similar to ofWrap.\nsource\n\n\n\nxexpx\n\n(xexpx x)\n\nx * exp(x)\nsource\n\n\n\nxexpy\n\n(xexpy x y)\n\nx * exp(x)\nsource\n\n\n\nxlog1py\n\n(xlog1py x y)\n\nx * log(1+y)\nsource\n\n\n\nxlogx\n\n(xlogx x)\n\nx * log(x)\nsource\n\n\n\nxlogy\n\n(xlogy x y)\n\nx * log(y)\nsource\n\n\n\nxor\n\n(xor x y)\n(xor x y & r)\n\nPrimitive boolean xor\nsource\n\n\n\nzero?\n\n(zero? x)\n\nPrimitive and inlined zero?\nsource\n\nsource: clay/core.clj",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "vector_matrix.html",
    "href": "vector_matrix.html",
    "title": "Vectors and matrices",
    "section": "",
    "text": "Vectors",
    "crumbs": [
      "Vectors and matrices"
    ]
  },
  {
    "objectID": "vector_matrix.html#vectors",
    "href": "vector_matrix.html#vectors",
    "title": "Vectors and matrices",
    "section": "",
    "text": "Reference\n\nfastmath.vector\nMathematical vector operations.\n### Types\n\nFixed size (custom types):\n\nNumber - 1d vector\nVec2 - 2d vector, creator vec2\nVec3 - 3d vector, creator vec3\nVec4 - 4d vector, creator vec4\nArrayVec - fixed size double array wrapper, n-dimensional, creator array-vec\n\nFixed size\n\ndoubles - double array itself\n\nVariable size:\n\nClojure’s IPersistentVector, creator []\nClojure’s ISeq\n\n\nVectorProto defines most of the functions.\nVectors implements also:\n\nSequable\nSequencial\nIFn\nCounted\nReversible\nIndexed\nILookup\nequals and toString from Object\nIPersistentVector\nAssociative\nclojure.core.matrix.protocols\nIReduce and IReduceInit\n\nThat means that vectors can be destructured, treated as sequence or called as a function. See vec2 for examples.\n\n\n-&gt;ArrayVec\n\n(-&gt;ArrayVec array)\n\nPositional factory function for class fastmath.vector.ArrayVec.\nsource\n\n\n\n-&gt;Vec2\n\n(-&gt;Vec2 x y)\n\nPositional factory function for class fastmath.vector.Vec2.\nsource\n\n\n\n-&gt;Vec3\n\n(-&gt;Vec3 x y z)\n\nPositional factory function for class fastmath.vector.Vec3.\nsource\n\n\n\n-&gt;Vec4\n\n(-&gt;Vec4 x y z w)\n\nPositional factory function for class fastmath.vector.Vec4.\nsource\n\n\n\nabs\n\n(abs v)\n\nAbsolute value of vector elements\nsource\n\n\n\nacos\n\n(acos vector)\n\nApply acos to vector elements.\nsource\n\n\n\nacosh\n\n(acosh vector)\n\nApply acosh to vector elements.\nsource\n\n\n\nacot\n\n(acot vector)\n\nApply acot to vector elements.\nsource\n\n\n\nacoth\n\n(acoth vector)\n\nApply acoth to vector elements.\nsource\n\n\n\nacsc\n\n(acsc vector)\n\nApply acsc to vector elements.\nsource\n\n\n\nacsch\n\n(acsch vector)\n\nApply acsch to vector elements.\nsource\n\n\n\nadd\n\n(add v)\n(add v1 v2)\n\nSum of two vectors.\nsource\n\n\n\naligned?\n\n(aligned? v1 v2 tol)\n(aligned? v1 v2)\n\nAre vectors aligned (have the same direction)?\nsource\n\n\n\nangle-between\n\n(angle-between v1 v2)\n\nAngle between two vectors\nSee also relative-angle-between.\nsource\n\n\n\napplyf DEPRECATED\nDeprecated: v1.3.0\nSame as fmap. Deprecated.\nsource\n\n\n\napprox\n\n(approx v)\n(approx v d)\n\nRound to 2 (or d) decimal places\nsource\n\n\n\narray-&gt;vec2\n\n(array-&gt;vec2 arr)\n\nDoubles array to Vec2\nsource\n\n\n\narray-&gt;vec3\n\n(array-&gt;vec3 arr)\n\nDoubles array to Vec3\nsource\n\n\n\narray-&gt;vec4\n\n(array-&gt;vec4 arr)\n\nDoubles array to Vec4\nsource\n\n\n\narray-vec\n\n(array-vec xs)\n\nMake ArrayVec type based on provided sequence xs.\nsource\n\n\n\nas-vec\n\n(as-vec v)\n(as-vec v xs)\n\nCreate vector from sequence as given type. If there is no sequence fill with 0.0.\nsource\n\n\n\nasec\n\n(asec vector)\n\nApply asec to vector elements.\nsource\n\n\n\nasech\n\n(asech vector)\n\nApply asech to vector elements.\nsource\n\n\n\nasin\n\n(asin vector)\n\nApply asin to vector elements.\nsource\n\n\n\nasinh\n\n(asinh vector)\n\nApply asinh to vector elements.\nsource\n\n\n\natan\n\n(atan vector)\n\nApply atan to vector elements.\nsource\n\n\n\natanh\n\n(atanh vector)\n\nApply atanh to vector elements.\nsource\n\n\n\naverage\n\n(average v)\n(average v weights)\n\nMean or weighted average of the vector\nsource\n\n\n\naverage-vectors\n\n(average-vectors init vs)\n(average-vectors vs)\n\nAverage / centroid of vectors. Input: initial vector (optional), list of vectors\nsource\n\n\n\naxis-rotate\n\n(axis-rotate v angle axis)\n(axis-rotate v angle axis pivot)\n\nRotate vector. Only for Vec3 types\nsource\n\n\n\nbase-from\n\n(base-from v)\n\nList of perpendicular vectors (basis). Works only for Vec2 and Vec3 types.\nsource\n\n\n\ncb\n\n(cb vector)\n\nApply cb to vector elements.\nsource\n\n\n\ncbrt\n\n(cbrt vector)\n\nApply cbrt to vector elements.\nsource\n\n\n\nceil\n\n(ceil vector)\n\nApply ceil to vector elements.\nsource\n\n\n\nclamp\n\n(clamp v mn mx)\n(clamp v)\n\nClamp elements.\nsource\n\n\n\ncos\n\n(cos vector)\n\nApply cos to vector elements.\nsource\n\n\n\ncosh\n\n(cosh vector)\n\nApply cosh to vector elements.\nsource\n\n\n\ncot\n\n(cot vector)\n\nApply cot to vector elements.\nsource\n\n\n\ncoth\n\n(coth vector)\n\nApply coth to vector elements.\nsource\n\n\n\ncross\n\n(cross v1 v2)\n\nCross product\nsource\n\n\n\ncsc\n\n(csc vector)\n\nApply csc to vector elements.\nsource\n\n\n\ncsch\n\n(csch vector)\n\nApply csch to vector elements.\nsource\n\n\n\ndegrees\n\n(degrees vector)\n\nApply degrees to vector elements.\nsource\n\n\n\ndelta-eq\n\n(delta-eq v1 v2)\n(delta-eq v1 v2 abs-tol)\n(delta-eq v1 v2 abs-tol rel-tol)\n\nEquality with given absolute (and/or relative) toleance.\nsource\n\n\n\ndhash-code\n\n(dhash-code state a)\n(dhash-code a)\n\ndouble hashcode\nsource\n\n\n\ndist\n\n(dist v1 v2)\n\nEuclidean distance between vectors\nsource\n\n\n\ndist-abs\n\n(dist-abs v1 v2)\n\nManhattan distance between vectors\nsource\n\n\n\ndist-ang\n\n(dist-ang v1 v2)\n\nAngular distance\nsource\n\n\n\ndist-canberra\n\n(dist-canberra v1 v2)\n\nCanberra distance\nsource\n\n\n\ndist-cheb\n\n(dist-cheb v1 v2)\n\nChebyshev distance between 2d vectors\nsource\n\n\n\ndist-discrete\n\n(dist-discrete v1 v2)\n\nDiscrete distance between 2d vectors\nsource\n\n\n\ndist-emd\n\n(dist-emd v1 v2)\n\nEarth Mover’s Distance\nsource\n\n\n\ndist-sq\n\n(dist-sq v1 v2)\n\nSquared Euclidean distance between vectors\nsource\n\n\n\ndistances\nsource\n\n\n\ndiv\n\n(div v1 v)\n(div v1)\n\nVector division or reciprocal.\nsource\n\n\n\ndot\n\n(dot v1 v2)\n\nDot product of two vectors.\nsource\n\n\n\neconstrain\n\n(econstrain v mn mx)\n\nElement-wise constrain\nsource\n\n\n\nedelta-eq\n\n(edelta-eq v1 v2)\n(edelta-eq v1 v2 abs-tol)\n(edelta-eq v1 v2 abs-tol rel-tol)\n\nElement-wise equality with given absolute (and/or relative) toleance.\nsource\n\n\n\nediv\n\n(ediv v1 v2)\n\nElement-wise division of two vectors.\nsource\n\n\n\neinterpolate\n\n(einterpolate v1 v2 v)\n(einterpolate v1 v2 v f)\n\nInterpolate vector selement-wise, optionally set interpolation fn (default: lerp)\nsource\n\n\n\nemn\n\n(emn v1 v2)\n\nElement-wise min from two vectors.\nsource\n\n\n\nemult\n\n(emult v1 v2)\n\nElement-wise vector multiplication (Hadamard product).\nsource\n\n\n\nemx\n\n(emx v1 v2)\n\nElement-wise max from two vectors.\nsource\n\n\n\nexp\n\n(exp vector)\n\nApply exp to vector elements.\nsource\n\n\n\nexpm1\n\n(expm1 vector)\n\nApply expm1 to vector elements.\nsource\n\n\n\nfaceforward\n\n(faceforward n v)\n\nFlip normal n to match the same direction as v.\nsource\n\n\n\nfloor\n\n(floor vector)\n\nApply floor to vector elements.\nsource\n\n\n\nfmap\n\n(fmap v f)\n\nApply function to all vector values (like map but returns the same type).\nsource\n\n\n\nfrac\n\n(frac vector)\n\nApply frac to vector elements.\nsource\n\n\n\nfrom-polar\n\n(from-polar v)\n\nFrom polar coordinates (2d, 3d only)\nsource\n\n\n\ngenerate-vec2\n\n(generate-vec2 f1 f2)\n(generate-vec2 f)\n\nGenerate Vec2 with fn(s)\nsource\n\n\n\ngenerate-vec3\n\n(generate-vec3 f1 f2 f3)\n(generate-vec3 f)\n\nGenerate Vec3 with fn(s)\nsource\n\n\n\ngenerate-vec4\n\n(generate-vec4 f1 f2 f3 f4)\n(generate-vec4 f)\n\nGenerate Vec4 with fn(s)\nsource\n\n\n\nheading\n\n(heading v)\n\nAngle between vector and unit vector [1,0,...]\nsource\n\n\n\ninterpolate\n\n(interpolate v1 v2 t)\n(interpolate v1 v2 t f)\n\nInterpolate vectors, optionally set interpolation fn (default: lerp)\nsource\n\n\n\nis-near-zero?\n\n(is-near-zero? v)\n(is-near-zero? v abs-tol)\n(is-near-zero? v abs-tol rel-tol)\n\nEquality to zero 0 with given absolute (and/or relative) toleance.\nsource\n\n\n\nis-zero?\n\n(is-zero? v)\n\nIs vector zero?\nsource\n\n\n\nlerp\n\n(lerp v1 v2 t)\n\nLinear interpolation of vectors\nsource\n\n\n\nlimit\n\n(limit v len)\n\nLimit length of the vector by given value\nsource\n\n\n\nln\n\n(ln vector)\n\nApply ln to vector elements.\nsource\n\n\n\nlog\n\n(log vector)\n\nApply log to vector elements.\nsource\n\n\n\nlog10\n\n(log10 vector)\n\nApply log10 to vector elements.\nsource\n\n\n\nlog1mexp\n\n(log1mexp vector)\n\nApply log1mexp to vector elements.\nsource\n\n\n\nlog1p\n\n(log1p vector)\n\nApply log1p to vector elements.\nsource\n\n\n\nlog1pexp\n\n(log1pexp vector)\n\nApply log1pexp to vector elements.\nsource\n\n\n\nlog1pmx\n\n(log1pmx vector)\n\nApply log1pmx to vector elements.\nsource\n\n\n\nlog1psq\n\n(log1psq vector)\n\nApply log1psq to vector elements.\nsource\n\n\n\nlog2\n\n(log2 vector)\n\nApply log2 to vector elements.\nsource\n\n\n\nlogexpm1\n\n(logexpm1 vector)\n\nApply logexpm1 to vector elements.\nsource\n\n\n\nlogit\n\n(logit vector)\n\nApply logit to vector elements.\nsource\n\n\n\nlogmeanexp\n\n(logmeanexp v)\n\nsource\n\n\n\nlogmxp1\n\n(logmxp1 vector)\n\nApply logmxp1 to vector elements.\nsource\n\n\n\nlogsoftmax\n\n(logsoftmax v)\n(logsoftmax v t)\n\nsource\n\n\n\nlogsumexp\n\n(logsumexp v)\n\nsource\n\n\n\nmag\n\n(mag v)\n\nLength of the vector.\nsource\n\n\n\nmagsq\n\n(magsq v)\n\nLength of the vector squared.\nsource\n\n\n\nmake-vector\n\n(make-vector dims xs)\n(make-vector dims)\n\nReturns fixed size vector for given number of dimensions.\nProper type is used.\nsource\n\n\n\nmaxdim\n\n(maxdim v)\n\nIndex of maximum value.\nsource\n\n\n\nmindim\n\n(mindim v)\n\nIndex of minimum value.\nsource\n\n\n\nmn\n\n(mn v)\n\nMinimum value of vector elements\nsource\n\n\n\nmult\n\n(mult v x)\n\nMultiply vector by number x.\nsource\n\n\n\nmx\n\n(mx v)\n\nMaximum value of vector elements\nsource\n\n\n\nnear-zero?\n\n(near-zero? v)\n(near-zero? v abs-tol)\n(near-zero? v abs-tol rel-tol)\n\nEquality to zero 0 with given absolute (and/or relative) toleance.\nsource\n\n\n\nnonzero-count\n\n(nonzero-count v)\n\nCount non zero velues in vector\nsource\n\n\n\nnormalize\n\n(normalize v)\n\nNormalize vector (set length = 1.0)\nsource\n\n\n\northogonal-polynomials\n\n(orthogonal-polynomials xs)\n\nCreates orthogonal list of vectors based on xs, starting from degree 1\nsource\n\n\n\northonormal-polynomials\n\n(orthonormal-polynomials xs)\n\nCreates orthonormal list of vector based on xs, starting from degree 1\nsource\n\n\n\npermute\n\n(permute v idxs)\n\nPermute vector elements with given indices.\nsource\n\n\n\nperpendicular\n\n(perpendicular v)\n(perpendicular v1 v2)\n\nPerpendicular vector. Only for Vec2 and Vec3 types.\nsource\n\n\n\nprod\n\n(prod v)\n\nProduct of elements\nsource\n\n\n\nproject\n\n(project v1 v2)\n\nProject v1 onto v2\nsource\n\n\n\nradians\n\n(radians vector)\n\nApply radians to vector elements.\nsource\n\n\n\nreciprocal\n\n(reciprocal v)\n\nReciprocal of elements.\nsource\n\n\n\nrelative-angle-between\n\n(relative-angle-between v1 v2)\n\nAngle between two vectors relative to each other.\nSee also angle-between.\nsource\n\n\n\nrint\n\n(rint vector)\n\nApply rint to vector elements.\nsource\n\n\n\nrotate\n\n(rotate v angle)\n(rotate v angle-x angle-y angle-z)\n\nRotate vector. Only for Vec2 and Vec3 types.\nsource\n\n\n\nround\n\n(round vector)\n\nApply round to vector elements.\nsource\n\n\n\nsafe-sqrt\n\n(safe-sqrt vector)\n\nApply safe-sqrt to vector elements.\nsource\n\n\n\nsec\n\n(sec vector)\n\nApply sec to vector elements.\nsource\n\n\n\nsech\n\n(sech vector)\n\nApply sech to vector elements.\nsource\n\n\n\nseq-&gt;vec2\n\n(seq-&gt;vec2 xs)\n\nAny seq to Vec2\nsource\n\n\n\nseq-&gt;vec3\n\n(seq-&gt;vec3 xs)\n\nAny seq to Vec3\nsource\n\n\n\nseq-&gt;vec4\n\n(seq-&gt;vec4 xs)\n\nAny seq to Vec4\nsource\n\n\n\nset-mag\n\n(set-mag v len)\n\nSet length of the vector\nsource\n\n\n\nsfrac\n\n(sfrac vector)\n\nApply sfrac to vector elements.\nsource\n\n\n\nsgn\n\n(sgn vector)\n\nApply sgn to vector elements.\nsource\n\n\n\nshift\n\n(shift v)\n(shift v x)\n\nAdd value to every vector element.\nsource\n\n\n\nsigmoid\n\n(sigmoid vector)\n\nApply sigmoid to vector elements.\nsource\n\n\n\nsignum\n\n(signum vector)\n\nApply signum to vector elements.\nsource\n\n\n\nsim-cos\n\n(sim-cos v1 v2)\n\nCosine similarity\nsource\n\n\n\nsin\n\n(sin vector)\n\nApply sin to vector elements.\nsource\n\n\n\nsinc\n\n(sinc vector)\n\nApply sinc to vector elements.\nsource\n\n\n\nsinh\n\n(sinh vector)\n\nApply sinh to vector elements.\nsource\n\n\n\nsize\n\n(size v)\n\nLength of the vector.\nsource\n\n\n\nsoftmax\n\n(softmax v)\n(softmax v t)\n\nsource\n\n\n\nsq\n\n(sq vector)\n\nApply sq to vector elements.\nsource\n\n\n\nsqrt\n\n(sqrt vector)\n\nApply sqrt to vector elements.\nsource\n\n\n\nsub\n\n(sub v)\n(sub v1 v2)\n\nSubtraction of two vectors.\nsource\n\n\n\nsum\n\n(sum v)\n\nSum of elements\nsource\n\n\n\ntan\n\n(tan vector)\n\nApply tan to vector elements.\nsource\n\n\n\ntanh\n\n(tanh vector)\n\nApply tanh to vector elements.\nsource\n\n\n\nto-polar\n\n(to-polar v)\n\nTo polar coordinates (2d, 3d only), first element is length, the rest angle.\nsource\n\n\n\nto-vec DEPRECATED\nDeprecated: v1.5.0\nSame as vec-&gt;Vec. Deprecated.\nsource\n\n\n\ntransform\n\n(transform v o vx vy)\n(transform v o vx vy vz)\n\nTransform vector; map point to coordinate system defined by origin, vx and vy (as bases), Only for Vec2 and Vec3 types.\nsource\n\n\n\ntriple-product\n\n(triple-product a b c)\n\na o (b x c)\nsource\n\n\n\ntrunc\n\n(trunc vector)\n\nApply trunc to vector elements.\nsource\n\n\n\nvec-&gt;RealVector\n\n(vec-&gt;RealVector v)\n\nConvert to Apache Commons Math RealVector\nsource\n\n\n\nvec-&gt;Vec\n\n(vec-&gt;Vec v)\n\nConvert to Clojure primitive vector Vec.\nsource\n\n\n\nvec-&gt;array\n\n(vec-&gt;array v)\n\nConvert to double array\nsource\n\n\n\nvec-&gt;seq\n\n(vec-&gt;seq v)\n\nConvert to sequence (same as seq)\nsource\n\n\n\nvec2\n\n(vec2 x y)\n(vec2 [x y])\n(vec2)\n\nMake 2d vector.\nsource\n\n\n\nvec3\n\n(vec3 x y z)\n(vec3 v z)\n(vec3 [x y z])\n(vec3)\n\nMake Vec2 vector\nsource\n\n\n\nvec4\n\n(vec4 x y z w)\n(vec4 v w)\n(vec4 v z w)\n(vec4 [x y z w])\n(vec4)\n\nMake Vec4 vector\nsource\n\n\n\nxlogx\n\n(xlogx vector)\n\nApply xlogx to vector elements.\nsource\n\n\n\nzero-count\n\n(zero-count v)\n\nCount zeros in vector\nsource\n\n\n\nzero?\n\n(zero? v)\n\nIs vector zero?\nsource",
    "crumbs": [
      "Vectors and matrices"
    ]
  },
  {
    "objectID": "vector_matrix.html#matrices",
    "href": "vector_matrix.html#matrices",
    "title": "Vectors and matrices",
    "section": "Matrices",
    "text": "Matrices\n\nReference\n\nfastmath.matrix\nFixed size (2x2, 3x3, 4x4) matrix types.\n\n\n-&gt;Mat2x2\n\n(-&gt;Mat2x2 a00 a01 a10 a11)\n\nPositional factory function for class fastmath.matrix.Mat2x2.\nsource\n\n\n\n-&gt;Mat3x3\n\n(-&gt;Mat3x3 a00 a01 a02 a10 a11 a12 a20 a21 a22)\n\nPositional factory function for class fastmath.matrix.Mat3x3.\nsource\n\n\n\n-&gt;Mat4x4\n\n(-&gt;Mat4x4 a00 a01 a02 a03 a10 a11 a12 a13 a20 a21 a22 a23 a30 a31 a32 a33)\n\nPositional factory function for class fastmath.matrix.Mat4x4.\nsource\n\n\n\nacos\n\n(acos vector)\n\nApply acos to matrix elements.\nsource\n\n\n\nacosh\n\n(acosh vector)\n\nApply acosh to matrix elements.\nsource\n\n\n\nacot\n\n(acot vector)\n\nApply acot to matrix elements.\nsource\n\n\n\nacoth\n\n(acoth vector)\n\nApply acoth to matrix elements.\nsource\n\n\n\nacsc\n\n(acsc vector)\n\nApply acsc to matrix elements.\nsource\n\n\n\nacsch\n\n(acsch vector)\n\nApply acsch to matrix elements.\nsource\n\n\n\nadd\n\n(add A)\n(add A B)\n\nAdd matrices, C=A+B.\nsource\n\n\n\nadds\n\n(adds A s)\n\nAdd scalar to all matrix elements\nsource\n\n\n\nasec\n\n(asec vector)\n\nApply asec to matrix elements.\nsource\n\n\n\nasech\n\n(asech vector)\n\nApply asech to matrix elements.\nsource\n\n\n\nasin\n\n(asin vector)\n\nApply asin to matrix elements.\nsource\n\n\n\nasinh\n\n(asinh vector)\n\nApply asinh to matrix elements.\nsource\n\n\n\natan\n\n(atan vector)\n\nApply atan to matrix elements.\nsource\n\n\n\natanh\n\n(atanh vector)\n\nApply atanh to matrix elements.\nsource\n\n\n\ncb\n\n(cb vector)\n\nApply cb to matrix elements.\nsource\n\n\n\ncbrt\n\n(cbrt vector)\n\nApply cbrt to matrix elements.\nsource\n\n\n\nceil\n\n(ceil vector)\n\nApply ceil to matrix elements.\nsource\n\n\n\ncholesky\n\n(cholesky A)\n(cholesky A upper?)\n\nCalculate L (lower by default) triangular for where L * L^T = A.\nChecks only for symmetry, can return NaNs when A is not positive-definite.\nsource\n\n\n\ncol\n\n(col A c)\n\nReturn column as a vector\nsource\n\n\n\ncols\n\n(cols A)\n\nReturn matrix columns\nsource\n\n\n\ncols-&gt;RealMatrix\n\n(cols-&gt;RealMatrix cols)\n\nReturn Apache Commons Math Array2DRowMatrix from sequence of columns\nsource\n\n\n\ncols-&gt;mat\n\n(cols-&gt;mat real-matrix-cols)\n(cols-&gt;mat [a00 a10] [a01 a11])\n(cols-&gt;mat [a00 a10 a20] [a01 a11 a21] [a02 a12 a22])\n(cols-&gt;mat [a00 a10 a20 a30] [a01 a11 a21 a31] [a02 a12 a22 a32] [a03 a13 a23 a33])\n\nCreate nxn matrix from nd vectors (columns).\nsource\n\n\n\ncols-&gt;mat2x2\n\n(cols-&gt;mat2x2 [a00 a10] [a01 a11])\n\nCreate 2x2 matrix from 2d vectors (columns).\nsource\n\n\n\ncols-&gt;mat3x3\n\n(cols-&gt;mat3x3 [a00 a10 a20] [a01 a11 a21] [a02 a12 a22])\n\nCreate 3x3 matrix from 3d vectors (columns).\nsource\n\n\n\ncols-&gt;mat4x4\n\n(cols-&gt;mat4x4 [a00 a10 a20 a30] [a01 a11 a21 a31] [a02 a12 a22 a32] [a03 a13 a23 a33])\n\nCreate 4x4 matrix from 4d vectors (columns).\nsource\n\n\n\ncondition\n\n(condition A)\n(condition A norm-type)\n\nCondition number calculated for L2 norm by default (see norm for other norm types).\nCond(A) = norm(A) * norm(inv(A))\nsource\n\n\n\ncos\n\n(cos vector)\n\nApply cos to matrix elements.\nsource\n\n\n\ncosh\n\n(cosh vector)\n\nApply cosh to matrix elements.\nsource\n\n\n\ncot\n\n(cot vector)\n\nApply cot to matrix elements.\nsource\n\n\n\ncoth\n\n(coth vector)\n\nApply coth to matrix elements.\nsource\n\n\n\ncsc\n\n(csc vector)\n\nApply csc to matrix elements.\nsource\n\n\n\ncsch\n\n(csch vector)\n\nApply csch to matrix elements.\nsource\n\n\n\ndegrees\n\n(degrees vector)\n\nApply degrees to matrix elements.\nsource\n\n\n\ndemean\n\n(demean A)\n(demean A rows?)\n\nSubract mean from columns (or rows)\nsource\n\n\n\ndet\n\n(det A)\n\nReturn determinant of the matrix.\nsource\n\n\n\ndiag\n\n(diag A)\n\nReturn diagonal of the matrix as a vector.\nsource\n\n\n\ndiagonal\n\n(diagonal v)\n(diagonal a11 a22)\n(diagonal a11 a22 a33)\n(diagonal a11 a22 a33 a44)\n\nCreate diagonal matrix\nsource\n\n\n\neigenvalues\n\n(eigenvalues A)\n\nReturn complex eigenvalues for given matrix as a sequence\nsource\n\n\n\neigenvalues-matrix\n\n(eigenvalues-matrix A)\n\nReturn eigenvalues for given matrix as a diagonal or block diagonal matrix\nsource\n\n\n\neigenvectors\n\n(eigenvectors A)\n(eigenvectors A normalize?)\n\nReturn eigenvectors as a matrix (columns). Vectors can be normalized.\nsource\n\n\n\nemulm\n\n(emulm A B)\n\nMultiply two matrices element-wise, Hadamard product, C=AoB\nsource\n\n\n\nentry\n\n(entry A row col)\n\nGet entry at given row and column\nsource\n\n\n\nexp\n\n(exp vector)\n\nApply exp to matrix elements.\nsource\n\n\n\nexpm1\n\n(expm1 vector)\n\nApply expm1 to matrix elements.\nsource\n\n\n\neye\n\n(eye size real-matrix?)\n(eye size)\n\nIdentity matrix for given size\nsource\n\n\n\nfloor\n\n(floor vector)\n\nApply floor to matrix elements.\nsource\n\n\n\nfmap\n\n(fmap A f)\n\nApply a function f to each matrix element.\nsource\n\n\n\nfrac\n\n(frac vector)\n\nApply frac to matrix elements.\nsource\n\n\n\ninverse\n\n(inverse m)\n\nMatrix inversion.\nReturns nil if inversion doesn’t exist.\nsource\n\n\n\nln\n\n(ln vector)\n\nApply ln to matrix elements.\nsource\n\n\n\nlog\n\n(log vector)\n\nApply log to matrix elements.\nsource\n\n\n\nlog10\n\n(log10 vector)\n\nApply log10 to matrix elements.\nsource\n\n\n\nlog1mexp\n\n(log1mexp vector)\n\nApply log1mexp to matrix elements.\nsource\n\n\n\nlog1p\n\n(log1p vector)\n\nApply log1p to matrix elements.\nsource\n\n\n\nlog1pexp\n\n(log1pexp vector)\n\nApply log1pexp to matrix elements.\nsource\n\n\n\nlog1pmx\n\n(log1pmx vector)\n\nApply log1pmx to matrix elements.\nsource\n\n\n\nlog1psq\n\n(log1psq vector)\n\nApply log1psq to matrix elements.\nsource\n\n\n\nlog2\n\n(log2 vector)\n\nApply log2 to matrix elements.\nsource\n\n\n\nlogexpm1\n\n(logexpm1 vector)\n\nApply logexpm1 to matrix elements.\nsource\n\n\n\nlogit\n\n(logit vector)\n\nApply logit to matrix elements.\nsource\n\n\n\nlogmxp1\n\n(logmxp1 vector)\n\nApply logmxp1 to matrix elements.\nsource\n\n\n\nmat\n\n(mat real-matrix-rows)\n(mat a00 a01 a10 a11)\n(mat a00 a01 a02 a10 a11 a12 a20 a21 a22)\n(mat a00 a01 a02 a03 a10 a11 a12 a13 a20 a21 a22 a23 a30 a31 a32 a33)\n\nCreate mat2x2, mat3x3 or mat4x4 or RealMatrix from rows\nsource\n\n\n\nmat-&gt;RealMatrix\n\n(mat-&gt;RealMatrix A)\n\nReturn Apache Commons Math Array2DRowMatrix from a 2x2, 3x3 or 4x4 matrix\nsource\n\n\n\nmat-&gt;array\n\n(mat-&gt;array A)\n\nReturn flat double array of entries (row order)\nsource\n\n\n\nmat-&gt;array2d\n\n(mat-&gt;array2d A)\n\nReturn doubles of doubles\nsource\n\n\n\nmat-&gt;float-array\n\n(mat-&gt;float-array A)\n\nReturn flat float array of entries (row order)\nsource\n\n\n\nmat-&gt;float-array2d\n\n(mat-&gt;float-array2d A)\n\nReturn doubles of doubles\nsource\n\n\n\nmat-&gt;seq\n\n(mat-&gt;seq A)\n\nReturn flat sequence of entries (row order)\nsource\n\n\n\nmat2x2\n\n(mat2x2 v)\n(mat2x2 d1 d2)\n(mat2x2 a00 a01 a10 a11)\n\nCreate 2x2 matrix.\nArity:\n\n1 - fills matrix with given value\n2 - creates diagonal matrix\n4 - creates row ordered matrix\n\nsource\n\n\n\nmat3x3\n\n(mat3x3 v)\n(mat3x3 d1 d2 d3)\n(mat3x3 a00 a01 a02 a10 a11 a12 a20 a21 a22)\n\nCreate 3x3 matrix.\nArity:\n\n1 - fills matrix with given value\n3 - creates diagonal matrix\n9 - creates row ordered matrix\n\nsource\n\n\n\nmat4x4\n\n(mat4x4 v)\n(mat4x4 d1 d2 d3 d4)\n(mat4x4 a00 a01 a02 a03 a10 a11 a12 a13 a20 a21 a22 a23 a30 a31 a32 a33)\n\nCreate 4x4 matrix.\nArity:\n\n1 - fills matrix with given value\n4 - creates diagonal matrix\n16 - creates row ordered matrix\n\nsource\n\n\n\nmulm\n\n(mulm A B)\n(mulm A transposeA? B transposeB?)\n\nMultiply two matrices, C=AxB.\nOptionally you can request transposition of matrices.\nsource\n\n\n\nmulmt\n\n(mulmt A B)\n\nMultiply with transposed matrix, C=AxB^T\nsource\n\n\n\nmuls\n\n(muls A s)\n\nMultply matrix by a scalar, C=sA\nsource\n\n\n\nmulv\n\n(mulv A v)\n\nMultply matrix by vector, x=Av\nsource\n\n\n\nncol\n\n(ncol A)\n\nReturn number of rows\nsource\n\n\n\nnegate\n\n(negate A)\n\nNegate all matrix elements, C=-A\nsource\n\n\n\nnorm\n\n(norm A)\n(norm A norm-type)\n\nCalculate norm of the matrix for given type, default: 1 (maximum absolute column sum).\nAll norm types are: * 1 - maximum absolute column sum * :inf - maximum absolute row sum * 2 - spectral norm, maximum singular value * :max - maximum absolute value * :frobenius - Frobenius norm * [p,q] - generalized L_pq norm, [2,2] - Frobenius norm, [p,p] - entrywise p-norm * [p] - Shatten p-norm, [1] - nuclear/trace norm\nsource\n\n\n\nnormalize\n\n(normalize A)\n(normalize A rows?)\n\nNormalize columns (or rows)\nsource\n\n\n\nnrow\n\n(nrow A)\n\nReturn number of rows\nsource\n\n\n\nouter\n\n(outer v1 v2)\n\nOuter project for two vectors.\nsource\n\n\n\nradians\n\n(radians vector)\n\nApply radians to matrix elements.\nsource\n\n\n\nrint\n\n(rint vector)\n\nApply rint to matrix elements.\nsource\n\n\n\nrotation-matrix-2d\n\n(rotation-matrix-2d theta)\n\nCreate rotation matrix for a plane\nsource\n\n\n\nrotation-matrix-3d\n\n(rotation-matrix-3d [x y z])\n(rotation-matrix-3d x y z)\n\nCreate rotation matrix for a 3d space. Tait–Bryan angles z-y′-x″\nsource\n\n\n\nrotation-matrix-3d-x\n\n(rotation-matrix-3d-x a)\n\nCreate rotation matrix for a 3d space, x-axis, right hand rule.\nsource\n\n\n\nrotation-matrix-3d-y\n\n(rotation-matrix-3d-y a)\n\nCreate rotation matrix for a 3d space, y-axis, right hand rule.\nsource\n\n\n\nrotation-matrix-3d-z\n\n(rotation-matrix-3d-z a)\n\nCreate rotation matrix for a 3d space, z-axis, right hand rule.\nsource\n\n\n\nrotation-matrix-axis-3d\n\n(rotation-matrix-axis-3d angle axis)\n\nCreate 3d rotation matrix for axis ratation.\nsource\n\n\n\nround\n\n(round vector)\n\nApply round to matrix elements.\nsource\n\n\n\nrow\n\n(row A r)\n\nReturn row as a vector\nsource\n\n\n\nrows\n\n(rows A)\n\nReturn matrix rows\nsource\n\n\n\nrows-&gt;RealMatrix\n\n(rows-&gt;RealMatrix rows)\n\nReturn Apache Commons Math Array2DRowMatrix from sequence of rows\nsource\n\n\n\nrows-&gt;mat\n\n(rows-&gt;mat real-matrix-rows)\n(rows-&gt;mat [a00 a01] [a10 a11])\n(rows-&gt;mat [a00 a01 a02] [a10 a11 a12] [a20 a21 a22])\n(rows-&gt;mat [a00 a01 a02 a03] [a10 a11 a12 a13] [a20 a21 a22 a23] [a30 a31 a32 a33])\n\nCreate nxn matrix from nd vectors (rows).\nsource\n\n\n\nrows-&gt;mat2x2\n\n(rows-&gt;mat2x2 [a00 a01] [a10 a11])\n\nCreate 2x2 matrix from 2d vectors (rows).\nsource\n\n\n\nrows-&gt;mat3x3\n\n(rows-&gt;mat3x3 [a00 a01 a02] [a10 a11 a12] [a20 a21 a22])\n\nCreate 3x3 matrix from 3d vectors (rows).\nsource\n\n\n\nrows-&gt;mat4x4\n\n(rows-&gt;mat4x4 [a00 a01 a02 a03] [a10 a11 a12 a13] [a20 a21 a22 a23] [a30 a31 a32 a33])\n\nCreate 4x4 matrix from 4d vectors (rows).\nsource\n\n\n\nsafe-sqrt\n\n(safe-sqrt vector)\n\nApply safe-sqrt to matrix elements.\nsource\n\n\n\nscale-cols\n\n(scale-cols A)\n(scale-cols A scale)\n\nShift columns by a value (default: sqrt(sum(x^2)/(n-1))) or a result of the function\nsource\n\n\n\nscale-rows\n\n(scale-rows A)\n(scale-rows A scale)\n\nShift rows by a value (default: sqrt(sum(x^2)/(n-1))) or a result of the function\nsource\n\n\n\nsec\n\n(sec vector)\n\nApply sec to matrix elements.\nsource\n\n\n\nsech\n\n(sech vector)\n\nApply sech to matrix elements.\nsource\n\n\n\nsfrac\n\n(sfrac vector)\n\nApply sfrac to matrix elements.\nsource\n\n\n\nsgn\n\n(sgn vector)\n\nApply sgn to matrix elements.\nsource\n\n\n\nshape\n\n(shape A)\n\nReturn [nrows, ncols] pair.\nsource\n\n\n\nshift-cols\n\n(shift-cols A)\n(shift-cols A shift)\n\nShift columns by a value or a result of the function (mean by default)\nsource\n\n\n\nshift-rows\n\n(shift-rows A)\n(shift-rows A shift)\n\nShift rows by a value or a result of the function (mean by default)\nsource\n\n\n\nsigmoid\n\n(sigmoid vector)\n\nApply sigmoid to matrix elements.\nsource\n\n\n\nsignum\n\n(signum vector)\n\nApply signum to matrix elements.\nsource\n\n\n\nsin\n\n(sin vector)\n\nApply sin to matrix elements.\nsource\n\n\n\nsinc\n\n(sinc vector)\n\nApply sinc to matrix elements.\nsource\n\n\n\nsingular-values\n\n(singular-values A)\n\nReturun singular values of the matrix as sqrt of eigenvalues of A^T * A matrix.\nsource\n\n\n\nsinh\n\n(sinh vector)\n\nApply sinh to matrix elements.\nsource\n\n\n\nsolve\n\n(solve A b)\n\nSolve linear equation Ax=b\nsource\n\n\n\nsq\n\n(sq vector)\n\nApply sq to matrix elements.\nsource\n\n\n\nsqrt\n\n(sqrt vector)\n\nApply sqrt to matrix elements.\nsource\n\n\n\nstandardize\n\n(standardize A)\n(standardize A rows?)\n\nNormalize columns (or rows) to have mean = 0 and stddev = 1\nsource\n\n\n\nsub\n\n(sub A)\n(sub A B)\n\nSubract matrices, C=A-B.\nsource\n\n\n\nsymmetric?\n\n(symmetric? A)\n\nCheck if matrix is symmetric\nsource\n\n\n\ntan\n\n(tan vector)\n\nApply tan to matrix elements.\nsource\n\n\n\ntanh\n\n(tanh vector)\n\nApply tanh to matrix elements.\nsource\n\n\n\ntmulm\n\n(tmulm A B)\n\nTranspose and multiply, C=A^TxB\nsource\n\n\n\ntmulmt\n\n(tmulmt A B)\n\nTranspose both and multiply, C=ATxBT\nsource\n\n\n\ntrace\n\n(trace A)\n\nReturn trace of the matrix (sum of diagonal elements)\nsource\n\n\n\ntranspose\n\n(transpose A)\n\nTranspose matrix, C=A^T\nsource\n\n\n\ntrunc\n\n(trunc vector)\n\nApply trunc to matrix elements.\nsource\n\n\n\nvtmul\n\n(vtmul A v)\n\nMultiply transposed vector by matrix, C=v^T A\nsource\n\n\n\nxlogx\n\n(xlogx vector)\n\nApply xlogx to matrix elements.\nsource\n\n\n\nzero\n\n(zero size real-matrix?)\n(zero size)\n\nZero matrix for given size\nsource\n\nsource: clay/vector_matrix.clj",
    "crumbs": [
      "Vectors and matrices"
    ]
  },
  {
    "objectID": "complex_quaternions.html",
    "href": "complex_quaternions.html",
    "title": "Complex numbers and quaternions",
    "section": "",
    "text": "Complex numbers\nComplex numbers extend the concept of real numbers by including an imaginary unit \\(i\\), where \\(i^2 = -1\\).\nIn fastmath, complex numbers are represented as 2-dimensional vectors using the fastmath.vector/Vec2 type, where the x-component is the real part and the y-component is the imaginary part.\nYou can create complex numbers using the dedicated complex function, the general 2D vector constructor fastmath.vector/vec2, or by converting sequences/arrays using functions like fastmath.vector/seq-&gt;vec2. ensure-complex accepts single argument which can be a real or complex number.\nThe implementation handles floating-point subtleties, including special values (##Inf, ##NaN) and signed zero (+0.0, -0.0), for robust calculations across a wide range of inputs.\nThis section covers constants, predicates, and a comprehensive set of operations including basic arithmetic, exponentiation, logarithms, trigonometric functions, and their inverses.\nPlots are based on domain coloring in HSB space, where\nWe will use these numbers to illustrate operations:",
    "crumbs": [
      "Complex numbers and quaternions"
    ]
  },
  {
    "objectID": "complex_quaternions.html#complex-numbers",
    "href": "complex_quaternions.html#complex-numbers",
    "title": "Complex numbers and quaternions",
    "section": "",
    "text": "Defined functions\n\n\n\n\ncomplex, ensure-complex\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(cplx/complex) ;; =&gt; #vec2 [0.0, 0.0]\n(cplx/complex 1.0) ;; =&gt; #vec2 [1.0, 0.0]\n(cplx/complex 1.0 -2.0) ;; =&gt; #vec2 [1.0, -2.0]\n(v/vec2 1.0 -2.0) ;; =&gt; #vec2 [1.0, -2.0]\n(v/seq-&gt;vec2 [1.0 -2.0]) ;; =&gt; #vec2 [1.0, -2.0]\n(cplx/ensure-complex 2.0) ;; =&gt; #vec2 [2.0, 0.0]\n(cplx/ensure-complex (cplx/complex 1.0 -2.0)) ;; =&gt; #vec2 [1.0, -2.0]\n\n\n\n\n\n\n\nhue - represents argument\nsaturation/brightness - represents fractional part of logarithm of magnitude\n\n\n\n\n\n\n(def z1 (cplx/complex 0.5 -1.5))\n\n\n(def z2 (cplx/complex -0.5 2.5))\n\n\n(def z3 (cplx/complex 1.5 0.5))\n\n\n\n\n\n\n\nExamples\n\n\n\n\nz1 ;; =&gt; #vec2 [0.5, -1.5]\nz2 ;; =&gt; #vec2 [-0.5, 2.5]\nz3 ;; =&gt; #vec2 [1.5, 0.5]\n\n\n\n\nConstants\nList of defined complex constants.\n\n\n\n\n\nconstant\nz\nvalue\n\n\n\n\nI\n\\(0+i\\)\n#vec2 [0.0, 1.0]\n\n\n-I\n\\(0-i\\)\n#vec2 [0.0, -1.0]\n\n\nI-\n\\(0-i\\)\n#vec2 [0.0, -1.0]\n\n\nZERO\n\\(0+0i\\)\n#vec2 [0.0, 0.0]\n\n\nONE\n\\(1+0i\\)\n#vec2 [1.0, 0.0]\n\n\nTWO\n\\(2+0i\\)\n#vec2 [2.0, 0.0]\n\n\nPI\n\\(\\pi+0i\\)\n#vec2 [3.141592653589793, 0.0]\n\n\n\n\n\n\n\nBasic operations\nThis section provides a collection of functions for performing fundamental operations on complex numbers. These operations include extracting components, calculating magnitude and argument, finding conjugates, performing arithmetic (addition, subtraction, multiplication, division), negation, reciprocals, and square roots.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nre, im\ndelta-eq, csgn, flip\nabs, norm, normalize, conjugate\narg\nadd, adds, sub, neg\nscale, mult, mult-I, mult-I-, muladd, sq\ndiv, reciprocal\nsqrt, sqrt1z\n\n\n\n\nComponents\n\nre: Returns the real part of a complex number \\(z\\). For \\(z = a + bi\\), returns \\(a\\).\nim: Returns the imaginary part of a complex number \\(z\\). For \\(z = a + bi\\), returns \\(b\\).\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(cplx/re z1) ;; =&gt; 0.5\n(cplx/im z1) ;; =&gt; -1.5\n\n\n\n\nProperties and Comparisons\n\nabs: Calculates the magnitude or modulus of a complex number \\(|z|\\). For \\(z = a + bi\\): \\[|z| = \\sqrt{a^2 + b^2}\\] This corresponds to the length of the vector (Vec2. a b).\nnorm: Calculates the squared magnitude \\(|z|^2\\). For \\(z = a + bi\\): \\[|z|^2 = a^2 + b^2\\] This avoids the square root calculation and is often used when comparing magnitudes.\narg: Computes the argument (phase angle) of a complex number \\(z\\). This is the angle \\(\\phi\\) such that \\(z = |z| (\\cos \\phi + i \\sin \\phi)\\), and is typically in the range \\((-\\pi, \\pi]\\). Calculated as: \\[\\operatorname{arg}(z)=\\operatorname{atan2}(\\operatorname{im}(z), \\operatorname{re}(z))\\]\nnormalize: Sets magnitude of a complex number \\(z\\) to 1.0: \\[\\frac{z}{|z|}\\]\nconjugate: Returns the complex conjugate of \\(z\\), denoted \\(\\bar{z}\\). For \\(z = a + bi\\): \\[\\bar{z} = a - bi\\] Geometrically, this is a reflection across the real axis.\nflip: Swaps the real and imaginary parts of a complex number. For \\(z = a + bi\\), returns \\(b + ai\\).\ndelta-eq: Checks if two complex numbers are approximately equal within a given tolerance. This is essential for floating-point comparisons and equivalent to checking if \\(|z_1 - z_2| &lt; \\text{tolerance}\\).\ncsgn: Implements the complex signum function. Returns 0 for the zero complex number. For non-zero \\(z\\), it returns the sign of the real part \\(\\operatorname{sgn}(\\operatorname{re}(z))\\).\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\nz1 ;; =&gt; #vec2 [0.5, -1.5]\n(cplx/abs z1) ;; =&gt; 1.5811388300841898\n(cplx/norm z1) ;; =&gt; 2.5\n(cplx/arg z1) ;; =&gt; -1.2490457723982544\n(cplx/normalize z1) ;; =&gt; #vec2 [0.31622776601683794, -0.9486832980505138]\n(cplx/abs (cplx/normalize z1)) ;; =&gt; 0.9999999999999999\n(cplx/conjugate z1) ;; =&gt; #vec2 [0.5, 1.5]\n(cplx/flip z1) ;; =&gt; #vec2 [-1.5, 0.5]\nz2 ;; =&gt; #vec2 [-0.5, 2.5]\n(cplx/abs z2) ;; =&gt; 2.5495097567963922\n(cplx/norm z2) ;; =&gt; 6.5\n(cplx/arg z2) ;; =&gt; 1.7681918866447774\n(cplx/normalize z2) ;; =&gt; #vec2 [-0.19611613513818404, 0.9805806756909202]\n(cplx/abs (cplx/normalize z2)) ;; =&gt; 1.0\n(cplx/conjugate z2) ;; =&gt; #vec2 [-0.5, -2.5]\n(cplx/flip z2) ;; =&gt; #vec2 [2.5, -0.5]\n(cplx/csgn z1) ;; =&gt; 1.0\n(cplx/csgn z2) ;; =&gt; -1.0\n(cplx/csgn cplx/ZERO) ;; =&gt; 0.0\n(cplx/delta-eq (cplx/complex 0.001) cplx/ZERO) ;; =&gt; false\n(cplx/delta-eq (cplx/complex 0.001) cplx/ZERO 0.01) ;; =&gt; true\n\n\n\n\nArithmetic Operations\n\nadd: Computes the sum of two complex numbers, \\(z_1 + z_2\\). If \\(z_1 = a + bi\\) and \\(z_2 = c + di\\): \\[z_1 + z_2 = (a+c) + (b+d)i\\]\nadds: Adds a real scalar \\(s\\) to a complex number \\(z\\). For \\(z = a + bi\\): \\[z + s = (a+s) + bi\\]\nsub: Computes the difference of two complex numbers, \\(z_1 - z_2\\). If \\(z_1 = a + bi\\) and \\(z_2 = c + di\\): \\[z_1 - z_2 = (a-c) + (b-d)i\\]\nneg: Returns the negation of a complex number, \\(-z\\). For \\(z = a + bi\\): \\[-z = -a - bi\\]\nscale: Multiplies a complex number \\(z\\) by a real scalar \\(s\\). For \\(z = a + bi\\): \\[z \\cdot s = (as) + (bs)i\\]\nmult: Computes the product of two complex numbers, \\(z_1 \\cdot z_2\\). If \\(z_1 = a + bi\\) and \\(z_2 = c + di\\): \\[z_1 \\cdot z_2 = (ac - bd) + (ad + bc)i\\]\nmult-I: Multiplies a complex number \\(z\\) by the imaginary unit \\(i\\). For \\(z = a + bi\\): \\[z \\cdot i = -b + ai\\]\nmult-I-: Multiplies a complex number \\(z\\) by \\(-i\\). For \\(z = a + bi\\): \\[z \\cdot (-i) = b - ai\\]\nmuladd: Computes the fused multiply-add operation \\[(x \\cdot y) + z\\] for complex numbers \\(x, y, z\\).\nsq: Computes the square of a complex number, \\(z^2\\). For \\(z = a + bi\\): \\[z^2 = (a^2 - b^2) + 2abi\\]\ndiv: Computes the division of two complex numbers, \\(z_1 / z_2\\). If \\(z_1 = a + bi\\) and \\(z_2 = c + di\\): \\[z_1 / z_2 = \\frac{z_1 \\cdot \\bar{z_2}}{|z_2|^2} = \\frac{(ac+bd) + (bc-ad)i}{c^2+d^2}\\]\nreciprocal: Computes the reciprocal of a complex number, \\(1/z\\). For \\(z = a + bi\\): \\[1/z = \\frac{\\bar{z}}{|z|^2} = \\frac{a - bi}{a^2+b^2}\\]\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(cplx/add z1 z2) ;; =&gt; #vec2 [0.0, 1.0]\n(cplx/adds z1 -3.5) ;; =&gt; #vec2 [-3.0, -1.5]\n(cplx/sub z1 z2) ;; =&gt; #vec2 [1.0, -4.0]\n(cplx/neg z1) ;; =&gt; #vec2 [-0.5, 1.5]\n(cplx/scale z1 5) ;; =&gt; #vec2 [2.5, -7.5]\n(cplx/mult z1 z2) ;; =&gt; #vec2 [3.5, 2.0]\n(cplx/mult-I z1) ;; =&gt; #vec2 [1.5, 0.5]\n(cplx/mult-I- z1) ;; =&gt; #vec2 [-1.5, -0.5]\n(cplx/muladd z1 z2 z3) ;; =&gt; #vec2 [5.0, 2.5]\n(cplx/sq z1) ;; =&gt; #vec2 [-2.0, -1.5]\n(cplx/div z1 z2) ;; =&gt; #vec2 [-0.6153846153846154, -0.07692307692307693]\n(cplx/reciprocal z1) ;; =&gt; #vec2 [0.2, 0.6]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoots\n\nsqrt: Computes the principal square root of a complex number: \\[\\sqrt{z} = \\sqrt{\\frac{|a| + x}{2}} + i \\cdot \\operatorname{sgn}(b) \\cdot \\sqrt{\\frac{|z| - a}{2}}\\]\nsqrt1z: Computes \\(\\sqrt{1 - z^2}\\) for a complex number \\(z\\).\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(cplx/sqrt z1) ;; =&gt; #vec2 [1.0200830432087846, -0.7352342586156434]\n(cplx/sqrt z2) ;; =&gt; #vec2 [1.0123017723970438, 1.2348096526988264]\n(cplx/sqrt z3) ;; =&gt; #vec2 [1.2411967672541269, 0.20141850719855633]\n(cplx/sqrt1z z1) ;; =&gt; #vec2 [1.782428394950227, 0.4207742662340966]\n(cplx/sqrt1z z2) ;; =&gt; #vec2 [2.686357605909836, 0.46531407331997443]\n(cplx/sqrt1z z3) ;; =&gt; #vec2 [0.6335517491618166, -1.1838022718621541]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicates\nPredicates are functions that return a boolean value, indicating whether a complex number satisfies a specific condition. They are useful for checking the nature or state of a complex number, such as whether it is real, imaginary, zero, infinite, or invalid.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nreal?, imaginary?\nzero?\ninf?,nan?\ninvalid?, valid?\n\n\n\n\nreal?: Checks if a complex number z has a zero imaginary part. \\(z = a + bi\\) is real if \\(b = \\operatorname{im}(z) = 0\\).\nimaginary?: Checks if a complex number z has a zero real part. \\(z = a + bi\\) is pure imaginary if \\(a = \\operatorname{re}(z) = 0\\).\nzero?: Checks if z is the complex number \\(0+0i\\), which is true if both real and imaginary parts are zero.\ninf?: Checks if either the real or imaginary part of z is positive or negative infinity (##Inf or ##-Inf).\nnan?: Checks if either the real or imaginary part of z is Not a Number (##NaN).\ninvalid?: Checks if z is either inf? or nan?. Represents values that are not finite numbers.\nvalid?: Checks if z is a finite, non-NaN number. This is the opposite of invalid?.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(cplx/real? cplx/ONE) ;; =&gt; true\n(cplx/real? cplx/I) ;; =&gt; false\n(cplx/imaginary? cplx/ONE) ;; =&gt; false\n(cplx/imaginary? cplx/I) ;; =&gt; true\n(cplx/zero? cplx/ZERO) ;; =&gt; true\n(cplx/zero? cplx/ONE) ;; =&gt; false\n(cplx/inf? (cplx/complex ##Inf 1)) ;; =&gt; true\n(cplx/nan? (cplx/complex 1 ##NaN)) ;; =&gt; true\n(cplx/invalid? (cplx/complex ##Inf 1)) ;; =&gt; true\n(cplx/invalid? (cplx/complex 1.0 2.0)) ;; =&gt; false\n(cplx/valid? (cplx/complex 1.0 2.0)) ;; =&gt; true\n(cplx/valid? (cplx/complex ##Inf 1)) ;; =&gt; false\n\n\n\n\n\nPower and logarithms\nComplex numbers extend the real-valued exponential and logarithm functions. Complex exponentiation allows raising a complex base to a complex power, while the complex logarithm is the inverse operation of exponentiation. Unlike real logarithms, the complex logarithm is multi-valued, but the functions provided here compute the principal value.\n\n\n\n\n\n\nDefined functions\n\n\n\n\npow, exp\nlog, logb\n\n\n\n\nexp: Computes the complex exponential \\(e^z\\). For \\(z = x + iy\\): \\[e^z = e^{x+iy} = e^x (\\cos y + i \\sin y)\\]\nlog: Computes the principal value of the complex natural logarithm \\(\\log z\\). This is given by: \\[\\log z = \\ln|z| + i \\arg(z)\\] where \\(\\ln|z|\\) is the real natural logarithm of the magnitude \\(|z|\\), and \\(\\arg(z)\\) is the principal argument of \\(z\\).\nlogb: Computes the logarithm of \\(z\\) with a complex base \\(b\\): \\[\\log_b z = \\frac{\\log z}{\\log b}\\]\npow: Computes the complex power \\(z_1^{z_2}\\). This is defined as \\[z_1^{z_2} = e^{z_2 \\log z_1}\\] where \\(\\log z_1\\) is the principal value of the logarithm. Handles various edge cases, including \\(0^0\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(cplx/exp z1) ;; =&gt; #vec2 [0.11662592901934625, -1.644591201830844]\n(cplx/log z1) ;; =&gt; #vec2 [0.45814536593707755, -1.2490457723982544]\n(cplx/logb z1 z2) ;; =&gt; #vec2 [-0.4446751692907921, -0.4944697164867257]\n(cplx/pow z1 z2) ;; =&gt; #vec2 [-3.571295577918147, 17.700466856264484]\n(cplx/pow z3 z1) ;; =&gt; #vec2 [1.761680541256831, -1.0235623709790516]\n(cplx/pow cplx/ZERO cplx/ZERO) ;; =&gt; #vec2 [NaN, NaN]\n(cplx/pow cplx/ZERO (cplx/complex 1 1)) ;; =&gt; #vec2 [0.0, 0.0]\n(cplx/pow (cplx/complex 1 1) cplx/ZERO) ;; =&gt; #vec2 [1.0, 0.0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrigonometric and Hyperbolic\nComplex trigonometric and hyperbolic functions extend their real-valued counterparts to the complex plane. They are defined in terms of the complex exponential function:\n\\[ \\sin z = \\frac{e^{iz} - e^{-iz}}{2i} \\quad \\cos z = \\frac{e^{iz} + e^{-iz}}{2} \\] \\[ \\sinh z = \\frac{e^{z} - e^{-z}}{2} \\quad \\cosh z = \\frac{e^{z} + e^{-z}}{2} \\]\nThis leads to relationships between complex trigonometric and hyperbolic functions, such as \\(\\sin(z) = -i \\sinh(iz)\\) and \\(\\cos(z) = \\cosh(iz)\\). These functions are implemented using formulas that provide numerical stability, especially for large arguments.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nsin, cos, tan\nsec, csc, cot\nsinh, cosh, tanh\nsech, csch, coth\nasin, acos, atan\nasec, acsc, acot\nasinh, acosh, atanh\nasech, acsch, acoth\n\n\n\n\nStandard Trigonometric Functions: sin, cos, tan, sec, csc, cot\n\nThese are the direct extensions of real trigonometric functions. For \\(z = x + iy\\): \\[ \\sin z = \\sin x \\cosh y + i \\cos x \\sinh y \\] \\[ \\cos z = \\cos x \\cosh y - i \\sin x \\sinh y \\] \\[ \\tan z = \\frac{\\sin x \\cos x + i \\sinh y \\cosh y}{\\cos^2 x + \\sinh^2 y} \\]\nsec(z), csc(z), and cot(z) are computed as the reciprocals of cos(z), sin(z), and tan(z), respectively.\n\nHyperbolic Trigonometric Functions: sinh, cosh, tanh, sech, csch, coth\n\nThese are also extensions of real hyperbolic functions. For \\(z = x + iy\\): \\[ \\sinh z = \\sinh x \\cos y + i \\cosh x \\sin y \\] \\[ \\cosh z = \\cosh x \\cos y + i \\sinh x \\sin y \\] \\[ \\tanh z = \\frac{\\sinh x \\cosh x + i \\sin y \\cos y}{\\sinh^2 x + \\cos^2 y} \\]\nsech(z), csch(z), and coth(z) are computed as the reciprocals of cosh(z), sinh(z), and tanh(z), respectively.\n\nInverse Trigonometric Functions: asin, acos, atan, asec, acsc, acot\n\nThese are the multi-valued inverse functions. The implementations return the principal values, which are related to complex logarithms. For example: \\[ \\arcsin z = -i \\log(iz + \\sqrt{1-z^2}) \\] \\[ \\arccos z = -i \\log(z + i\\sqrt{1-z^2}) \\] \\[ \\arctan z = \\frac{i}{2} \\log\\left(\\frac{1-iz}{1+iz}\\right) \\] \\[ \\operatorname{arcsec} z = \\arccos(1/z) \\] \\[ \\operatorname{arccsc} z = \\arcsin(1/z) \\] \\[ \\operatorname{arccot} z = \\operatorname{atan}(1/z) \\]\n\nInverse Hyperbolic Trigonometric Functions: asinh, acosh, atanh, asech, acsch, acoth\n\nThese are the multi-valued inverse hyperbolic functions. The implementations return the principal values, which are also related to complex logarithms. For example: \\[ \\operatorname{arsinh} z = \\log(z + \\sqrt{z^2+1}) \\] \\[ \\operatorname{arcosh} z = \\log(z + \\sqrt{z^2-1}) \\] \\[ \\operatorname{artanh} z = \\frac{1}{2} \\log\\left(\\frac{1+z}{1-z}\\right) \\] \\[ \\operatorname{arsech} z = \\operatorname{arcosh}(1/z) \\] \\[ \\operatorname{arcsch} z = \\operatorname{arsinh}(1/z) \\] \\[ \\operatorname{arcoth} z = \\operatorname{artanh}(1/z) \\]\n\n\nThese functions are continuous and analytic everywhere in the complex plane, except at certain points (poles) where the denominator is zero for tangent, cotangent, secant, cosecant, and at branch cuts for the inverse functions.\n\n\n\n\n\n\nExamples\n\n\n\n\n(cplx/sin z1) ;; =&gt; #vec2 [1.1278052468056998, -1.868618519182647]\n(cplx/cos z1) ;; =&gt; #vec2 [2.0644336567607153, 1.0208309495976968]\n(cplx/tan z1) ;; =&gt; #vec2 [0.07932445480395665, -0.9443729864226214]\n(cplx/sec z1) ;; =&gt; #vec2 [0.38922334164348615, -0.1924650047020439]\n(cplx/csc z1) ;; =&gt; #vec2 [0.23675084882124306, 0.3922636659056108]\n(cplx/cot z1) ;; =&gt; #vec2 [0.08832153001414375, 1.0514849080401338]\n(cplx/asin z1) ;; =&gt; #vec2 [0.2734872901415567, -1.2264568712514057]\n(cplx/acos z1) ;; =&gt; #vec2 [1.2973090366533397, 1.2264568712514057]\n(cplx/atan z1) ;; =&gt; #vec2 [1.2767950250211129, -0.6412373393653842]\n(cplx/asec z1) ;; =&gt; #vec2 [1.3991340865479363, -0.5764844975630772]\n(cplx/acsc z1) ;; =&gt; #vec2 [0.17166224024696028, 0.5764844975630772]\n(cplx/acot z1) ;; =&gt; #vec2 [0.2940013017737837, 0.6412373393653842]\n(cplx/sinh z1) ;; =&gt; #vec2 [0.03686082371280443, -1.1248012470579227]\n(cplx/cosh z1) ;; =&gt; #vec2 [0.07976510530654181, -0.5197899547729212]\n(cplx/tanh z1) ;; =&gt; #vec2 [2.1247991277429965, -0.25514922181365146]\n(cplx/sech z1) ;; =&gt; #vec2 [0.28843542211554024, 1.8795917643457205]\n(cplx/csch z1) ;; =&gt; #vec2 [0.02910363957175099, 0.8880922016091114]\n(cplx/coth z1) ;; =&gt; #vec2 [0.46394286018382835, 0.05571098848654149]\n(cplx/asinh z1) ;; =&gt; #vec2 [1.0693110431581105, -1.1711572594583723]\n(cplx/acosh z1) ;; =&gt; #vec2 [-1.2264568712514057, 1.2973090366533397]\n(cplx/atanh z1) ;; =&gt; #vec2 [0.14694666622552977, -1.0172219678978514]\n(cplx/asech z1) ;; =&gt; #vec2 [0.5764844975630772, 1.3991340865479363]\n(cplx/acsch z1) ;; =&gt; #vec2 [0.24366128541928853, 0.6219473660246817]\n(cplx/acoth z1) ;; =&gt; #vec2 [0.14694666622552977, 0.5535743588970452]\n\n\n\n\n\n\n\n\nsin\ncos\ntan\n\n\n\n\n\n\n\nsec\ncsc\ncot\n\n\n\n\n\n\n\nasin\nacos\natan\n\n\n\n\n\n\n\nasec\nacsc\nacot\n\n\n\n\n\n\n\nsinh\ncosh\ntanh\n\n\n\n\n\n\n\nsech\ncsch\ncoth\n\n\n\n\n\n\n\nasinh\nacosh\natanh\n\n\n\n\n\n\n\nasech\nacsch\nacoth",
    "crumbs": [
      "Complex numbers and quaternions"
    ]
  },
  {
    "objectID": "complex_quaternions.html#quaternions",
    "href": "complex_quaternions.html#quaternions",
    "title": "Complex numbers and quaternions",
    "section": "Quaternions",
    "text": "Quaternions\n\n\n\n\n\n\nDefined functions\n\n\n\n\nquaternion, complex-&gt;quaternion, ensure-quaternion\n\n\n\nQuaternions extend complex numbers with three imaginary units: \\(i\\), \\(j\\), and \\(k\\), satisfying the relations \\(i^2 = j^2 = k^2 = ijk = -1\\). They are particularly useful for representing 3D rotations concisely and avoiding gimbal lock.\nIn fastmath, quaternions are represented as 4-dimensional vectors using the fastmath.vector/Vec4 type. A quaternion \\(q = a + bi + cj + dk\\) corresponds to the vector (Vec4. a b c d), where \\(a\\) is the scalar (real) part, and \\((b, c, d)\\) is the vector (imaginary) part.\nYou can create quaternions using the dedicated quaternion function, or by converting numbers or complex numbers using ensure-quaternion.\nThe implementation handles floating-point subtleties, including special values (##Inf, ##NaN), for robust calculations across a wide range of inputs.\nThis section covers constants, creation, accessors, predicates, basic operations, and advanced functions including power, logarithms, trigonometric functions, and functions specific to 3D rotations.\n\n\n\n\n\n\nExamples\n\n\n\n\n(quat/quaternion 1.0 -2.0 3.0 -4.0) ;; =&gt; #vec4 [1.0, -2.0, 3.0, -4.0]\n(quat/quaternion 5.0 [1.0 2.0 3.0]) ;; =&gt; #vec4 [5.0, 1.0, 2.0, 3.0]\n(quat/quaternion 7.0) ;; =&gt; #vec4 [7.0, 0.0, 0.0, 0.0]\n(quat/complex-&gt;quaternion (cplx/complex 1.0 -2.0)) ;; =&gt; #vec4 [1.0, -2.0, 0.0, 0.0]\n(quat/ensure-quaternion 5.0) ;; =&gt; #vec4 [5.0, 0.0, 0.0, 0.0]\n(quat/ensure-quaternion (cplx/complex 1.0 -2.0)) ;; =&gt; #vec4 [1.0, -2.0, 0.0, 0.0]\n\n\n\nWe will use these quaternions to illustrate operations:\n\n(def q1 (quat/quaternion 0.5 -1.5 2.0 -0.5))\n\n\n(def q2 (quat/quaternion -0.5 2.5 -1.0 1.5))\n\n\n(def q3 (quat/quaternion 1.5 0.0 0.0 0.0))\n\n\n(def q4 (quat/quaternion 0.0 1.0 -2.0 3.0))\n\n\n\n\n\n\n\nExamples\n\n\n\n\nq1 ;; =&gt; #vec4 [0.5, -1.5, 2.0, -0.5]\nq2 ;; =&gt; #vec4 [-0.5, 2.5, -1.0, 1.5]\nq3 ;; =&gt; #vec4 [1.5, 0.0, 0.0, 0.0]\nq4 ;; =&gt; #vec4 [0.0, 1.0, -2.0, 3.0]\n\n\n\nAll of the trigonometric, hyperbolic, exp, log and sqrt functions are based on the following general formula. Let \\(r_q\\) is the real part and \\(v_q\\) is the vector (imaginary) part of quaternion \\(q\\). Let \\(z_{in}\\) is defined as follows:\n\\[z_{in} = r_q + |v_q|i\\]\nThen, if \\(f_z\\) is given operation on complex number, corresponding quaternion version \\(f_q\\) is:\n\\[f_q(q) = \\Re(f_z(z_{in})) + \\frac{\\Im(f_z(z_{in}))}{|v_q|}v_q\\]\n\nConstants\nPredefined quaternion constants for common values.\n\n\n\n\n\nconstant\nq\nvalue\n\n\n\n\nZERO\n\\(0+0i+0j+0k\\)\n#vec4 [0.0, 0.0, 0.0, 0.0]\n\n\nONE\n\\(1+0i+0j+0k\\)\n#vec4 [1.0, 0.0, 0.0, 0.0]\n\n\nI\n\\(0+i+0j+0k\\)\n#vec4 [0.0, 1.0, 0.0, 0.0]\n\n\nJ\n\\(0+0i+j+0k\\)\n#vec4 [0.0, 0.0, 1.0, 0.0]\n\n\nK\n\\(0+0i+0j+k\\)\n#vec4 [0.0, 0.0, 0.0, 1.0]\n\n\n-I\n\\(0-i+0j+0k\\)\n#vec4 [0.0, -1.0, 0.0, 0.0]\n\n\n-J\n\\(0+0i-j+0k\\)\n#vec4 [0.0, 0.0, -1.0, 0.0]\n\n\n-K\n\\(0+0i+0j-k\\)\n#vec4 [0.0, 0.0, 0.0, -1.0]\n\n\n\n\n\n\n\nBasic operations\nFunctions to create quaternions and access their scalar and vector components.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nscalar, re, vector, im-i, im-j, im-k\ndelta-eq, qsgn, arg, norm, normalize, conjugate, reciprocal\nadd, adds, sub, scale, mult, div, neg, sq, sqrt\n\n\n\n\nComponents\nscalar, re: Returns the scalar (real) part \\(a\\) of a quaternion \\(a+bi+cj+dk\\).\nim-i, im-j, im-k: Return the coefficients of the \\(i\\), \\(j\\), and \\(k\\) components, respectively.\nvector: Returns the vector (imaginary) part \\((b, c, d)\\) as a Vec3.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(quat/scalar q1) ;; =&gt; 0.5\n(quat/re q1) ;; =&gt; 0.5\n(quat/vector q1) ;; =&gt; #vec3 [-1.5, 2.0, -0.5]\n(quat/im-i q1) ;; =&gt; -1.5\n(quat/im-j q1) ;; =&gt; 2.0\n(quat/im-k q1) ;; =&gt; -0.5\n\n\n\n\nProperties and Comparisons\narg: Computes the argument \\(\\theta\\) such that \\(q = |q| (\\cos \\theta + \\mathbf{v} \\sin \\theta)\\), where \\(\\mathbf{v}\\) is a unit 3D vector. Calculated as: \\[\\operatorname{arg}(z)=\\operatorname{atan2}(|v|, \\operatorname{re}(q))\\]\nnorm: Calculates the magnitude (Euclidean norm) of the quaternion: \\[|q| = \\sqrt{a^2+b^2+c^2+d^2}\\]\nnormalize: Returns the unit quaternion: \\[\\hat{q} = q / |q|\\]\nconjugate: Returns the conjugate: \\[\\bar{q} = a - bi - cj - dk\\]\ndelta-eq: Checks if two quaternions are approximately equal within a tolerance.\nqsgn: Computes the complex signum, returning the sign of the scalar part or 0 for zero.\n\n\n\n\n\n\n\nExamples\n\n\n\n\nq1 ;; =&gt; #vec4 [0.5, -1.5, 2.0, -0.5]\n(quat/arg q1) ;; =&gt; 1.37713802635057\n(quat/norm q1) ;; =&gt; 2.598076211353316\n(quat/normalize q1) ;; =&gt; #vec4 [0.19245008972987526, -0.5773502691896257, 0.769800358919501, -0.19245008972987526]\n(quat/norm (quat/normalize q1)) ;; =&gt; 1.0\n(quat/conjugate q1) ;; =&gt; #vec4 [0.5, 1.5, -2.0, 0.5]\n(quat/qsgn q1) ;; =&gt; 1.0\nq2 ;; =&gt; #vec4 [-0.5, 2.5, -1.0, 1.5]\n(quat/arg q2) ;; =&gt; 1.7316168074693612\n(quat/norm q2) ;; =&gt; 3.122498999199199\n(quat/normalize q2) ;; =&gt; #vec4 [-0.16012815380508713, 0.8006407690254357, -0.32025630761017426, 0.48038446141526137]\n(quat/conjugate q2) ;; =&gt; #vec4 [-0.5, -2.5, 1.0, -1.5]\n(quat/qsgn q2) ;; =&gt; -1.0\n(quat/qsgn quat/ZERO) ;; =&gt; 0.0\n(quat/delta-eq (quat/quaternion 1.0000001) quat/ONE) ;; =&gt; true\n(quat/delta-eq (quat/quaternion 1.0001) quat/ONE) ;; =&gt; false\n\n\n\n\nArithemtic Operations\n\nBasic arithmetic extended to quaternions.\n\nadd: Sum of two quaternions: \\[q_1 + q_2 = (a_1+a_2) + (b_1+b_2)i + (c_1+c_2)j + (d_1+d_2)k\\]\nadds: Adds a real scalar \\(s\\) to a quaternion: \\(q = a+bi+cj+dk\\): \\[q+s = (a+s)+bi+cj+dk\\]\nsub: Difference of two quaternions: \\(q_1 - q_2 = (a_1-a_2) + (b_1-b_2)i + (c_1-c_2)j + (d_1-d_2)k\\)$\nscale: Multiplies a quaternion \\(q\\) by a real scalar \\(s\\): \\[q \\cdot s = (as) + (bs)i + (cs)j + (ds)k\\]\nmult: Quaternion multiplication. Note that quaternion multiplication is non-commutative (\\(q_1 q_2 \\neq q_2 q_1\\) in general). If \\(q_1 = a_1 + \\mathbf{v}_1\\) and \\(q_2 = a_2 + \\mathbf{v}_2\\), where \\(\\mathbf{v}_1, \\mathbf{v}_2\\) are vector parts: \\[ q_1 q_2 = (a_1 a_2 - \\mathbf{v}_1 \\cdot \\mathbf{v}_2) + (a_1 \\mathbf{v}_2 + a_2 \\mathbf{v}_1 + \\mathbf{v}_1 \\times \\mathbf{v}_2) \\]\ndiv: Quaternion division \\[q_1 / q_2 = q_1 \\cdot q_2^{-1}\\]\nreciprocal: Returns the reciprocal: \\[q^{-1} = \\bar{q} / |q|^2\\]\nneg: Negation: \\[-q = -a - bi - cj - dk\\]\nsq: Square of a quaternion: \\[q^2 = q \\cdot q\\]\nsqrt: Computes the principal square root of a quaternion.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(quat/add q1 q2) ;; =&gt; #vec4 [0.0, 1.0, 1.0, 1.0]\n(quat/adds q1 5.0) ;; =&gt; #vec4 [5.5, -1.5, 2.0, -0.5]\n(quat/sub q1 q2) ;; =&gt; #vec4 [1.0, -4.0, 3.0, -2.0]\n(quat/scale q1 3.0) ;; =&gt; #vec4 [1.5, -4.5, 6.0, -1.5]\n(quat/mult q1 q2) ;; =&gt; #vec4 [6.25, 4.5, -0.5, -2.5]\n(quat/mult q2 q1) ;; =&gt; #vec4 [6.25, -0.5, -2.5, 4.5]\n(quat/div q1 q2) ;; =&gt; #vec4 [-0.6923076923076922, -0.3076923076923077, -0.15384615384615385, 0.30769230769230765]\n(quat/reciprocal q1) ;; =&gt; #vec4 [0.07407407407407407, 0.2222222222222222, -0.2962962962962963, 0.07407407407407407]\n(quat/mult q1 (quat/reciprocal q1)) ;; =&gt; #vec4 [0.9999999999999998, 0.0, 0.0, 0.0]\n(quat/neg q1) ;; =&gt; #vec4 [-0.5, 1.5, -2.0, 0.5]\n(quat/sq q1) ;; =&gt; #vec4 [-6.25, -1.5, 2.0, -0.5]\n(quat/sqrt q1) ;; =&gt; #vec4 [1.2446035937906728, -0.6026015060070129, 0.8034686746760172, -0.2008671686690043]\n\n\n\n\n\nPredicates\nFunctions to check the type or state of a quaternion and compute fundamental properties.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nreal?, imaginary?\nzero?\ninf?, nan?\ninvalid?, valid?\n\n\n\n\nreal?: Checks if the quaternion has a zero vector part (\\(b=c=d=0\\)).\nimaginary?: Checks if the quaternion has a zero scalar part (\\(a=0\\)).\nzero?: Checks if all components are zero (\\(a=b=c=d=0\\)).\ninf?, nan?: Checks if any component is infinite or NaN, respectively.\ninvalid?, valid?: Checks if the quaternion is not a finite, non-NaN value.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(quat/real? q3) ;; =&gt; true\n(quat/real? q1) ;; =&gt; false\n(quat/imaginary? q4) ;; =&gt; true\n(quat/imaginary? q1) ;; =&gt; false\n(quat/zero? quat/ZERO) ;; =&gt; true\n(quat/zero? q1) ;; =&gt; false\n(quat/inf? (quat/quaternion ##Inf 1 2 3)) ;; =&gt; true\n(quat/nan? (quat/quaternion 1 ##NaN 2 3)) ;; =&gt; true\n(quat/invalid? (quat/quaternion 1 2 3 4)) ;; =&gt; false\n(quat/valid? (quat/quaternion 1 2 3 4)) ;; =&gt; true\n\n\n\n\n\nPower and Logarithms\nExtensions of exponential, logarithm, and power functions to quaternions.\n\nexp: Computes the quaternion exponential \\(e^q\\).\nlog: Computes the principal value of the quaternion natural logarithm \\(\\log q\\).\nlogb: Computes the logarithm of \\(q\\) with a quaternion base \\(b\\): \\(\\log_b q = (\\log q) (\\log b)^{-1}\\).\npow: Computes the quaternion power \\(q_1^{q_2}\\) defined as \\(e^{(\\log q_1) q_2}\\).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(quat/exp q1) ;; =&gt; #vec4 [-1.3680759934546958, -0.5413604352188154, 0.7218139136250872, -0.1804534784062718]\n(quat/log q1) ;; =&gt; #vec4 [0.9547712524422192, -0.8102369618391012, 1.0803159491188017, -0.2700789872797004]\n(quat/logb q1 q2) ;; =&gt; #vec4 [-0.20614694633615405, -0.7036653773111262, 0.3406342086858799, -0.011638982288984245]\n(quat/pow q1 q2) ;; =&gt; #vec4 [-9.0178461035907, -18.182612909089467, 4.1908222233068875, 1.419324015861925]\n(quat/pow q3 q1) ;; =&gt; #vec4 [0.6265933369573483, -0.6191317465962343, 0.8255089954616457, -0.20637724886541142]\n\n\n\n\n\nTrigonometric and Hyperbolic\nExtensions of standard and hyperbolic trigonometric functions to quaternions.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nsin, cos, tan\nsec, csc, cot\nsinh, cosh, tanh\nsech, csch, coth\nasin, acos, atan\nasec, acsc, acot\nasinh, acosh, atanh\nasech, acsch, acoth\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(quat/sin q1) ;; =&gt; #vec4 [3.087247974413335, -3.2845213397125193, 4.379361786283359, -1.0948404465708397]\n(quat/cos q1) ;; =&gt; #vec4 [5.651169511045248, 1.7943421858300115, -2.3924562477733486, 0.5981140619433372]\n(quat/tan q1) ;; =&gt; #vec4 [0.010202871653497671, -0.5844504887495585, 0.7792673183327447, -0.19481682958318616]\n(quat/sec q1) ;; =&gt; #vec4 [0.1370413436810704, -0.043512951379207854, 0.05801726850561047, -0.014504317126402617]\n(quat/csc q1) ;; =&gt; #vec4 [0.07585997324070858, 0.08070738178603104, -0.10760984238137472, 0.02690246059534368]\n(quat/cot q1) ;; =&gt; #vec4 [0.010338328329859864, 0.5922098454672464, -0.7896131272896618, 0.19740328182241546]\n(quat/asin q1) ;; =&gt; #vec4 [0.18096798983808618, -0.9889006571112673, 1.3185342094816896, -0.3296335523704224]\n(quat/acos q1) ;; =&gt; #vec4 [1.3898283369568103, 0.9889006571112673, -1.3185342094816896, 0.3296335523704224]\n(quat/atan q1) ;; =&gt; #vec4 [1.484700919533427, -0.23215315248464086, 0.3095375366461878, -0.07738438416154696]\n(quat/asec q1) ;; =&gt; #vec4 [1.5014657553875428, -0.2177519631502909, 0.29033595086705455, -0.07258398771676364]\n(quat/acsc q1) ;; =&gt; #vec4 [0.0693305714073537, 0.2177519631502909, -0.29033595086705455, 0.07258398771676364]\n(quat/acot q1) ;; =&gt; #vec4 [0.08609540726146947, 0.23215315248464086, -0.3095375366461878, 0.07738438416154696]\n(quat/sinh q1) ;; =&gt; #vec4 [-0.43239448075125825, -0.3702579047997209, 0.4936772063996279, -0.12341930159990698]\n(quat/cosh q1) ;; =&gt; #vec4 [-0.9356815127034374, -0.17110253041909437, 0.22813670722545917, -0.05703417680636479]\n(quat/tanh q1) ;; =&gt; #vec4 [0.6120359971326302, 0.28378993640212685, -0.37838658186950247, 0.09459664546737562]\n(quat/sech q1) ;; =&gt; #vec4 [-0.9745918753714102, 0.17821783773427158, -0.23762378364569542, 0.059405945911423855]\n(quat/csch q1) ;; =&gt; #vec4 [-0.7416646097306075, 0.6350848511892305, -0.8467798015856407, 0.21169495039641018]\n(quat/coth q1) ;; =&gt; #vec4 [1.00788189192935, -0.46733646280857977, 0.623115283744773, -0.15577882093619325]\n(quat/asinh q1) ;; =&gt; #vec4 [1.6120874585828928, -0.8010575888838687, 1.0680767851784916, -0.2670191962946229]\n(quat/acosh q1) ;; =&gt; #vec4 [1.6808079158716929, -0.8177032858484982, 1.0902710477979976, -0.2725677619494994]\n(quat/atanh q1) ;; =&gt; #vec4 [0.0648777988712712, -0.7107620150571328, 0.9476826867428436, -0.2369206716857109]\n(quat/asech q1) ;; =&gt; #vec4 [-0.3701071697421568, -0.8833849829668168, 1.1778466439557558, -0.29446166098893894]\n(quat/acsch q1) ;; =&gt; #vec4 [0.07987279637814454, 0.22711377640869046, -0.3028183685449206, 0.07570459213623015]\n(quat/acoth q1) ;; =&gt; #vec4 [0.0648777988712712, 0.2134134990436804, -0.2845513320582405, 0.07113783301456013]\n\n\n\n\n\nRotation Functions\nFunctions specifically for using quaternions to represent and manipulate 3D rotations.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nrotation-quaternion, rotate\nslerp\nto-euler, from-euler\nto-angles, from-angles\nto-rotation-matrix, from-rotation-matrix\n\n\n\n\nrotation-quaternion: Creates a unit quaternion representing a rotation by a given angle around a u (Vec3) axis.\nrotate: Rotates a 3D vector in (Vec3) using a quaternion rotq. Can also create the rotation quaternion directly from angle and axis.\nto-euler, from-euler: Converts between a quaternion and ZYX (body 3-2-1) Euler angles [roll pitch yaw].\nto-angles, from-angles: Converts between a quaternion and Tait-Bryan angles (z-y’-x’’ intrinsic) [x y z].\nto-rotation-matrix, from-rotation-matrix: Converts between a quaternion and a 3x3 rotation matrix.\n\n\n(def rot-q (quat/rotation-quaternion m/HALF_PI (v/vec3 1 1 0)))\n\n\nrot-q\n\n\n[0.7071067811865476 0.4999999999999999 0.4999999999999999 0.0]\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(quat/rotate (v/vec3 1 0 0) rot-q) ;; =&gt; #vec3 [0.5000000000000002, 0.4999999999999998, -0.7071067811865475]\n(quat/rotate (v/vec3 1 0 0) m/HALF_PI (v/vec3 1 1 0)) ;; =&gt; #vec3 [0.5000000000000002, 0.4999999999999998, -0.7071067811865475]\n(quat/from-euler m/HALF_PI 1 m/HALF_PI) ;; =&gt; #vec4 [0.6785040502472879, 0.19907851164308485, 0.6785040502472878, 0.19907851164308485]\n(quat/to-euler (quat/from-euler m/HALF_PI 1 m/HALF_PI)) ;; =&gt; #vec3 [1.570796326794897, 1.0000000000000004, 1.570796326794897]\n(quat/from-angles m/HALF_PI 1 m/HALF_PI) ;; =&gt; #vec4 [0.199078511643085, 0.6785040502472878, -0.19907851164308474, 0.6785040502472878]\n(quat/to-angles (quat/from-angles m/HALF_PI 1 m/HALF_PI)) ;; =&gt; #vec3 [1.5707963267948963, 1.0000000000000002, 1.5707963267948963]\n\n\n\nFor a rotation quaternion rot-q conversion between rotation matrix and back looks as follows\n\n(quat/to-rotation-matrix rot-q)\n\n\n#mat3x3 [[0.5000000000000002, 0.5, 0.7071067811865477]\n         [0.5, 0.5000000000000002, -0.7071067811865477]\n         [-0.7071067811865477, 0.7071067811865477, 2.220446049250313E-16]]\n\n\n(quat/from-rotation-matrix (mat/rows-&gt;mat3x3\n                            [0.5 0.5 m/SQRT2_2]\n                            [0.5 0.5 (- m/SQRT2_2)]\n                            [(- m/SQRT2_2) m/SQRT2_2 0.0]))\n\n\n[0.7071067811865475 0.5 0.5 0.0]\n\n\nSlerp\nslerp function performs Spherical Linear Interpolation between two unit quaternions q1 and q2 based on a parameter t in [0, 1]. Useful for animating rotations.\n\n(quat/slerp quat/ONE (quat/quaternion 0 0 0 1) 0.0) ;; =&gt; #vec4 [1.0, 0.0, 0.0, 0.0]\n(quat/slerp quat/ONE (quat/quaternion 0 0 0 1) 0.5) ;; =&gt; #vec4 [0.7071067811865476, 0.0, 0.0, 0.7071067811865475]\n(quat/slerp quat/ONE (quat/quaternion 0 0 0 1) 1.0) ;; =&gt; #vec4 [0.0, 0.0, 0.0, 1.0]\n\nLet’s see how quaternion components changes for different t.\n\n(def q-start (quat/quaternion -1 -2 -3 -4))\n\n\n(def q-end (quat/quaternion 1 2 3 4))",
    "crumbs": [
      "Complex numbers and quaternions"
    ]
  },
  {
    "objectID": "complex_quaternions.html#reference",
    "href": "complex_quaternions.html#reference",
    "title": "Complex numbers and quaternions",
    "section": "Reference",
    "text": "Reference\n\nfastmath.complex\nComplex numbers operations.\nComplex numbers are represented using the Vec2 type defined in the fastmath.vector namespace. A complex number \\(z = a + bi\\) corresponds to the vector (Vec2. a b).\nTo create a complex number, use complex, vec2, or convert from sequences/arrays using functions like fastmath.vector/seq-&gt;vec2.\nThe implementation correctly handles floating-point special values such as ##Inf, ##NaN, and distinguishes between +0.0 and -0.0 where necessary.\nThis namespace provides standard complex number operations like arithmetic (+, -, *, /), exponentiation (pow, exp, log), trigonometric functions (sin, cos, tan, etc.), and their inverses, as well as utility functions like abs, arg, conjugate, etc.\n\n\n-I\nz=0-i\nsource\n\n\n\nI\nz=0+i\nsource\n\n\n\nI-\nz=0-i\nsource\n\n\n\nONE\nz=1+0i\nsource\n\n\n\nPI\nz=pi+0i\nsource\n\n\n\nTWO\nz=2+0i\nsource\n\n\n\nZERO\nz=0+0i\nsource\n\n\n\nabs\n\n(abs z)\n\nCalculates the magnitude (absolute value) of the complex number z.\nsource\n\n\n\nacos\n\n(acos z)\n\nacos(z)\nsource\n\n\n\nacosh\n\n(acosh z)\n\nacosh(z)\nsource\n\n\n\nacot\n\n(acot z)\n\nacot(z)\nsource\n\n\n\nacoth\n\n(acoth z)\n\nacoth(z)\nsource\n\n\n\nacsc\n\n(acsc z)\n\nacsc(z)\nsource\n\n\n\nacsch\n\n(acsch z)\n\nacsch(z)\nsource\n\n\n\nadd\n\n(add z1 z2)\n\nSum of two complex numbers.\nsource\n\n\n\nadds\n\n(adds z v)\n\nAdd scalar to complex number.\nsource\n\n\n\narg\n\n(arg z)\n\nArgument (angle) of the complex number.\nsource\n\n\n\nasec\n\n(asec z)\n\nasec(z)\nsource\n\n\n\nasech\n\n(asech z)\n\nasech(z)\nsource\n\n\n\nasin\n\n(asin z)\n\nasin(z)\nsource\n\n\n\nasinh\n\n(asinh z)\n\nasinh(z)\nsource\n\n\n\natan\n\n(atan z)\n\natan(z)\nsource\n\n\n\natanh\n\n(atanh z)\n\natanh(z)\nsource\n\n\n\ncomplex\n\n(complex a b)\n(complex a)\n(complex)\n\nCreates a complex number represented as a Vec2. Takes optional real and imaginary parts.\nExamples:\n(complex 1 2) ;; =&gt; #vec2 [1.0, 2.0] (complex 3) ;; =&gt; #vec2 [3.0, 0.0] (complex) ;; =&gt; #vec2 [0.0, 0.0]\nsource\n\n\n\nconjugate\n\n(conjugate z)\n\nComplex conjugate.\nsource\n\n\n\ncos\n\n(cos z)\n\ncos(z)\nsource\n\n\n\ncosh\n\n(cosh z)\n\ncosh(z)\nsource\n\n\n\ncot\n\n(cot z)\n\ncsc(z)\nsource\n\n\n\ncoth\n\n(coth z)\n\ncoth(z)\nsource\n\n\n\ncsc\n\n(csc z)\n\ncsc(z)\nsource\n\n\n\ncsch\n\n(csch z)\n\ncsch(z)\nsource\n\n\n\ncsgn\n\n(csgn re im)\n(csgn z)\n\nComplex signum function.\nReturns 0.0 for the zero. For any other vector, returns the sign of the real part.\nsource\n\n\n\ndelta-eq\n\n(delta-eq q1 q2)\n(delta-eq q1 q2 accuracy)\n\nCompare complex numbers with given accuracy (10e-6 by default)\nsource\n\n\n\ndiv\n\n(div z1 z2)\n\nDivision of two complex numbers.\nsource\n\n\n\nensure-complex\n\n(ensure-complex v)\n\nConvert possible number to complex or return input.\nsource\n\n\n\nexp\n\n(exp z)\n\nexp(z)\nsource\n\n\n\nflip\n\n(flip z)\n\nExchange imaginary and real parts\nsource\n\n\n\nim\n\n(im z)\n\nImaginary part\nsource\n\n\n\nimaginary?\n\n(imaginary? z)\n\nIs z is a pure imaginary number?\nsource\n\n\n\ninf?\n\n(inf? z)\n\nIs infinite?\nsource\n\n\n\ninvalid?\n\n(invalid? z)\n\nIs NaN or Inf?\nsource\n\n\n\nlog\n\n(log z)\n\nlog(z), principal value\nsource\n\n\n\nlogb\n\n(logb z b)\n\nlog with base b\nsource\n\n\n\nmuladd\n\n(muladd x y z)\n\n(+ z (* x y))\nsource\n\n\n\nmult\n\n(mult z1 z2)\n\nMultiplication of two complex numbers.\nsource\n\n\n\nmult-I\n\n(mult-I z)\n\nMultiplication by 0+i.\nsource\n\n\n\nmult-I-\n\n(mult-I- z)\n\nMultiplication by 0-i.\nsource\n\n\n\nnan?\n\n(nan? z)\n\nIs NaN?\nsource\n\n\n\nneg\n\n(neg z)\n\nNegation of the complex number, -z.\nsource\n\n\n\nnorm\n\n(norm z)\n\nCalculates the squared magnitude (norm) of the complex number z.\nAlso known as the squared modulus or squared Euclidean norm.\nsource\n\n\n\nnormalize\n\n(normalize z)\n\nNormalize complex number to make abs(z) = 1\nsource\n\n\n\npow\n\n(pow z1 z2)\n\nComplex power, z^z\nsource\n\n\n\nre\n\n(re z)\n\nReal part\nsource\n\n\n\nreal?\n\n(real? z)\n\nIs z is a real number?\nsource\n\n\n\nreciprocal\n\n(reciprocal z)\n\nReciprocal, 1/z.\nsource\n\n\n\nscale\n\n(scale z v)\n\nMultiplication by real number\nsource\n\n\n\nsec\n\n(sec z)\n\nsec(z)\nsource\n\n\n\nsech\n\n(sech z)\n\nsech(z)\nsource\n\n\n\nsin\n\n(sin z)\n\nsin(z)\nsource\n\n\n\nsinh\n\n(sinh z)\n\nsinh(z)\nsource\n\n\n\nsq\n\n(sq z)\n\nSquare of the complex number.\nsource\n\n\n\nsqrt\n\n(sqrt z)\n\nSqrt of the complex number, sqrt(z)\nsource\n\n\n\nsqrt1z\n\n(sqrt1z z)\n\nsqrt(1-z^2)\nsource\n\n\n\nsub\n\n(sub z1 z2)\n\nDifference of two complex numbers.\nsource\n\n\n\ntan\n\n(tan z)\n\ntan(z)\nsource\n\n\n\ntanh\n\n(tanh z)\n\ntanh(z)\nsource\n\n\n\nvalid?\n\n(valid? z)\n\nIs valid complex (not NaN or Inf)?\nsource\n\n\n\nzero?\n\n(zero? z)\n\nIs zero?\nsource\n\n\n\nfastmath.quaternion\nOperations for quaternions.\nQuaternions extend complex numbers and are widely used in fields like 3D graphics and physics for representing rotations.\nIn fastmath, quaternions are represented as 4-dimensional vectors (Vec4) where the components correspond to the scalar part and the three imaginary parts (\\(i\\), \\(j\\), \\(k\\)): \\(q = a + bi + cj + dk\\) is (Vec4. a b c d).\nThe namespace provides functions for creating quaternions, accessing scalar and vector parts, predicates (e.g., real?, zero?, inf?, nan?), and fundamental properties (magnitude, argument, normalization).\nA comprehensive set of operations is included: - Arithmetic: Addition, subtraction, multiplication, division, negation, square, reciprocal, scaling, conjugation. - Transcendental Functions: Extensions of standard complex functions like exponential, logarithm, power, trigonometric, hyperbolic functions, and their inverses. - Rotations: Functions for creating rotation quaternions, rotating 3D vectors (rotate), spherical linear interpolation (SLERP), and conversions between quaternions, Euler angles (ZYX body 3-2-1 and z-y’-x’’), and rotation matrices.\nThe implementation correctly handles floating-point special values, including ##Inf and ##NaN.\n\n\n-I\n0-1i+0j+0k\nsource\n\n\n\n-J\n0+0i-1j+0k\nsource\n\n\n\n-K\n0+0i+0j-1k\nsource\n\n\n\nI\n0+1i+0j+0k\nsource\n\n\n\nJ\n0+0i+1j+0k\nsource\n\n\n\nK\n0+0i+0j+1k\nsource\n\n\n\nONE\n1+0i+0j+0k\nsource\n\n\n\nZERO\n0+0i+0j+0k\nsource\n\n\n\nacos\n\n(acos q)\n\nacos(q)\nsource\n\n\n\nacosh\n\n(acosh q)\n\nacosh(q)\nsource\n\n\n\nacot\n\n(acot q)\n\nacot(q)\nsource\n\n\n\nacoth\n\n(acoth q)\n\nacoth(q)\nsource\n\n\n\nacsc\n\n(acsc q)\n\nacsc(q)\nsource\n\n\n\nacsch\n\n(acsch q)\n\nacsch(q)\nsource\n\n\n\nadd\n\n(add q1 q2)\n\nSum of two quaternions\nsource\n\n\n\nadds\n\n(adds q1 s)\n\nAdds scalar to a quaternion\nsource\n\n\n\narg\n\n(arg quaternion)\n\nArgument of quaternion, atan2(|vector(q)|, re(q))\nsource\n\n\n\nasec\n\n(asec q)\n\nasec(q)\nsource\n\n\n\nasech\n\n(asech q)\n\nasech(q)\nsource\n\n\n\nasin\n\n(asin q)\n\nasin(q)\nsource\n\n\n\nasinh\n\n(asinh q)\n\nasinh(q)\nsource\n\n\n\natan\n\n(atan q)\n\natan(q)\nsource\n\n\n\natanh\n\n(atanh q)\n\natanh(q)\nsource\n\n\n\ncomplex-&gt;quaternion\n\n(complex-&gt;quaternion z)\n\nCreate quaternion from complex number\nsource\n\n\n\nconjugate\n\n(conjugate quaternion)\n\nConjugate of the quaternion\nsource\n\n\n\ncos\n\n(cos q)\n\ncos(q)\nsource\n\n\n\ncosh\n\n(cosh q)\n\ncosh(q)\nsource\n\n\n\ncot\n\n(cot q)\n\ncot(q)\nsource\n\n\n\ncoth\n\n(coth q)\n\ncoth(q)\nsource\n\n\n\ncsc\n\n(csc q)\n\ncsc(q)\nsource\n\n\n\ncsch\n\n(csch q)\n\ncsch(q)\nsource\n\n\n\ndelta-eq\n\n(delta-eq q1 q2)\n(delta-eq q1 q2 accuracy)\n\nCompare quaternions with given accuracy (10e-6 by default)\nsource\n\n\n\ndiv\n\n(div q1 q2)\n\nDivision two quaternions\nsource\n\n\n\nensure-quaternion\n\n(ensure-quaternion q)\n\nConvert possible number, complex number to a quaternion\nsource\n\n\n\nexp\n\n(exp q)\n\nexp(q)\nsource\n\n\n\nfrom-angles\n\n(from-angles [x y z])\n(from-angles x y z)\n\nConverts Tait–Bryan angles (z-y′-x’’ intrinsic rotation sequence) to a quaternion.\nThe angles [x y z] correspond to rotations around the local (body) x-axis, followed by the intermediate local y’-axis, and finally the local z’’-axis.\nParameters:\n\nx: Rotation around the x-axis (radians).\ny: Rotation around the y’-axis (radians).\nz: Rotation around the z’’-axis (radians).\n\nCan accept individual double values or a Vec3 containing [x y z].\nReturns A quaternion Vec4 representing the rotation.\nsource\n\n\n\nfrom-euler\n\n(from-euler [roll pitch yaw])\n(from-euler roll pitch yaw)\n\nConverts Euler angles (ZYX body 3-2-1 convention) to a quaternion.\nThe rotation sequence is intrinsic Z-Y’-X’’ (yaw, pitch, roll), applied to the body frame. The order of input parameters or vector components is [roll pitch yaw].\nParameters:\n\nroll: Rotation around the x-axis (radians), expected range [-pi, pi].\npitch: Rotation around the y’-axis (radians), expected range [-pi/2, pi/2].\nyaw: Rotation around the z’’-axis (radians), expected range [-pi, pi].\n\nCan accept individual double values or a Vec3 containing [roll pitch yaw].\nReturns A quaternion Vec4 representing the rotation.\nsource\n\n\n\nfrom-rotation-matrix\n\n(from-rotation-matrix m)\n\nConverts a 3x3 rotation matrix to a quaternion.\nTakes a Mat3x3 rotation matrix as input. Returns a Vec4 representing the quaternion that represents the same rotation.\nThe resulting quaternion is a unit quaternion if the input matrix is a valid rotation matrix. The method handles numerical stability and normalization.\nsource\n\n\n\nim-i\n\n(im-i quaternion)\n\nReturn i imaginary part\nsource\n\n\n\nim-j\n\n(im-j quaternion)\n\nReturn j imaginary part\nsource\n\n\n\nim-k\n\n(im-k quaternion)\n\nReturn k imaginary part\nsource\n\n\n\nimaginary?\n\n(imaginary? quaternion)\n\nIs q is a pure imaginary number?\nsource\n\n\n\ninf?\n\n(inf? quaternion)\n\nIs infinitive?\nsource\n\n\n\ninvalid?\n\n(invalid? z)\n\nIs NaN or Inf?\nsource\n\n\n\nlog\n\n(log q)\n\nlog(q)\nsource\n\n\n\nlogb\n\n(logb quaternion b)\n\nlog with base b\nsource\n\n\n\nmult\n\n(mult q1 q2)\n\nMultiplication of two quaternions.\nsource\n\n\n\nnan?\n\n(nan? quaternion)\n\nIs NaN?\nsource\n\n\n\nneg\n\n(neg quaternion)\n\nNegation of the quaternion.\nsource\n\n\n\nnorm\n\n(norm quaternion)\n\nNorm of the quaternion, length of the vector\nsource\n\n\n\nnormalize\n\n(normalize quaternion)\n\nNormalize quaternion, unit of quaternion.\nsource\n\n\n\npow\n\n(pow q p)\n\nQuaternion power\nsource\n\n\n\nqsgn\n\n(qsgn re im-i im-j im-k)\n(qsgn q)\n\nComputes the signum of a quaternion.\nReturns 0.0 for the zero quaternion (\\(0+0i+0j+0k\\)). For any other quaternion, returns the sign of its scalar part.\nsource\n\n\n\nquaternion\n\n(quaternion a b c d)\n(quaternion scalar [i j k])\n(quaternion a)\n\nCreate quaternion from individual values or scalar and vector parts, reprezented as Vec4.\nsource\n\n\n\nre\n\n(re quaternion)\n\nReturns scalar part of quaternion\nsource\n\n\n\nreal?\n\n(real? quaternion)\n\nIs q is a real number?\nsource\n\n\n\nreciprocal\n\n(reciprocal quaternion)\n\nReciprocal of the quaternion\nsource\n\n\n\nrotate\n\n(rotate in rotq)\n(rotate in angle u)\n\nRotate 3d in vector around axis u, the same as fastmath.vector/axis-rotate.\nsource\n\n\n\nrotation-quaternion\n\n(rotation-quaternion angle u)\n\nCreates a unit quaternion representing a rotation.\nThe rotation is defined by an angle (in radians) and a 3D vector u specifying the axis of rotation. The axis vector u is normalized internally to ensure a unit quaternion result.\nParameters:\n\nangle: The rotation angle in radians (double).\nu: The axis of rotation (Vec3). It will be normalized before use.\n\nReturns A unit quaternion (Vec4) representing the rotation.\nsource\n\n\n\nscalar\n\n(scalar quaternion)\n\nReturns scalar part of quaternion, double\nsource\n\n\n\nscale\n\n(scale quaternion scale)\n\nScale the quaternion\nsource\n\n\n\nsec\n\n(sec q)\n\nsec(q)\nsource\n\n\n\nsech\n\n(sech q)\n\nsech(q)\nsource\n\n\n\nsin\n\n(sin q)\n\nsin(q)\nsource\n\n\n\nsinh\n\n(sinh q)\n\nsinh(q)\nsource\n\n\n\nslerp\n\n(slerp q1 q2 t)\n\nPerforms Spherical Linear Interpolation (SLERP) between two quaternions.\nSLERP interpolates along the shortest arc on the unit sphere, providing smooth interpolation between rotations represented by unit quaternions.\nParameters:\n\nq1: The starting quaternion (Vec4).\nq2: The ending quaternion (Vec4).\nt: The interpolation parameter (double). Should be in the range [0.0, 1.0].\n\nIf t=0.0, returns q1.\nIf t=1.0, returns q2.\nFor 0.0 &lt; t &lt; 1.0, returns an interpolated quaternion. The parameter t is internally constrained to [0.0, 1.0].\n\n\nNote: This function is typically used with unit quaternions for rotation interpolation.\nsource\n\n\n\nsq\n\n(sq quaternion)\n\nSquare of the quaternion.\nsource\n\n\n\nsqrt\n\n(sqrt q)\n\nsqrt(q)\nsource\n\n\n\nsub\n\n(sub q1 q2)\n\nDifference of two quaternions\nsource\n\n\n\ntan\n\n(tan q)\n\ntan(q)\nsource\n\n\n\ntanh\n\n(tanh q)\n\ntanh(q)\nsource\n\n\n\nto-angles\n\n(to-angles q)\n\nConverts a quaternion q to Tait–Bryan angles using the z-y′-x’’ intrinsic rotation sequence. The input quaternion is normalized before calculation. Returns a Vec3 representing the angles [x y z]. These angles correspond to rotations around the local (body) x-axis, followed by the intermediate local y’-axis, and finally the local z’’-axis. Output angles are typically in the range [-pi, pi] for x and z, and [-pi/2, pi/2] for y.\nsource\n\n\n\nto-euler\n\n(to-euler q)\n\nConverts a quaternion q to ZYX (body 3-2-1) Euler angles. The input quaternion is normalized before calculation. Returns a Vec3 representing the angles [roll pitch yaw]. Roll is the angle around the x-axis, pitch around the y’-axis, and yaw around the z’’-axis. Output angles are typically in [-pi, pi] for roll/yaw and [-pi/2, pi/2] for pitch.\nsource\n\n\n\nto-rotation-matrix\n\n(to-rotation-matrix q)\n\nConverts a quaternion to a 3x3 rotation matrix.\nThe input quaternion is normalized internally to ensure a valid rotation matrix. Unit quaternions are used to represent rotations.\nParameters:\n\nq: The quaternion (Vec4) to convert.\n\nReturns A 3x3 rotation matrix (Mat3x3).\nsource\n\n\n\nvalid?\n\n(valid? z)\n\nIs valid complex (not NaN or Inf)?\nsource\n\n\n\nvector\n\n(vector quaternion)\n\nReturns vector part of quaternion, Vec3 type\nsource\n\n\n\nzero?\n\n(zero? quaternion)\n\nIs zero?\nsource\n\nsource: clay/complex_quaternions.clj",
    "crumbs": [
      "Complex numbers and quaternions"
    ]
  },
  {
    "objectID": "random.html",
    "href": "random.html",
    "title": "Random",
    "section": "",
    "text": "Common functions\nCollection of functions which deal with randomness. There are four groups:\nSet initial seed\nList of the functions which work with PRNG and distribution objects:",
    "crumbs": [
      "Random"
    ]
  },
  {
    "objectID": "random.html#common-functions",
    "href": "random.html#common-functions",
    "title": "Random",
    "section": "",
    "text": "Random number generation\n\n\n\n\n\n\nDefined functions\n\n\n\n\nirandom, lrandom, frandom, drandom\n\n\n\n\nirandom, for random integer, returns long\nlrandom, returns long\nfrandom, for random float, returns boxed float\ndrandom, random double\nFirst argument should be PRNG or distribution object\nWith no additional arguments, functions return:\n\nfull range for integers and longs\n\\([0,1)\\) for floats and doubles\n\nOnly for PRNGs\n\nWith one additional argument, number from \\([0,max)\\) range is returned\nWith two additional arguments, number form \\([min,max)\\) range is returned\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/irandom (r/rng :mersenne)) ;; =&gt; 831504811\n(r/drandom (r/rng :mersenne) 0 10) ;; =&gt; 2.549327739423648\n\n\n\nFor PRNGs, see more examples in PRNG &gt; Functions section.\nFor Distributions, see more examples in Distributions &gt; Functions &gt; Examples section.\n\n\nSampling\n\n\n\n\n\n\nDefined functions\n\n\n\n\n-&gt;seq\n\n\n\nTo generate infinite lazy sequence of random values, call -&gt;seq on given PRNG or distribution. Second (optional) argument can limit number of returned numbers. Third (optional) argument controls sampling method (for given n number of samples), there are the following options:\n\n:antithetic - random values in pairs [r1,1-r1,r2,1-r2,...]\n:uniform - spacings between numbers follow uniform distribution\n:systematic - the same spacing with random starting point\n:stratified - divide \\([0,1]\\) into n intervals and get random value from each subinterval\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(take 3 (r/-&gt;seq (r/rng :mersenne))) ;; =&gt; (0.9214876931544849 0.8571843374173553 0.9969970144220404)\n(r/-&gt;seq (r/rng :mersenne) 3) ;; =&gt; (0.27221362546663275 0.03800939496126432 0.24552514870076503)\n\n\n\nFor PRNGs, see more examples in PRNG &gt; Functions section.\nFor Distributions, see more examples in Distributions &gt; Functions &gt; Examples section.\n\n\nSeed\n\n\n\n\n\n\nDefined functions\n\n\n\n\nset-seed!, set-seed\n\n\n\n\nset-seed! mutates object if it’s supported by underlying Java implementation, can also return newly created object.\nset-seed is implemented only for PRNGs currently\n\nSee examples in PRNG &gt; Seed section.",
    "crumbs": [
      "Random"
    ]
  },
  {
    "objectID": "random.html#prng",
    "href": "random.html#prng",
    "title": "Random",
    "section": "PRNG",
    "text": "PRNG\nRelated functions:\n\n\n\n\n\n\nDefined functions\n\n\n\n\nrng, synced-rng\ngrandom, brandom\nirand, lrand, frand, drand, grand, brand\n-&gt;seq\nset-seed, set-seed!\nrandval, flip, flipb, roll-a-dice\nrandval-rng, flip-rng, flipb-rng, roll-a-dice-rng\n\n\n\nRandom number generation is based on PRNG (Pseudorandom Numeber Generator) objects which are responsible for keeping the state. All PRNGs are based on Apache Commons Math 3.6.1 algorithms.\n\n\n\n\n\n\n\n\n\nAlgorithm\nDescription\n\n\n\n\n\n:mersenne\n\nMersenne Twister\n\n\n\n:isaac\n\nISAAC, cryptographically secure PRNG\n\n\n\n:well512a\n\nWELL \\(2^{512}-1\\) period, variant a\n\n\n\n:well1024a\n\nWELL \\(2^{1024}-1\\) period, variant a\n\n\n\n:well19937a\n\nWELL \\(2^{19937}-1\\) period, variant a\n\n\n\n:well19937c\n\nWELL \\(2^{19937}-1\\) period, variant c\n\n\n\n:well44497a\n\nWELL \\(2^{44497}-1\\) period, variant a\n\n\n\n:well44497b\n\nWELL \\(2^{44497}-1\\) period, variant b\n\n\n\n:jdk\n\njava.util.Random instance, thread safe\n\n\n\n\n\nTo create PRNG, call rng with algorithm name (as a keyword) and optional seed parameter. rng creates a PRNG with optional seed synced-rng wraps PRNG to ensure thread safety\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/rng :isaac) ;; =&gt; #object[org.apache.commons.math3.random.ISAACRandom 0x555a81ec \"org.apache.commons.math3.random.ISAACRandom@555a81ec\"]\n(r/rng :isaac 5336) ;; =&gt; #object[org.apache.commons.math3.random.ISAACRandom 0x60f3d10f \"org.apache.commons.math3.random.ISAACRandom@60f3d10f\"]\n(r/synced-rng (r/rng :isaac)) ;; =&gt; #object[org.apache.commons.math3.random.SynchronizedRandomGenerator 0x68a3855f \"org.apache.commons.math3.random.SynchronizedRandomGenerator@68a3855f\"]\n\n\n\n\nFunctions\nTwo additional functions are supported by PRNGs\n\ngrandom\n\n1-arity - returns random number from Normal (Gaussian) distribution, N(0,1)\n2-arity - N(0,stddev)\n3-arity - N(mean, stddev)\n\nbrandom\n\n1-arity - returns true/false with probability=0.5\n2-arity - returns true with given probability\n\n\nAll examples will use Mersenne Twister PRNG with random seed\n\n(def my-prng (r/rng :mersenne))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/irandom my-prng) ;; =&gt; 1583725514\n(r/irandom my-prng 5) ;; =&gt; 0\n(r/irandom my-prng 10 20) ;; =&gt; 16\n(r/lrandom my-prng) ;; =&gt; -1206173237915992054\n(r/lrandom my-prng 5) ;; =&gt; 0\n(r/lrandom my-prng 10 20) ;; =&gt; 14\n(r/frandom my-prng) ;; =&gt; 0.205284\n(r/frandom my-prng 2.5) ;; =&gt; 1.835058\n(r/frandom my-prng 10.1 10.11) ;; =&gt; 10.107537\n(r/drandom my-prng) ;; =&gt; 0.19604607646722827\n(r/drandom my-prng 2.5) ;; =&gt; 2.424977886502112\n(r/drandom my-prng 10.1 10.11) ;; =&gt; 10.102066365074132\n(r/grandom my-prng) ;; =&gt; 1.2037627372997057\n(r/grandom my-prng 20.0) ;; =&gt; 8.96187050896474\n(r/grandom my-prng 10.0 20.0) ;; =&gt; -6.228950866884318\n(r/brandom my-prng) ;; =&gt; false\n(r/brandom my-prng 0.01) ;; =&gt; false\n(r/brandom my-prng 0.99) ;; =&gt; true\n\n\n\n\n\nSampling\n\n\n\n\n\n\nExamples\n\n\n\n\n(take 2 (r/-&gt;seq my-prng)) ;; =&gt; (0.8414876485282521 0.8301999829385021)\n(r/-&gt;seq my-prng 2) ;; =&gt; (0.016607633344843498 0.7229342635196061)\n(r/-&gt;seq my-prng 5 :antithetic) ;; =&gt; (0.6186588441384513 0.3813411558615487 0.24598443873001008 0.7540155612699899 0.33122169929893186)\n(r/-&gt;seq my-prng 5 :uniform) ;; =&gt; (0.06266416426852474 0.19433617679731968 0.4320069326370171 0.6574570501997111 0.8836891348926493)\n(r/-&gt;seq my-prng 5 :systematic) ;; =&gt; (0.0022903724833732045 0.20229037248337323 0.40229037248337324 0.6022903724833732 0.8022903724833732)\n(r/-&gt;seq my-prng 5 :stratified) ;; =&gt; (0.1249556145225578 0.2878976517026268 0.4326546831305351 0.6612531658276928 0.9401056648492514)\n\n\n\n\n\nSeed\nLet’s define two copies of the same PRNG with the same seed. Second one is obtained by setting a seed new seed.\n\n(def isaac-prng (r/rng :isaac 9988))\n\n\n(def isaac-prng2 ;; new instance\n  (r/set-seed isaac-prng 12345))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/-&gt;seq isaac-prng 3) ;; =&gt; (0.2017025716542833 0.18388677470983206 0.956730341010493)\n(r/-&gt;seq isaac-prng2 3) ;; =&gt; (0.9750469483729258 0.8824640458238968 0.931330617691251)\n(r/-&gt;seq isaac-prng 3) ;; =&gt; (0.5016097251798766 0.9741775702598627 0.4876589891468033)\n(r/-&gt;seq isaac-prng2 3) ;; =&gt; (0.7878968013812606 0.4365686819291106 0.9175915936830081)\n\n\n\nLet’s now reseed both PRNGs\n\n(r/set-seed! isaac-prng 9988)\n\n\n#object[org.apache.commons.math3.random.ISAACRandom 0x55ac9f2f \"org.apache.commons.math3.random.ISAACRandom@55ac9f2f\"]\n\n\n(r/set-seed! isaac-prng2 9988)\n\n\n#object[org.apache.commons.math3.random.ISAACRandom 0x69dcdc0e \"org.apache.commons.math3.random.ISAACRandom@69dcdc0e\"]\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/-&gt;seq isaac-prng 3) ;; =&gt; (0.2017025716542833 0.18388677470983206 0.956730341010493)\n(r/-&gt;seq isaac-prng2 3) ;; =&gt; (0.2017025716542833 0.18388677470983206 0.956730341010493)\n\n\n\n\n\nDefault PRNG\nThere is defined one global variable default-rng which is synchonized :jvm PRNG. Following set of functions work on this particular PRNG. The are the same as xrandom, ie (irand) is the same as (irandom default-rng).\n\nirand, random integer, as long\nlrand, random long\nfrand, random float as boxed Float\ndrand, random double\ngrand, random gaussian\nbrand, random boolean\n-&gt;seq, returns infinite lazy sequence\nset-seed, seeds default-rng, returns new instance\nset-seed!, seeds default-rng and Smile’s RNG\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/irand) ;; =&gt; 105368218\n(r/irand 5) ;; =&gt; 3\n(r/irand 10 20) ;; =&gt; 19\n(r/lrand) ;; =&gt; -6326787748595902987\n(r/lrand 5) ;; =&gt; 4\n(r/lrand 10 20) ;; =&gt; 16\n(r/frand) ;; =&gt; 0.20756257\n(r/frand 2.5) ;; =&gt; 0.95548046\n(r/frand 10.1 10.11) ;; =&gt; 10.105913\n(r/drand) ;; =&gt; 0.5293197586726303\n(r/drand 2.5) ;; =&gt; 0.5856958321231587\n(r/drand 10.1 10.11) ;; =&gt; 10.10447314352286\n(r/grand) ;; =&gt; 2.1340019834469315\n(r/grand 20.0) ;; =&gt; -33.23574732491791\n(r/grand 10.0 20.0) ;; =&gt; 11.061399646894813\n(r/brand) ;; =&gt; false\n(r/brand 0.01) ;; =&gt; false\n(r/brand 0.99) ;; =&gt; true\n(take 3 (r/-&gt;seq)) ;; =&gt; (0.8820950564851541 0.8432355490117891 0.7757833765798361)\nr/default-rng ;; =&gt; #object[org.apache.commons.math3.random.JDKRandomGenerator 0x42829589 \"org.apache.commons.math3.random.JDKRandomGenerator@42829589\"]\n(r/set-seed) ;; =&gt; #object[org.apache.commons.math3.random.JDKRandomGenerator 0x7493cf64 \"org.apache.commons.math3.random.JDKRandomGenerator@7493cf64\"]\n(r/set-seed 9988) ;; =&gt; #object[org.apache.commons.math3.random.JDKRandomGenerator 0x5d8b4f71 \"org.apache.commons.math3.random.JDKRandomGenerator@5d8b4f71\"]\n(r/set-seed!) ;; =&gt; #object[org.apache.commons.math3.random.JDKRandomGenerator 0x42829589 \"org.apache.commons.math3.random.JDKRandomGenerator@42829589\"]\n(r/set-seed! 9988) ;; =&gt; #object[org.apache.commons.math3.random.JDKRandomGenerator 0x42829589 \"org.apache.commons.math3.random.JDKRandomGenerator@42829589\"]\n\n\n\nAdditionally there are some helpers:\n\nrandval, A macro, returns value with given probability (default true/false with prob=0.5)\nflip, Returns 1 with given probability (or 0)\nflipb, Returns true with given probability (default probability 0.5)\nroll-a-dice, Returns a result of rolling a n-sides dice(s)\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/randval) ;; =&gt; false\n(r/randval 0.99) ;; =&gt; true\n(r/randval 0.01) ;; =&gt; false\n(r/randval :value1 :value2) ;; =&gt; :value2\n(r/randval 0.99 :highly-probable-value :less-probably-value) ;; =&gt; :highly-probable-value\n(r/randval 0.01 :less-probably-value :highly-probable-value) ;; =&gt; :highly-probable-value\n(repeatedly 10 r/flip) ;; =&gt; (0 1 1 1 0 0 1 0 0 0)\n(repeatedly 10 (fn* [] (r/flip 0.8))) ;; =&gt; (1 1 1 1 1 1 1 0 1 1)\n(repeatedly 10 (fn* [] (r/flip 0.2))) ;; =&gt; (0 0 0 0 0 0 0 0 0 0)\n(repeatedly 10 r/flipb) ;; =&gt; (true true false true true false false true false true)\n(repeatedly 10 (fn* [] (r/flipb 0.8))) ;; =&gt; (true true false true false true true false true true)\n(repeatedly 10 (fn* [] (r/flipb 0.2))) ;; =&gt; (false false false false true false false false false false)\n(r/roll-a-dice 6) ;; =&gt; 6\n(r/roll-a-dice 100) ;; =&gt; 30\n(r/roll-a-dice 10 6) ;; =&gt; 31\n\n\n\nAbove helpers can accept custom PRNG\n\n(def isaac3-prng (r/rng :isaac 1234))\n\n\nrandval-rng, A macro, returns value with given probability (default true/false with prob=0.5)\nflip-rng, Returns 1 with given probability (or 0)\nflipb-rng, Returns true with given probability\nroll-a-dice-rng, Returns a result of rolling a n-sides dice(s)\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/randval-rng isaac3-prng) ;; =&gt; false\n(r/randval-rng isaac3-prng 0.99) ;; =&gt; true\n(r/randval-rng isaac3-prng 0.01) ;; =&gt; false\n(r/randval-rng isaac3-prng :value1 :value2) ;; =&gt; :value1\n(r/randval-rng isaac3-prng 0.99 :highly-probable-value :less-probably-value) ;; =&gt; :highly-probable-value\n(r/randval-rng isaac3-prng 0.01 :less-probably-value :highly-probable-value) ;; =&gt; :highly-probable-value\n(repeatedly 10 (fn* [] (r/flip-rng isaac3-prng))) ;; =&gt; (0 0 0 1 1 0 0 1 0 1)\n(repeatedly 10 (fn* [] (r/flip-rng isaac3-prng 0.8))) ;; =&gt; (1 0 0 0 1 1 1 1 1 1)\n(repeatedly 10 (fn* [] (r/flip-rng isaac3-prng 0.2))) ;; =&gt; (1 0 0 0 1 0 0 0 0 1)\n(repeatedly 10 (fn* [] (r/flipb-rng isaac3-prng))) ;; =&gt; (false false true false false true true false false true)\n(repeatedly 10 (fn* [] (r/flipb-rng isaac3-prng 0.8))) ;; =&gt; (false true false true true true true true false false)\n(repeatedly 10 (fn* [] (r/flipb-rng isaac3-prng 0.2))) ;; =&gt; (false false true true false false false false false false)\n(r/roll-a-dice-rng isaac3-prng 6) ;; =&gt; 1\n(r/roll-a-dice-rng isaac3-prng 100) ;; =&gt; 12\n(r/roll-a-dice-rng isaac3-prng 10 6) ;; =&gt; 30",
    "crumbs": [
      "Random"
    ]
  },
  {
    "objectID": "random.html#distributions",
    "href": "random.html#distributions",
    "title": "Random",
    "section": "Distributions",
    "text": "Distributions\nCollection of probability distributions.\nRelated functions:\n\n\n\n\n\n\nDefined functions\n\n\n\n\ndistribution, distribution?,\npdf, lpdf, observe1\ncdf, ccdf, icdf\nsample, dimensions, continuous?\nlog-likelihood, observe, likelihood\nmean, means, variance, covariance\nlower-bound, upper-bound\ndistribution-id, distribution-parameters\nintegrate-pdf\n\n\n\ndistribution is Distribution creator, a multimethod.\n\nFirst parameter is distribution as a :key.\nSecond parameter is a map with configuration.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/distribution :cauchy) ;; =&gt; #object[org.apache.commons.math3.distribution.CauchyDistribution 0x7369dd91 \"org.apache.commons.math3.distribution.CauchyDistribution@7369dd91\"]\n(r/distribution :cauchy {:median 2.0, :scale 2.0}) ;; =&gt; #object[org.apache.commons.math3.distribution.CauchyDistribution 0x7f19509 \"org.apache.commons.math3.distribution.CauchyDistribution@7f19509\"]\n\n\n\n\nFunctions\nHere are the quick description of available functions:\n\ndistribution?, checks if given object is a distribution\npdf, probability density (for continuous) or probability mass (for discrete)\nlpdf, log of pdf\nobserve1, log of pdf, alias of lpdf\ncdf, cumulative density or probability, distribution\nccdf, complementary cdf, 1-cdf\nicdf, inverse cdf, quantiles\nsample, returns random sample\ndimensions, number of dimensions\ncontinuous?, is distribution continuous (true) or discrete (false)?\nlog-likelihood, sum of lpdfs for given seq of samples\nobserve, macro version of log-likelihood\nlikelihood, exp of log-likelihood\nmean, distribution mean\nmeans, distribution means for multivariate distributions\nvariance, distribution variance\ncovariance, covariance for multivariate distributions\nlower-bound, support lower bound\nupper-bound, support upper bound\ndistribution-id, name of distribution\ndistribution-parameters, list of parameters\nintegrate-pdf, construct cdf and icdf out of pdf function\n\nNotes:\n\ndistribution-parameters by default returns only obligatory parameters, when last argument is true, returns also optional parameters, for example :rng\ndrandom, frandom, lrandom and irandom work only on univariate distributions\nlrandom and irandom use round-even to convert double to integer.\ncdf, ccdf and icdf are defined only for univariate distributions\n\n\nExamples\nLet’s define Beta, Dirichlet and Bernoulli distributions as examples of univariate continuous, multivariate continuous and univariate discrete.\n\nLog-logistic, univariate continuous\n\n(def log-logistic (r/distribution :log-logistic {:alpha 3 :beta 7}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/drandom log-logistic) ;; =&gt; 6.830798083482128\n(r/frandom log-logistic) ;; =&gt; 12.077189\n(r/lrandom log-logistic) ;; =&gt; 19\n(r/irandom log-logistic) ;; =&gt; 7\n(r/sample log-logistic) ;; =&gt; 14.419422937703711\n(r/distribution? log-logistic) ;; =&gt; true\n(r/distribution? 2.3) ;; =&gt; false\n(r/pdf log-logistic 1) ;; =&gt; 0.008695578691184421\n(r/lpdf log-logistic 1) ;; =&gt; -4.744940578912748\n(r/observe1 log-logistic 1) ;; =&gt; -4.744940578912748\n(r/cdf log-logistic 1) ;; =&gt; 0.0029069767441860456\n(r/ccdf log-logistic 1) ;; =&gt; 0.997093023255814\n(r/icdf log-logistic 0.002907) ;; =&gt; 1.0000026744341148\n(r/dimensions log-logistic) ;; =&gt; 1\n(r/continuous? log-logistic) ;; =&gt; true\n(r/log-likelihood log-logistic (range 1 10)) ;; =&gt; -24.684504975903153\n(r/observe log-logistic (range 1 10)) ;; =&gt; -24.684504975903153\n(r/likelihood log-logistic (range 1 10)) ;; =&gt; 1.9039507073388636E-11\n(r/mean log-logistic) ;; =&gt; 8.464397033093016\n(r/variance log-logistic) ;; =&gt; 46.85554132946835\n(r/lower-bound log-logistic) ;; =&gt; 0.0\n(r/upper-bound log-logistic) ;; =&gt; ##Inf\n(r/distribution-id log-logistic) ;; =&gt; :log-logistic\n(r/distribution-parameters log-logistic) ;; =&gt; [{:alpha 3, :beta 7}]\n(r/distribution-parameters log-logistic true) ;; =&gt; [:rng {:alpha 3, :beta 7}]\n\n\n\n\n\nDirichlet, multivariate continuous\n\n(def dirichlet (r/distribution :dirichlet {:alpha [1 2 3]}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample dirichlet) ;; =&gt; [0.19361278167995002 0.13229367709440795 0.674093541225642]\n(reduce + (r/sample dirichlet)) ;; =&gt; 1.0\n(r/distribution? dirichlet) ;; =&gt; true\n(r/pdf dirichlet [0.5 0.1 0.4]) ;; =&gt; 0.9600000000000001\n(r/lpdf dirichlet [0.5 0.1 0.4]) ;; =&gt; -0.040821994520255034\n(r/observe1 dirichlet [0.5 0.1 0.4]) ;; =&gt; -0.040821994520255034\n(r/dimensions dirichlet) ;; =&gt; 3\n(r/continuous? dirichlet) ;; =&gt; true\n(r/log-likelihood dirichlet (repeatedly 10 (fn* [] (r/sample dirichlet)))) ;; =&gt; 11.727981883790177\n(r/observe dirichlet (repeatedly 10 (fn* [] (r/sample dirichlet)))) ;; =&gt; 8.759078092782449\n(r/likelihood dirichlet (repeatedly 10 (fn* [] (r/sample dirichlet)))) ;; =&gt; 1091013.3705988028\n(r/means dirichlet) ;; =&gt; (0.16666666666666666 0.3333333333333333 0.5)\n(r/covariance dirichlet) ;; =&gt; [[0.019841269841269844 -0.007936507936507936 -0.011904761904761904] [-0.007936507936507936 0.03174603174603175 -0.023809523809523808] [-0.011904761904761904 -0.023809523809523808 0.03571428571428571]]\n(r/distribution-id dirichlet) ;; =&gt; :dirichlet\n(r/distribution-parameters dirichlet) ;; =&gt; [:alpha]\n(r/distribution-parameters dirichlet true) ;; =&gt; [:alpha :rng]\n\n\n\n\nmultivariate distributions don’t implement some of the functions, like drandom or cdf, icdf, etc.\n\n\n\nPoisson, univariate discrete\n\n(def poisson (r/distribution :poisson {:p 10}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/drandom poisson) ;; =&gt; 11.0\n(r/frandom poisson) ;; =&gt; 15.0\n(r/lrandom poisson) ;; =&gt; 10\n(r/irandom poisson) ;; =&gt; 6\n(r/sample poisson) ;; =&gt; 9\n(r/distribution? poisson) ;; =&gt; true\n(r/pdf poisson 5) ;; =&gt; 0.03783327480207072\n(r/lpdf poisson 5) ;; =&gt; -3.274566277811817\n(r/observe1 poisson 5) ;; =&gt; -3.274566277811817\n(r/cdf poisson 5) ;; =&gt; 0.0670859628790319\n(r/ccdf poisson 5) ;; =&gt; 0.9329140371209681\n(r/icdf poisson 0.999) ;; =&gt; 21\n(r/dimensions poisson) ;; =&gt; 1\n(r/continuous? poisson) ;; =&gt; false\n(r/log-likelihood poisson (range 1 10)) ;; =&gt; -35.34496599408817\n(r/observe poisson (range 1 10)) ;; =&gt; -35.34496599408817\n(r/likelihood poisson (range 1 10)) ;; =&gt; 4.46556387351644E-16\n(r/mean poisson) ;; =&gt; 10.0\n(r/variance poisson) ;; =&gt; 10.0\n(r/lower-bound poisson) ;; =&gt; 0.0\n(r/upper-bound poisson) ;; =&gt; 2.147483647E9\n(r/distribution-id poisson) ;; =&gt; :poisson\n(r/distribution-parameters poisson) ;; =&gt; [:p]\n(r/distribution-parameters poisson true) ;; =&gt; [:rng :p :epsilon :max-iterations]\n\n\n\n\n\n\nPDF integration\nintegrate-pdf returns a pair of CDF and iCDF functions using Romberg integration and interpolation. Given interval is divided into steps number of subinterval. Each subinteval is integrated and added to cumulative sum. All points a later interpolated to build CDF and iCDF.\nParameters:\n\npdf-func - univariate function, double-&gt;double\nmn - lower bound for integration, value of pdf-func should be 0.0 at this point\nmx - upper bound for integration\nsteps - how much subintervals to integrate (default 1000)\ninterpolator - interpolation method between integrated points (default :linear)\n\nLet’s compare cdf and icdf to integrated pdf of beta distribution\n\n(def beta (r/distribution :beta))\n\n\n(def integrated (r/integrate-pdf (partial r/pdf beta)\n                                 {:mn -0.001 :mx 1.001 :steps 10000\n                                  :interpolator :spline}))\n\n\n(def beta-cdf (first integrated))\n\n\n(def beta-icdf (second integrated))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/cdf beta 0.5) ;; =&gt; 0.890625\n(beta-cdf 0.5) ;; =&gt; 0.890624992926758\n(r/icdf beta 0.5) ;; =&gt; 0.26444998329511316\n(beta-icdf 0.5) ;; =&gt; 0.26444998499009115\n\n\n\n\n\n\nSampling\n\n(def weibull (r/distribution :weibull))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample weibull) ;; =&gt; 0.5388035684998905\n(r/-&gt;seq weibull 3) ;; =&gt; (1.208624065765813 0.5853224136589527 1.7680254729688276)\n(r/-&gt;seq weibull 5 :antithetic) ;; =&gt; (0.8789298248121516 0.7871567720020644 0.3359031152662685 1.4959111180822753 0.5761405777290171)\n(r/-&gt;seq weibull 5 :uniform) ;; =&gt; (0.27819628819981307 0.5105865483391676 0.6372189139644687 1.1425580683435526 1.7502730290537003)\n(r/-&gt;seq weibull 5 :systematic) ;; =&gt; (0.42415794076771685 0.6734882729621214 0.911929539913042 1.2027888245275842 1.8282741464907257)\n(r/-&gt;seq weibull 5 :stratified) ;; =&gt; (0.400168757730896 0.6539622749107047 0.891072131829612 1.1739728271224017 1.719290334861315)\n\n\n\n\n\nPRNG and seed\nAll distributions accept :rng optional parameter which is used internally for random number generation. By default, creator constructs custom PRNG instance.\n\n(def cauchy-mersenne-2288 (r/distribution :cauchy {:rng (r/rng :mersenne 2288)}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(vec (r/-&gt;seq cauchy-mersenne-2288 3)) ;; =&gt; [-0.1079136480774541 -5.558982159984645 -3.127061922456471]\n(vec (r/-&gt;seq cauchy-mersenne-2288 3)) ;; =&gt; [8.13804818209913 1.6141542337020716 -0.6694661861436221]\n(r/set-seed! cauchy-mersenne-2288 2288) ;; =&gt; #object[org.apache.commons.math3.distribution.CauchyDistribution 0x745a7bee \"org.apache.commons.math3.distribution.CauchyDistribution@745a7bee\"]\n(vec (r/-&gt;seq cauchy-mersenne-2288 3)) ;; =&gt; [-0.1079136480774541 -5.558982159984645 -3.127061922456471]\n\n\n\n\n\nDefault Normal\ndefault-normal is a public var which is a normal distribution N(0,1), thread-safe.\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/-&gt;seq r/default-normal 3) ;; =&gt; (-0.8916664792841649 -0.03329017964260368 0.21945999852141862)\n(r/pdf r/default-normal 0.0) ;; =&gt; 0.3989422804014327\n(r/cdf r/default-normal 0.0) ;; =&gt; 0.5\n(r/icdf r/default-normal 0.5) ;; =&gt; 0.0\n(r/mean r/default-normal) ;; =&gt; 0.0\n(r/variance r/default-normal) ;; =&gt; 1.0\n\n\n\n\n\nUnivariate, cont.\n\n\n\n\n\n\n\n\n\nname\nparameters\n\n\n\n\n\n:anderson-darling\n\n\n[:rng {:n 1.0}]\n\n\n\n\n:beta\n\n\n[:rng :alpha :beta :inverse-cumm-accuracy]\n\n\n\n\n:cauchy\n\n\n[:rng :median :scale :inverse-cumm-accuracy]\n\n\n\n\n:chi\n\n\n[:rng {:nu 1.0}]\n\n\n\n\n:chi-squared\n\n\n[:rng :degrees-of-freedom :inverse-cumm-accuracy]\n\n\n\n\n:chi-squared-noncentral\n\n\n[:rng {:nu 1.0, :lambda 1.0}]\n\n\n\n\n:cramer-von-mises\n\n\n[:rng {:n 1.0}]\n\n\n\n\n:erlang\n\n\n[:rng {:k 1, :lambda 1}]\n\n\n\n\n:exgaus\n\n\n[:mu :sigma :nu :rng]\n\n\n\n\n:exponential\n\n\n[:rng :mean :inverse-cumm-accuracy]\n\n\n\n\n:f\n\n\n[:rng :numerator-degrees-of-freedom :denominator-degrees-of-freedom :inverse-cumm-accuracy]\n\n\n\n\n:fatigue-life\n\n\n[:rng {:mu 0.0, :beta 1.0, :gamma 1.0}]\n\n\n\n\n:folded-normal\n\n\n[:rng {:mu 0.0, :sigma 1.0}]\n\n\n\n\n:frechet\n\n\n[:rng {:alpha 1.0, :beta 1.0, :delta 0.0}]\n\n\n\n\n:gamma\n\n\n[:rng :shape :scale :inverse-cumm-accuracy]\n\n\n\n\n:gumbel\n\n\n[:rng :mu :beta]\n\n\n\n\n:half-cauchy\n\n\n[:scale :rng]\n\n\n\n\n:half-normal\n\n\n[:sigma :rng]\n\n\n\n\n:hyperbolic-secant\n\n\n[:rng {:mu 0.0, :sigma 1.0}]\n\n\n\n\n:hypoexponential\n\n\n[:rng :lambdas]\n\n\n\n\n:hypoexponential-equal\n\n\n[:rng {:n 1.0, :k 1.0, :h 1.0}]\n\n\n\n\n:inverse-gamma\n\n\n[:rng {:alpha 2.0, :beta 1.0}]\n\n\n\n\n:inverse-gaussian\n\n\n[:rng {:mu 1.0, :lambda 1.0}]\n\n\n\n\n:johnson-sb\n\n\n[:rng {:gamma 0.0, :delta 1.0, :xi 0.0, :lambda 1.0}]\n\n\n\n\n:johnson-sl\n\n\n[:rng {:gamma 0.0, :delta 1.0, :xi 0.0, :lambda 1.0}]\n\n\n\n\n:johnson-su\n\n\n[:rng {:gamma 0.0, :delta 1.0, :xi 0.0, :lambda 1.0}]\n\n\n\n\n:kolmogorov\n\n\n[:rng]\n\n\n\n\n:kolmogorov-smirnov\n\n\n[:rng {:n 1.0}]\n\n\n\n\n:kolmogorov-smirnov+\n\n\n[:rng {:n 1.0}]\n\n\n\n\n:laplace\n\n\n[:rng :mu :beta]\n\n\n\n\n:levy\n\n\n[:rng :mu :c]\n\n\n\n\n:log-logistic\n\n\n[:rng {:alpha 3.0, :beta 1.0}]\n\n\n\n\n:log-normal\n\n\n[:rng :scale :shape :inverse-cumm-accuracy]\n\n\n\n\n:logistic\n\n\n[:rng :mu :s]\n\n\n\n\n:nakagami\n\n\n[:rng :mu :omega :inverse-cumm-accuracy]\n\n\n\n\n:normal\n\n\n[:rng :mu :sd :inverse-cumm-accuracy]\n\n\n\n\n:normal-inverse-gaussian\n\n\n[:rng {:alpha 1.0, :beta 0.0, :mu 0.0, :delta 1.0}]\n\n\n\n\n:pareto\n\n\n[:rng :scale :shape :inverse-cumm-accuracy]\n\n\n\n\n:pearson-6\n\n\n[:rng {:alpha1 1.0, :alpha2 1.0, :beta 1.0}]\n\n\n\n\n:power\n\n\n[:rng {:a 0.0, :b 1.0, :c 2.0}]\n\n\n\n\n:rayleigh\n\n\n[:rng {:a 0.0, :beta 1.0}]\n\n\n\n\n:reciprocal-sqrt\n\n\n[:a :rng]\n\n\n\n\n:t\n\n\n[:rng :degrees-of-freedom :inverse-cumm-accuracy]\n\n\n\n\n:triangular\n\n\n[:rng :a :c :b]\n\n\n\n\n:uniform-real\n\n\n[:rng :lower :upper]\n\n\n\n\n:watson-g\n\n\n[:rng {:n 2.0}]\n\n\n\n\n:watson-u\n\n\n[:rng {:n 2.0}]\n\n\n\n\n:weibull\n\n\n[:rng :alpha :beta :inverse-cumm-accuracy]\n\n\n\n\n:zaga\n\n\n[:mu :sigma :nu :lower-tail? :rng]\n\n\n\n\n\n\n\nAnderson-Darling\nDistribution of Anderson-Darling statistic \\(A^2\\) on \\(n\\) independent uniforms \\(U[0,1]\\).\n\nName: :anderson-darling\nDefault parameters:\n\n:n: \\(1\\)\n\nsource\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeta\n\nName: :beta\nDefault parameters:\n\n:alpha: \\(2.0\\)\n:beta: \\(5.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 1, :beta 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 0.5, :beta 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 3, :beta 3}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 5, :beta 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCauchy\n\nName :cauchy\nDefault parameters:\n\n:median, location: \\(0.0\\)\n:scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:median 0, :scale 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:median 1, :scale 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChi\n\nName: :chi\nDefault parameters:\n\n:nu, degrees of freedom: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:nu 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:nu 3}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChi-squared\n\nName: :chi-squered\nDefault parameters:\n\n:degrees-of-freedom: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:degrees-of-freedom 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:degrees-of-freedom 3}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChi-squared noncentral\n\nName: :chi-squared-noncentral\nDefault parameters:\n\n:nu, degrees-of-freedom: \\(1.0\\)\n:lambda, noncentrality: \\(1.0\\)\n\nsource\n\n\n\n\n\n\n\n\n\n\n\n\n{:nu 1, :lambda 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:nu 3, :lambda 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCramer-von Mises\nDistribution of Cramer-von Mises statistic \\(W^2\\) on \\(n\\) independent uniforms \\(U[0,1]\\).\n\nName: :cramer-von-mises\nDefault parameters\n\n:n: \\(1\\)\n\nsource\n\nNote: PDF is calculated using finite difference method from CDF.\n\n\n\n\n\n\n\n\n\n\n\n{:n 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nErlang\n\nName: :erlang\nDefault parameters\n\n:k, shape: \\(1.0\\)\n:lambda, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:k 1, :lambda 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:k 7, :lambda 2.0}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nex-Gaussian\n\nName: :exgaus\nDefault parameters\n\n:mu, mean: \\(5.0\\)\n:sigma, standard deviation: \\(1.0\\)\n:nu, mean of exponential variable: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0, :sigma 1, :nu 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu -2, :sigma 0.5, :nu 4}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExponential\n\nName: :exponential\nDefault parameters\n\n:mean: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mean 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mean 3}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\n\nName: :f\nDefault parameters\n\n:numerator-degrees-of-freedom: \\(1.0\\)\n:denominator-degrees-of-freedom: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:numerator-degrees-of-freedom 1, :denominator-degrees-of-freedom 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:numerator-degrees-of-freedom 10, :denominator-degrees-of-freedom 15}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFatigue life\n\nName: :fatigue-life\nDefault parameters\n\n:mu, location: \\(0.0\\)\n:beta, scale: \\(1.0\\)\n:gamma, shape: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0, :beta 1, :gamma 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu -1, :beta 3, :gamma 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFolded Normal\n\nName: :folded-normal\nDefault parameters\n\n:mu: \\(0.0\\)\n:sigma: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0, :sigma 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 2, :sigma 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrechet\n\nName: :frechet\nDefault parameters\n\n:delta, location: \\(0.0\\)\n:alpha, shape: \\(1.0\\)\n:beta, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:delta 0, :alpha 1, :beta 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:delta 1, :alpha 3, :beta 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGamma\n\nName: :gamma\nDefault parameters\n\n:shape: \\(2.0\\)\n:scale: \\(2.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:shape 2, :scale 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:shape 5, :scale 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGumbel\n\nName: :gumbel\nDefault parameters\n\n:mu, location: \\(1.0\\)\n:beta, scale: \\(2.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :beta 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :beta 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHalf Cauchy\n\nName: :half-cauchy\nDefault parameters\n\n:scale: \\(1.0\\)\n\ninfo\n\n\n\n\n\n\n\n\n\n\n\n\n{:scale 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:scale 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHalf Normal\n\nName: :half-normal\nDefault parameters\n\n:sigma: \\(1.0\\)\n\nwiki\n\n\n\n\n\n\n\n\n\n\n\n\n{:sigma 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:sigma 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperbolic secant\n\nName: :hyperbolic-secant\nDefault parameters\n\n:mu: \\(0.0\\)\n:sigma, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0, :sigma 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :sigma 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypoexponential\n\nName: :hypoexponential\nDefault parameters\n\n:lambdas, list of rates: [1.0]\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:lambdas [1]}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:lambdas [1 2 3 4 1]}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypoexponential equal\nHypoexponential distribution, where \\(\\lambda_i=(n+1-i)h,\\text{ for } i=1\\dots k\\)\n\nName: :hypoexponential-equal\nDefault parameters\n\n:k, number of rates: \\(1\\)\n:h, difference between rates: \\(1\\)\n:n \\(=\\frac{\\lambda_1}{h}\\): \\(1\\)\n\nsource\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 1, :k 1, :h 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 5, :h 0.5, :k 6}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInverse Gamma\n\nName: :inverse-gamma\nDefault parameters\n\n:alpha, shape: \\(2.0\\)\n:beta, scale: \\(1.0\\) *wiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 2, :beta 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 1.5, :beta 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInverse Gaussian\n\nName: :inverse-gaussian\nDefault parameters\n\n:mu, location: \\(1.0\\)\n:lambda, scale: \\(1.0\\) *wiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :lambda 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 2, :lambda 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJohnson Sb\n\nName: :johnson-sb\nDefault parameters\n\n:gamma, shape: \\(0.0\\)\n:delta, shape: \\(1.0\\)\n:xi, location: \\(0.0\\)\n:lambda, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:gamma 0, :delta 1, :xi 0, :lambda 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:gamma 0.5, :delta 2, :xi 0, :lambda 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJohnson Sl\n\nName: :johnson-sl\nDefault parameters\n\n:gamma, shape: \\(0.0\\)\n:delta, shape: \\(1.0\\)\n:xi, location: \\(0.0\\)\n:lambda, scale: \\(1.0\\)\n\nsource\n\n\n\n\n\n\n\n\n\n\n\n\n{:gamma 0, :delta 1, :xi 0, :lambda 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:gamma 0.5, :delta 2, :xi 0, :lambda 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJohnson Su\n\nName: :johnson-su\nDefault parameters\n\n:gamma, shape: \\(0.0\\)\n:delta, shape: \\(1.0\\)\n:xi, location: \\(0.0\\)\n:lambda, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:gamma 0, :delta 1, :xi 0, :lambda 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:gamma 0.5, :delta 2, :xi 0, :lambda 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov\n\nName: :kolmogorov\nwiki, info\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov\n\nName: :kolmogorov-smirnov\nDefault parameters\n\n:n, sample size: 1\n\nsource\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 10}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov+\n\nName: :kolmogorov-smirnov+\nDefault parameters\n\n:n, sample size: 1\n\nsource\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 10}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaplace\n\nName: :laplace\nDefault parameters\n\n:mu: \\(0.0\\)\n:beta, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0, :beta 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :beta 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevy\n\nName: :levy\nDefault parameters\n\n:mu: \\(0.0\\)\n:c, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0, :c 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :c 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog Logistic\n\nName: :log-logistic\nDefault parameters\n\n:alpha, shape: \\(3.0\\)\n:beta, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 3, :beta 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 5, :beta 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog Normal\n\nName: :log-normal\nDefault parameters\n\n:scale: \\(1.0\\)\n:shape: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:scale 1, :shape 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:scale 2, :shape 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic\n\nName: :logistic\nDefault parameters\n\n:mu, location: \\(0.0\\)\n:s, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0, :scale 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :scale 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNakagami\n\nName: :nakagami\nDefault parameters\n\n:mu, shape: \\(1.0\\)\n:omega, spread: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :omega 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.5, :omega 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormal\n\nName: :normal\nDefault parameters\n\n:mu, mean: \\(0.0\\)\n:sd, standard deviation: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0, :sd 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.5, :sd 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormal-Inverse Gaussian\n\nName: :normal-inverse-gaussian\nDefault parameters\n\n:alpha, tail heavyness: \\(1.0\\)\n:beta, asymmetry: \\(0.0\\)\n:mu, location: \\(0.0\\)\n:delta, scale: \\(1.0\\)\n\nwiki, source\n\nOnly PDF is supported, you may call integrate-pdf to get CDF and iCDF pair.\n\n\n\n\n\n\n\n\n\n\n{:alpha 1, :beta 0, :mu 0, :delta 1}\n\n\n{:alpha 2, :beta 1, :mu 0, :delta 0.5}\n\n\n\n\n\n\n\n\n\n\n\n(let [[cdf icdf] (r/integrate-pdf\n                  (partial r/pdf (r/distribution :normal-inverse-gaussian))\n                  {:mn -800.0 :mx 800.0 :steps 5000\n                   :interpolator :monotone})]\n  [(cdf 0.0) (icdf 0.5)])\n\n\n[0.5000000000001334 -2.5040386431030015E-13]\n\n\n\n\n\n\nCDF\niCDF\n\n\n\n\n\n\n\n\n\n\n\nPareto\n\nName: :pareto\nDefault parameters\n\n:shape: \\(1.0\\)\n:scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:scale 1, :shape 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:scale 2, :shape 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPearson VI\n\nName: :pearson-6\nDefault parameters:\n\n:alpha1: \\(1.0\\)\n:alpha2: \\(1.0\\)\n:beta: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha1 1, :alpha2 1, :beta 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:scale 2, :shape 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower\n\nName: :power\nDefault parameters\n\n:a: \\(0.0\\)\n:b: \\(1.0\\)\n:c: \\(2.0\\)\n\nsource\n\n\n\n\n\n\n\n\n\n\n\n\n{:a 0, :b 1, :c 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:a 1, :b 2, :c 1.25}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRayleigh\n\nName: :rayleigh\nDefault parameters\n\n:a, location: \\(0.0\\)\n:beta, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:a 0, :b 1, :c 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:a 1, :b 2, :c 1.25}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReciprocal Sqrt\n\\(\\operatorname{PDF}(x)=\\frac{1}{\\sqrt{x}}, x\\in(a,(\\frac{1}{2}(1+2\\sqrt{a}))^2)\\)\n\nName: :reciprocal-sqrt\nDefault parameters\n\n:a, location, lower limit: \\(0.5\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:a 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:a 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent’s t\n\nName: :t\nDefault parameters\n\n:degrees-of-freedom: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:degrees-of-freedom 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:degrees-of-freedom 50}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTriangular\n\nName: :triangular\nDefault parameters\n\n:a, lower limit: \\(-1.0\\)\n:b, mode: \\(0.0\\)\n:c, upper limit: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:a -1, :b 1, :c 0}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:a -0.5, :b 1, :c 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniform\n\nName: :uniform-real\nDefault parameters\n\n:lower, lower limit: \\(0.0\\)\n:upper, upper limit: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:lower 0, :upper 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:lower -1, :upper 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWatson G\n\nName: :watson-g\nDefault parameters\n\n:n: \\(2\\)\n\nsource\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 10}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWatson U\n\nName: :watson-u\nDefault parameters\n\n:n: \\(2\\)\n\nsource\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:n 10}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeibull\n\nName: :weibull\nDefault parameters\n\n:alpha, shape: \\(2.0\\)\n:beta, scale: \\(1.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 2, :beta 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha 1.2, :beta 0.8}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero adjusted Gamma (zaga)\n\nName: :zaga\nDefault parameters:\n\n:mu, location: \\(0.0\\)\n:sigma, scale: \\(1.0\\)\n:nu, density at 0.0: \\(0.1\\)\n:lower-tail? - true\n\nsource, book\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0, :sigma 1, :nu 0.1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :sigma 1, :nu 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnivariate, discr.\n\n\n\n\n\n\n\n\n\nname\nparameters\n\n\n\n\n\n:bb\n\n\n[:mu :sigma :bd :rng]\n\n\n\n\n:bernoulli\n\n\n[:rng :trials :p]\n\n\n\n\n:binomial\n\n\n[:rng :trials :p]\n\n\n\n\n:fishers-noncentral-hypergeometric\n\n\n[:rng :ns :nf :n :omega]\n\n\n\n\n:geometric\n\n\n[:rng :p]\n\n\n\n\n:hypergeometric\n\n\n[:rng :population-size :number-of-successes :sample-size]\n\n\n\n\n:logarithmic\n\n\n[:rng :theta]\n\n\n\n\n:nbi\n\n\n[:mu :sigma :rng]\n\n\n\n\n:negative-binomial\n\n\n[:r :p :rng]\n\n\n\n\n:pascal\n\n\n[:rng :r :p]\n\n\n\n\n:poisson\n\n\n[:rng :p :epsilon :max-iterations]\n\n\n\n\n:uniform-int\n\n\n[:rng :lower :upper]\n\n\n\n\n:zabb\n\n\n[:mu :sigma :bd :nu :rng]\n\n\n\n\n:zabi\n\n\n[:mu :sigma :bd :rng]\n\n\n\n\n:zanbi\n\n\n[:mu :sigma :nu :rng]\n\n\n\n\n:zibb\n\n\n[:mu :sigma :bd :nu :rng]\n\n\n\n\n:zibi\n\n\n[:mu :sigma :bd :rng]\n\n\n\n\n:zinbi\n\n\n[:mu :sigma :nu :rng]\n\n\n\n\n:zip\n\n\n[:mu :sigma :rng]\n\n\n\n\n:zip2\n\n\n[:mu :sigma :rng]\n\n\n\n\n:zipf\n\n\n[:rng :number-of-elements :exponent]\n\n\n\n\n\n\n\nBeta Binomial (bb)\n\nName: :bb\nDefault parameters\n\n:mu, probability: \\(0.5\\)\n:sigma, dispersion: \\(1.0\\)\n:bd, binomial denominator: \\(10\\)\n\nwiki, source\n\nParameters \\(\\mu,\\sigma\\) in terms of \\(\\alpha, \\beta\\) (Wikipedia definition)\n\nprobability: \\(\\mu=\\frac{\\alpha}{\\alpha+\\beta}\\)\ndispersion: \\(\\sigma=\\frac{1}{\\alpha+\\beta}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.5, :sigma 1, :bd 10}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.65, :sigma 0.3, :bd 20}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli\nThe same as Binomial with trials=1.\n\nName: :bernoulli\nDefault parameters\n\n:p, probability, \\(0.5\\)\n\nwiki\n\n\n\n\n\n\n\n\n\n\n\n\n{:p 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:p 0.25}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial\n\nName: :binomial\nDefault parameters\n\n:p, probability: \\(0.5\\)\n:trials: \\(20\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:trials 20, :p 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:trials 50, :p 0.25}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFisher’s noncentral hypergeometric\n\nName: :fishers-noncentral-hypergeometric\nDefault parameters\n\n:ns, number of sucesses: \\(10\\)\n:nf, number of failures: \\(10\\)\n:n, sample size, (\\(n&lt;ns+nf\\)): \\(5\\)\n:omega, odds ratio: \\(1\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:ns 10, :nf 10, :n 5, :omega 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:ns 30, :nf 60, :n 20, :omega 0.75}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric\n\nName: :geometric\nDefault parameters\n\n:p, probability: \\(0.5\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:p 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:p 0.15}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypergeometric\n\nName: :hypergeometric\nDefault parameters\n\n:population-size: \\(100\\)\n:number-of-successes: $50%\n:sample-size: $25%\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:population-size 100, :number-of-successes 50, :sample-size 25}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:population-size 2000, :number-of-successes 20, :sample-size 200}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogarithmic\n\nName: :logarithmic\nDefault parameters\n\n:theta, shape: \\(0.5\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:theta 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:theta 0.99}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegative binomial\n\nName: :negative-binomial\nDefault parameters\n\n:r, number of successes: \\(20\\), can be a real number.\n:p, probability of success: \\(0.5\\)\n\nwiki\n\n\n\n\n\n\n\n\n\n\n\n\n{:r 20, :p 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:r 100, :p 0.95}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:r 21.2345, :p 0.7}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPascal\nThe same as :negative-binomial but r is strictly integer\n\nName: :pascal\nDefault parameters\n\n:r, number of successes: \\(20\\)\n:p, probability of success: \\(0.5\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:r 20, :p 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:r 100, :p 0.95}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\n\nName: :poisson\nDefault parameters\n\n:p, lambda, mean: \\(0.5\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:p 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:p 4}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniform\n\nName: :uniform-int\nDefault parameters\n\n:lower, lower bound: \\(0\\)\n:upper, upper bound: \\(2147483647\\)\n\nwiki,source\n\n\n\n\n\n\n\n\n\n\n\n\n{:lower 0, :upper 20}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:lower -5, :upper 5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Adjusted Beta Binomial (zabb)\n\nName: :zabb\nDefault parameters\n\n:mu, probability: \\(0.5\\)\n:sigma, dispersion: \\(0.1\\)\n:nu, probability at 0.0: \\(0.1\\)\n:bd, binomial denominator: \\(1\\)\n\nsource, book\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.5, :sigma 0.1, :bd 10, :nu 0.1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:nu 0.1, :mu 0.65, :sigma 0.3, :bd 20}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Adjusted Binomial (zabi)\n\nName: :zabi\nDefault parameters\n\n:mu, probability: \\(0.5\\)\n:sigma, probability at 0.0: \\(0.1\\)\n:bd, binomial denominator: \\(1\\)\n\nsource, book\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.5, :sigma 0.1, :bd 10}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.65, :sigma 0.3, :bd 20}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Adjusted Negative Binomial (zanbi)\n\nName: :zanbi\nDefault parameters\n\n:mu, mean: \\(1.0\\)\n:sigma, dispersion: \\(1.0\\)\n:nu, probability at 0.0: \\(0.3\\)\n\nsource, book\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :sigma 1, :nu 0.3}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:nu 0.1, :mu 2, :sigma 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Inflated Beta Binomial (zibb)\n\nName: :zibb\nDefault parameters\n\n:mu, probability: \\(0.5\\)\n:sigma, dispersion: \\(0.1\\)\n:nu, probability factor at 0.0: \\(0.1\\)\n:bd, binomial denominator: \\(1\\)\n\nsource, book\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.5, :sigma 0.5, :bd 10, :nu 0.1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:nu 0.1, :mu 0.65, :sigma 1, :bd 20}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Inflated Binomial (zibi)\n\nName: :zibi\nDefault parameters\n\n:mu, probability: \\(0.5\\)\n:sigma, probability factor at 0.0: \\(0.1\\)\n:bd, binomial denominator: \\(1\\)\n\nsource, book\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.5, :sigma 0.1, :bd 10}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 0.65, :sigma 0.3, :bd 20}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Inflated Negative Binomial (zinbi)\n\nName: :zinbi\nDefault parameters\n\n:mu, mean: \\(1.0\\)\n:sigma, dispersion: \\(1.0\\)\n:nu, probability factor at 0.0: \\(0.3\\)\n\nsource, book\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 1, :sigma 1, :nu 0.3}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:nu 0.1, :mu 2, :sigma 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Inflated Poisson (zip)\n\nName: :zip\nDefault parameters\n\n:mu, mean: \\(5\\)\n:sigma, probability at 0.0: \\(0.1\\)\n\nsource, book\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 5, :sigma 0.1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 2, :sigma 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Inflated Poisson, type 2 (zip2)\n\nName: :zip2\nDefault parameters\n\n:mu, mean: \\(5\\)\n:sigma, probability at 0.0: \\(0.1\\)\n\nsource, book\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 5, :sigma 0.1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:mu 2, :sigma 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZipf\n\nName: :zipf\nDefault parameters\n\n:number-of-elements: \\(100\\)\n:exponent: \\(3.0\\)\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n\n\n{:number-of-elements 100, :exponent 3}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{:number-of-elements 20, :exponent 0.5}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate\n\n\n\n\n\n\n\n\n\n\nname\nparameters\ncontinuous?\n\n\n\n\n\n:dirichlet\n\n\n[:alpha :rng]\n\ntrue\n\n\n\n:multi-normal\n\n\n[:means :covariances :rng]\n\ntrue\n\n\n\n:multinomial\n\n\n[:n :ps :rng]\n\nfalse\n\n\n\n\n\n\nDirichlet\n\nName: :dirichlet\nDefault parameters\n\n:alpha, concentration, vector: [1 1]\n\nwiki\n\nPlease note, PDF doesn’t validate input.\nProjections of the 2d and 3d Dirichlet distributions.\n\n2d case - all vectors \\([x,1-x]\\)\n3d case - all (supported) vectors \\([x,y,1-x-y]\\)\n\n\n\n\n\n\n\n\n\n\n\n\n{:alpha [0.6 0.6]}\n\n\n{:alpha [3 3]}\n\n\n{:alpha [0.5 2]}\n\n\n\n\n\n\n\n\n\n{:alpha [3 1 3]}\n\n\n{:alpha [3 3 3]}\n\n\n{:alpha [1 3 1]}\n\n\n\n\n\n\n\n\n\n\n\n\n(def dirichlet3 (r/distribution :dirichlet {:alpha [3 1 3]}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample dirichlet3) ;; =&gt; [0.554796832426502 0.09449962972281489 0.3507035378506831]\n(r/sample dirichlet3) ;; =&gt; [0.42998860588323373 0.191581286996475 0.37843010712029135]\n(r/pdf dirichlet3 [0.2 0.3 0.5]) ;; =&gt; 1.8000000000000003\n(r/pdf dirichlet3 [0.3 0.5 0.2]) ;; =&gt; 0.648\n(r/pdf dirichlet3 [0.5 0.2 0.3]) ;; =&gt; 4.05\n(r/means dirichlet3) ;; =&gt; (0.42857142857142855 0.14285714285714285 0.42857142857142855)\n(r/covariance dirichlet3) ;; =&gt; [[0.03061224489795918 -0.007653061224489796 -0.02295918367346939] [-0.007653061224489796 0.015306122448979591 -0.007653061224489796] [-0.02295918367346939 -0.007653061224489796 0.03061224489795918]]\n\n\n\n\n\nMulti normal\n\nName: :multi-normal\nDefault parameters\n\n:means, vector: [0 0]\n:covariances, vector of vectors (row-wise matrix): [[1 0] [0 1]]\n\nwiki, source\n\n\n\n\n\n\n\n\n\n\n{:means [0 0], :convariances [[1 0] [0 1]]}\n\n\n\n\n\n\n\n{:means [0.5 0], :convariances [[0.5 -0.1] [0.1 0.1]]}\n\n\n\n\n\n\n\n{:means [0 -0.5], :convariances [[1 0.2] [0.3 1]]}\n\n\n\n\n\n\n\n\n\n\n\nMultinomial\n\nName: :multinomial\nDefault parameters\n\n:ps, probabilities or weights, vector: [0.5 0.5]\n:trials: \\(20\\)\n\nwiki, source\n\n\n(def multinomial (r/distribution :multinomial {:trials 150 :ps [1 2 3 4 5]}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample multinomial) ;; =&gt; [8 18 17 38 69]\n(r/sample multinomial) ;; =&gt; [9 28 27 39 47]\n(r/pdf multinomial [10 10 10 10 110]) ;; =&gt; 3.9088379874482466E-28\n(r/pdf multinomial [10 20 30 40 50]) ;; =&gt; 8.791729390927823E-5\n(r/pdf multinomial [110 10 10 10 10]) ;; =&gt; 4.955040820983416E-98\n(r/means multinomial) ;; =&gt; (10.0 20.0 30.0 40.0 50.0)\n(r/covariance multinomial) ;; =&gt; [[9.333333333333334 -1.3333333333333333 -2.0 -2.6666666666666665 -3.333333333333333] [-1.3333333333333333 17.333333333333336 -4.0 -5.333333333333333 -6.666666666666666] [-2.0 -4.0 24.0 -8.0 -10.0] [-2.6666666666666665 -5.333333333333333 -8.0 29.333333333333336 -13.333333333333332] [-3.3333333333333335 -6.666666666666667 -10.0 -13.333333333333334 33.333333333333336]]\n\n\n\n\n\n\nMixture\n\n\n\n\n\n\n\n\n\nname\nparameters\n\n\n\n\n\n:mixture\n\n\n[:distrs :weights :rng]\n\n\n\n\n\n\nMixture distribution\nCreates a distribution from other distributions and weights.\n\nName: :mixture\nDefault parameters:\n\n:distrs, list of distributions: [default-normal]\n:weights, list of weights: [1.0]\n\nwiki\n\nPlease note: set-seed! doesn’t affect distributions which are part of the mixture\n\n(def three-normals\n  (r/distribution :mixture {:distrs [(r/distribution :normal {:mu -2 :sd 2})\n                                     (r/distribution :normal)\n                                     (r/distribution :normal {:mu 2 :sd 0.5})]\n                            :weights [2 1 3]}))\n\n\n(def mixture-of-three\n  (r/distribution :mixture {:distrs [(r/distribution :gamma)\n                                     (r/distribution :laplace)\n                                     (r/distribution :log-logistic)]\n                            :weights [2 1 3]}))\n\n\n\n\n\n\n\n\n\n\n\n\nthree normals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngamma, laplace and log-logistic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample mixture-of-three) ;; =&gt; 3.874021138411308\n(r/sample mixture-of-three) ;; =&gt; 2.4730276664059767\n(r/pdf mixture-of-three 0) ;; =&gt; 0.08333333333333333\n(r/pdf mixture-of-three 1) ;; =&gt; 0.45620084174033965\n(r/pdf mixture-of-three 2) ;; =&gt; 0.14666525453903217\n(r/cdf mixture-of-three 0) ;; =&gt; 0.08333333333333333\n(r/cdf mixture-of-three 1) ;; =&gt; 0.4160780500460631\n(r/cdf mixture-of-three 2) ;; =&gt; 0.6879135433937651\n(r/icdf mixture-of-three 0.01) ;; =&gt; -2.1202635780176586\n(r/icdf mixture-of-three 0.5) ;; =&gt; 1.2029317631660499\n(r/icdf mixture-of-three 0.99) ;; =&gt; 10.808075578087667\n(r/mean mixture-of-three) ;; =&gt; 1.937933121411406\n(r/variance mixture-of-three) ;; =&gt; 5.7869481264261236\n(r/lower-bound mixture-of-three) ;; =&gt; ##-Inf\n(r/upper-bound mixture-of-three) ;; =&gt; ##Inf\n\n\n\n\n\nTruncated\n\n\n\n\n\n\n\n\n\nname\nparameters\n\n\n\n\n\n:truncated\n\n\n[:distr :left :right :rng]\n\n\n\n\n\n\n\nName: :truncated\nDefault parameters\n\n:distr, distribution to truncate: default-normal\n:left, lower boundary\n:right, upper boundary\n\nwiki\n\nBy default boundaries are the same as boundaries from a distributions. This way you can make one side truncation.\nPlease note: derived mean or variance is not calculated. Also, set-seed! doesn’t affect original distribution.\n\n(def truncated-normal (r/distribution :truncated {:distr r/default-normal\n                                                  :left -2 :right 2}))\n\n\n(def left-truncated-laplace (r/distribution :truncated {:distr (r/distribution :laplace)\n                                                        :left -0.5}))\n\n\n\n\n\n\n\n\n\n\n\n\ntruncated normal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrucated levy (left side)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample truncated-normal) ;; =&gt; -0.6158738977263767\n(r/sample truncated-normal) ;; =&gt; 1.8994625350224648\n(r/pdf truncated-normal 2.0000001) ;; =&gt; 0.0\n(r/pdf truncated-normal 2.0) ;; =&gt; 0.0565646741125192\n(r/cdf truncated-normal 2.0) ;; =&gt; 1.0\n(r/cdf truncated-normal 0.0) ;; =&gt; 0.5000000000000001\n(map (partial r/icdf truncated-normal) [1.0E-4 0.5 0.9999]) ;; =&gt; (-1.9982352293163053 -1.3914582123358836E-16 1.9982352293163053)\n(r/lower-bound truncated-normal) ;; =&gt; -2.0\n(r/upper-bound truncated-normal) ;; =&gt; 2.0\n\n\n\n\n\nFrom data\nAll below distributions can be constructed from datasets or list of values with probabilities.\n\n\n\n\n\n\n\n\n\n\nname\nparameters\ncontinuous?\n\n\n\n\n\n:continuous-distribution\n\n\n[:data :steps :kde :bandwidth :rng]\n\n\ntrue\n\n\n\n\n:kde\n\n\n[:data :steps :kde :bandwidth :rng]\n\n\ntrue\n\n\n\n\n:empirical\n\n\n[:rng :bin-count :data]\n\n\ntrue\n\n\n\n\n:real-discrete-distribution\n\n\n[:data :probabilities :rng]\n\n\nfalse\n\n\n\n\n:integer-discrete-distribution\n\n\n[:data :probabilities :rng]\n\n\nfalse\n\n\n\n\n:categorical-distribution\n\n\n[:data :probabilities :rng]\n\n\nfalse\n\n\n\n\n:enumerated-real\n\n\n[:rng :data :probabilities]\n\n\nfalse\n\n\n\n\n:enumerated-int\n\n\n[:data :probabilities :rng]\n\n\nfalse\n\n\n\n\n\n\n\nContinuous\nContinous distribution build from data is based on KDE (Kernel Density Estimation) and PDF integration for CDF and iCDF. Mean and variance are calculated from samples.\n:continous-distribution and :kde are two names for the same distribution\n\nName: :continuous-distribution or :kde\nDefault parameters:\n\n:data, samples, sequence of numbers\n:kde, density estimation kernel: :epenechnikov\n:bandwidth, KDE bandwidth, smoothing parameter: nil (auto)\n:steps, number of steps for PDF integration: 5000\n:min-iterations, number of PDF integrator iterations: nil (default)\n:interpolator, CDF/iCDF interpolator for PDF integration: nil (default, :linear)\n\nwiki, pdf integration\n\n\n(def random-data (repeatedly 1000 (fn [] (+ (* (r/drand -2 2) (r/drand -2 2))\n                                            (m/sqrt (* (r/drand) (r/drand)))))))\n\n\n(def kde-distr (r/distribution :continuous-distribution {:data random-data}))\n\n\n\n\n\n\n\n\n\n\n\n\ndefault\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbandwidth=1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngaussian kernel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntriangular kernel, bandwidth=0.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample kde-distr) ;; =&gt; 1.4165101892026575\n(r/sample kde-distr) ;; =&gt; 1.88554358059656\n(r/pdf kde-distr 0.0) ;; =&gt; 0.2895732291670891\n(r/cdf kde-distr 0.0) ;; =&gt; 0.31552593767229803\n(map (partial r/icdf kde-distr) [1.0E-4 0.5 0.9999]) ;; =&gt; (-4.036565369328684 0.49883262884502827 5.012273231015241)\n(r/lower-bound kde-distr) ;; =&gt; -4.1635203556799745\n(r/upper-bound kde-distr) ;; =&gt; 5.139228217366505\n(r/mean kde-distr) ;; =&gt; 0.5262235226230889\n(r/variance kde-distr) ;; =&gt; 1.8233876483481286\n\n\n\n\nKernels\nDistributions for various kernels:\n\n:data: [-2 -2 -2 -1 0 1 2 -1 0 1 2 0 1 2 1 2 2]\n:steps: 100\n:bandwidth: auto\n\nfastmath.kernel/kernel-list contains three types of kernels: RBF, KDE and what we can call “vector kernels” which includes Marcer, positive definite, and similar.\nKDE kernels\nHere’s the list of KDE kernels\n:cauchy , :cosine , :epanechnikov , :gaussian , :laplace , :logistic , :quartic , :sigmoid , :silverman , :triangular , :tricube , :triweight , :uniform , :wigner\nKDE kernels distribution plots\n\n\n\n\n\n\n\n\n\n\n\n:cauchy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:cosine\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:epanechnikov\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:gaussian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:laplace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:logistic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:quartic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:sigmoid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:silverman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:triangular\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:tricube\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:triweight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:uniform\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:wigner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmpirical\nEmpirical distribution calculates PDF, CDF and iCDF from a histogram.\n\nName: :empirical\nDefault parameters\n\n:data, samples, sequence of numbers\n:bin-count, number of bins for histogram: 10% of the size of the data\n\nwiki, source\n\n\n(def empirical-distr (r/distribution :empirical {:data random-data}))\n\n\n\n\n\n\n\n\n\n\n\n\ndefault\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbin-count=10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample empirical-distr) ;; =&gt; -1.3490922120615432\n(r/sample empirical-distr) ;; =&gt; -0.5974950785774867\n(r/pdf empirical-distr 0.0) ;; =&gt; 0.3367652060230363\n(r/cdf empirical-distr 0.0) ;; =&gt; 0.3060844150814384\n(map (partial r/icdf empirical-distr) [1.0E-4 0.5 0.9999]) ;; =&gt; (-3.8445695885397138 0.49402601627672765 4.820277450226245)\n(r/lower-bound empirical-distr) ;; =&gt; -3.8445695885397138\n(r/upper-bound empirical-distr) ;; =&gt; 4.820277450226245\n(r/mean empirical-distr) ;; =&gt; 0.5262235226230891\n(r/variance empirical-distr) ;; =&gt; 1.8233876483481297\n\n\n\n\n\nDiscrete\n\nDefault parameters\n\n:data, sequence of numbers (integers/longs or doubles)\n:probabilities, optional, probabilities or weights\n\nwiki, source1, source2\n\nPlease note: data can contain duplicates.\nThere are four discrete distributions:\n\n:enumerated-int for integers, backed by Apache Commons Math\n:enumerated-real for doubles, backed by Apache Commons Math\n:integer-discrete-distribution - for longs, custom implementation\n:real-discrete-distribution - for doubles, custom implementation\n\nPlease note:\n\nApache Commons Math implementation have some issues with iCDF.\n:integer-discrete-distribution is backed by clojure.data.int-map\n\n\nDoubles\n\n(def data-doubles (repeatedly 100 #(m/sq (m/approx (r/drand 2.0) 1))))\n\n\n\n\n\n\n\n\n\n\n\n\n:real-discrete-distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:enumerated-real\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(def real-distr (r/distribution :real-discrete-distribution {:data [0.1 0.2 0.3 0.4 0.3 0.2 0.1]\n                                                             :probabilities [5 4 3 2 1 5 4]}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample real-distr) ;; =&gt; 0.2\n(r/sample real-distr) ;; =&gt; 0.2\n(r/pdf real-distr 0.1) ;; =&gt; 0.375\n(r/pdf real-distr 0.15) ;; =&gt; 0.0\n(r/cdf real-distr 0.2) ;; =&gt; 0.75\n(map (partial r/icdf real-distr) [1.0E-4 0.5 0.9999]) ;; =&gt; (0.1 0.2 0.4)\n(r/lower-bound real-distr) ;; =&gt; 0.1\n(r/upper-bound real-distr) ;; =&gt; 0.4\n(r/mean real-distr) ;; =&gt; 0.19583333333333333\n(r/variance real-distr) ;; =&gt; 0.008732638888888894\n\n\n\n\n\nIntegers / Longs\n\n(def data-ints (repeatedly 500 #(int (m/sqrt (r/drand 100.0)))))\n\n\n\n\n\n\n\n\n\n\n\n\n:integer-discrete-distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:enumerated-int\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(def int-distr (r/distribution :integer-discrete-distribution {:data [10 20 30 40 30 20 10]\n                                                               :probabilities [5 4 3 2 1 5 4]}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample int-distr) ;; =&gt; 30\n(r/sample int-distr) ;; =&gt; 20\n(r/pdf int-distr 10) ;; =&gt; 0.375\n(r/pdf int-distr 15) ;; =&gt; 0.0\n(r/cdf int-distr 20) ;; =&gt; 0.75\n(map (partial r/icdf int-distr) [1.0E-4 0.5 0.9999]) ;; =&gt; (10 20 40)\n(r/lower-bound int-distr) ;; =&gt; 10.0\n(r/upper-bound int-distr) ;; =&gt; 40.0\n(r/mean int-distr) ;; =&gt; 19.583333333333332\n(r/variance int-distr) ;; =&gt; 87.32638888888891\n\n\n\n\n\n\nCategorical\nCategorical distribution is a discrete distribution which accepts any data.\n\nName: :categorical-distribution\nDefault parameters:\n\n:data, sequence of any values\n:probabilities, optional, probabilities or weights\n\n\nOrder for CDF/iCDF is created by calling (distinct data). If sorted data is needed, external sort is necessary. lower-bound and upper-bound are not defined though.\n\n(def cat-distr (r/distribution :categorical-distribution {:data (repeatedly 100 #(rand-nth [:a :b nil \"s\"]))}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/sample cat-distr) ;; =&gt; nil\n(r/sample cat-distr) ;; =&gt; \"s\"\n(r/pdf cat-distr nil) ;; =&gt; 0.2800000000000001\n(r/pdf cat-distr \"ss\") ;; =&gt; 0.0\n(r/cdf cat-distr :b) ;; =&gt; 0.21000000000000005\n(map (partial r/icdf cat-distr) [1.0E-4 0.5 0.9999]) ;; =&gt; (:b \"s\" :a)",
    "crumbs": [
      "Random"
    ]
  },
  {
    "objectID": "random.html#sequences",
    "href": "random.html#sequences",
    "title": "Random",
    "section": "Sequences",
    "text": "Sequences\nRelated functions:\n\n\n\n\n\n\nDefined functions\n\n\n\n\nsequence-generator, jittered-sequence-generator\n\n\n\nSequence generators can create random or quasi random vector with certain property like low discrepancy or from ball/sphere.\nThere are two multimethods:\n\nsequence-generator, returns lazy sequence of generated vectors (for dim&gt;1) or primitives (for dim=1)\njittered-sequence-generator, returns lazy sequence like above and also add jittering, works only for low discrepancy sequences.\n\nParameters:\n\nseq-generator - generator name\ndimensions - vector dimensionality, 1 for primitive\njitter - only for jittered sequences, from 0.0 to 1.0, default 0.25\n\nFor given dimensionality, returns sequence of:\n\n1 - doubles\n2 - Vec2 type\n3 - Vec3 type\n4 - Vec4 type\nn&gt;4 - Clojure vector\n\nVec2, Vec3 and Vec4 are fixed size vectors optimized for speed. They act exactly like 2,3 and 4 elements Clojure vectors\n\nLow discrepancy\nThere are 3 types of sequences:\n\n:sobol - up to 1000 dimensions, wiki, source\n:halton - up to 40 dimensions, wiki, source\n:r2 - up to 15 dimensions, info\n\n1000 samples from each of the sequence type without and with jittering\n\n\n\n\n\n:sobol\n:halton\n:r2\n\n\n\n\n\n\n\n:sobol (jittered)\n:halton (jittered)\n:r2 (jittered)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(first (r/sequence-generator :sobol 4)) ;; =&gt; #vec4 [0.0, 0.0, 0.0, 0.0]\n(first (r/jittered-sequence-generator :sobol 4)) ;; =&gt; #vec4 [0.02900236218752402, 0.03321895859029258, 0.03343024406202439, 0.01735820373097604]\n(first (r/sequence-generator :halton 3)) ;; =&gt; #vec3 [0.0, 0.0, 0.0]\n(first (r/jittered-sequence-generator :halton 3)) ;; =&gt; #vec3 [0.00988332128494876, 0.163602670427742, 0.09647432740056015]\n(first (r/sequence-generator :r2 2)) ;; =&gt; #vec2 [0.2548776662466927, 0.06984029099805333]\n(first (r/jittered-sequence-generator :r2 2)) ;; =&gt; #vec2 [0.29704197409033695, 0.07653356059721561]\n\n\n\n15 dimensional sequence\n\n(take 2 (r/sequence-generator :r2 15))\n\n\n([0.45625055763798894 0.4144151289829652 0.37440997700257395 0.33615502811293263 0.2995737119048001 0.26459280788164197 0.231142298902816 0.19915523103853916 0.16856757955612012 0.13931812076922045 0.11134830949363828 0.08460216186433356 0.05902614327914302 0.03456906124489478 0.01118196291144713] [0.4125011152759779 0.32883025796593035 0.2488199540051479 0.17231005622586526 0.09914742380960018 0.029185615763283934 0.962284597805632 0.8983104620770783 0.8371351591122402 0.7786362415384409 0.7226966189872766 0.6692043237286671 0.6180522865582859 0.5691381224897896 0.5223639258228941])\n\nOne dimensional sequence is just a sequence of numbers\n\n(take 20 (r/sequence-generator :sobol 1))\n\n\n(0.0 0.5 0.75 0.25 0.375 0.875 0.625 0.125 0.1875 0.6875 0.9375 0.4375 0.3125 0.8125 0.5625 0.0625 0.09375 0.59375 0.84375 0.34375)\n\n\n\nSphere and ball\nUnit sphere or unit ball sequences can generate any dimension.\n\n:sphere - source\n:ball - dropped coordinates, info\n\n500 samples\n\n\n\n\n\n\n\n\n\n\n:sphere\n\n\n:ball\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(first (r/sequence-generator :sphere 4)) ;; =&gt; #vec4 [0.6660104296529389, 0.5123194276384198, -0.5184346693290323, 0.15869595235999656]\n(first (r/sequence-generator :ball 3)) ;; =&gt; #vec3 [0.6106439662537149, -0.19719273807051046, -0.5338472481721354]\n\n\n\n20 dimensional sequence\n\n(take 2 (r/sequence-generator :sphere 20))\n\n\n([0.06989556686898989 0.21329492562603777 0.09274137389610065 -0.040826737583005576 -0.11896236825433075 -0.28625964372254425 -0.2585089948452389 0.09263835856674177 -0.05108856652618556 -0.6646688468242976 -0.2300692204910127 -0.02106725747930995 -0.09358613515838518 -0.33646631194670895 -0.08906714735178296 -0.2854443588682922 -0.1017929066749205 -0.1652692224156813 0.003749209320535714 -0.1449384730998652] [0.07159271383242655 0.1040972645156023 0.3778901359225604 0.03751398139301165 0.2565355439941502 0.37013318008863444 -0.0390823853499183 -0.10700268174954537 -0.23153317912295723 -0.07048455811369976 0.5861893500027203 0.02488939121579818 -0.0015452921922422746 0.07850646314601553 -0.17832871105060458 0.24821438014604072 -0.09235751822307534 0.1321940792208099 -0.05418979951286866 -0.3044863277953992])\n\n\n\nUniform and Gaussian\nAdditionally uniform and gaussian N(0,1) sequences can generate any number of dimensions. They rely on default-rng\n\n:default - uniform distribution U(0,1)\n:gaussian - gaussian, normal distribution, N(0,1) for each dimension\n\n1000 samples\n\n\n\n\n\n\n\n\n\n\n:default\n\n\n:gaussian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(first (r/sequence-generator :default 4)) ;; =&gt; #vec4 [0.47512411831706514, 0.9192840007208322, 0.8915903760933624, 0.5999298799277241]\n(first (r/sequence-generator :gaussian 3)) ;; =&gt; #vec3 [-0.5036463956341344, -0.32895615013785706, 0.4149779826362254]",
    "crumbs": [
      "Random"
    ]
  },
  {
    "objectID": "random.html#noise",
    "href": "random.html#noise",
    "title": "Random",
    "section": "Noise",
    "text": "Noise\nValue, gradient and simplex noises plus various combinations. 1d ,2d and 3d versions are prepared.\nRelated functions:\n\n\n\n\n\n\nDefined functions\n\n\n\n\nsingle-noise, fbm-noise, billow-noise, ridgemulti-noise\nvnoise, noise, simplex\nrandom-noise-cfg, random-noise-fn\ndiscrete-noise\n\n\n\n\nGeneration\nThere are four main methods of noise creation:\n\nsingle-noise, single frequency (octave) noise\nfbm-noise, multi frequency (octaves), fractal brownian motion *.billow-noise, multi frequency, “billowy” noise\nridgemulti-noise, multi frequency, ridged multi-fractal\n\nEach noise can be configured in, here is the list of options:\n\n:seed - seed for noise randomness\n:noise-type\n\n:value - value noise\n:gradient (default) - gradient noise (Perlin)\n:simplex - OpenSimplex noise\n\n:interpolation - interpolation between knots, only for value and gradient noise\n\n:none\n:linear\n:hermite (default)\n:quintic\n\n:octaves (default: 6) - number of frequencies/octaves for multi frequency creators\n:lacunarity (default: 2) - noise length (1/frequency) for each octave\n:gain (default: 0.5) - amplitude factor for each octave\n:normalize? (default: true) - if true, range is [0,1], [-1,1] otherwise.\n\nmore info about octaves, gain and lacunarity.\n\nSingle\n\n(def single-g-noise (r/single-noise {:noise-type :gradient :seed 1}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(single-g-noise 0.2) ;; =&gt; 0.23759999999999998\n(single-g-noise 0.2 0.3) ;; =&gt; 0.48526400000000003\n(single-g-noise 0.2 0.3 0.4) ;; =&gt; 0.660938496\n\n\n\nSingle octave of simplex noise:\n\n\n\nValue and gradient single noise for different interpolations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:none\n\n\n:linear\n\n\n:hermite\n\n\n:quintic\n\n\n\nvalue\n\n\n\n\n\n\ngradient\n\n\n\n\n\n\n\n\n\n\n\nFBM\n\n(def fbm-noise (r/fbm-noise {:noise-type :gradient :octaves 3 :seed 1}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(fbm-noise 0.2) ;; =&gt; 0.7029714285714286\n(fbm-noise 0.2 0.3) ;; =&gt; 0.44446811428571426\n(fbm-noise 0.2 0.3 0.4) ;; =&gt; 0.54825344\n\n\n\n6 octave of simplex noise:\n\n\n\nValue and gradient FBM noise for different interpolations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:none\n\n\n:linear\n\n\n:hermite\n\n\n:quintic\n\n\n\nvalue\n\n\n\n\n\n\ngradient\n\n\n\n\n\n\n\n\n\nDifferent number of octaves for FBM gradient noise\n\n\n\n\n\noctaves=2\noctaves=4\noctaves=6\noctaves=8\n\n\n\n\n\n\n\n\n\n\n\nDifferent gains and lacunarities for FBM gradient noise\n\n\n\n\n\n\nlacunarity=0.5\nlacunarity=2\nlacunarity=5\nlacunarity=8\n\n\ngain=0.25\n\n\n\n\n\n\ngain=0.5\n\n\n\n\n\n\ngain=0.75\n\n\n\n\n\n\n\n\n\n\n\nBillow\n\n(def billow-noise (r/billow-noise {:seed 1}))\n\n\n\n\n\n\n\n\n\nsimplex noise\nvalue noise\ngradient noise, 1 octave\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(billow-noise 0.2) ;; =&gt; 0.3879619047619048\n(billow-noise 0.2 0.3) ;; =&gt; 0.1804361142857142\n(billow-noise 0.2 0.3 0.4) ;; =&gt; 0.11801290199365083\n\n\n\n\n\nRidged Multi\n\n(def ridgedmulti-noise (r/ridgedmulti-noise {:seed 1}))\n\n\n\n\n\n\n\n\n\nsimplex noise\nvalue noise\ngradient noise, 1 octave\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(ridgedmulti-noise 0.2) ;; =&gt; 0.33387061650044203\n(ridgedmulti-noise 0.2 0.3) ;; =&gt; 0.6479155481384621\n(ridgedmulti-noise 0.2 0.3 0.4) ;; =&gt; 0.786227445531883\n\n\n\n\n\n\nPredefined\nThere are three ready to use preconfigured noises:\n\nvnoise, FBM value noise, 6 octaves, hermite interpolation\nnoise, Perlin noise, FBM gradient noise, 6 octaves, quintic interpolation\nsimplex, FBM simplex noise, 6 octaves\n\n\n\n\n\n\n\n\n\n\n\n\nvnoise\n\n\nnoise\n\n\nsimplex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/vnoise 0.2) ;; =&gt; 0.4840309105744845\n(r/vnoise 0.2 0.3) ;; =&gt; 0.2780623431461534\n(r/vnoise 0.2 0.3 0.4) ;; =&gt; 0.25221316247851827\n(r/noise 0.2) ;; =&gt; 0.7482407619047619\n(r/noise 0.2 0.3) ;; =&gt; 0.5435882829206349\n(r/noise 0.2 0.3 0.4) ;; =&gt; 0.41990295926725485\n(r/simplex 0.2) ;; =&gt; 0.30590875038476195\n(r/simplex 0.2 0.3) ;; =&gt; 0.3195303464509513\n(r/simplex 0.2 0.3 0.4) ;; =&gt; 0.27202692960846564\n\n\n\n\n\nWarping\nWarp noise info\nwarp-noise-fn, create warp noise.\nDefault parameters:\nParameters:\n\nnoise function, default: vnoise\nscale factor, default: 4.0\ndepth (1 or 2), default 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvnoise\n\n\nnoise\n\n\nsimplex\n\n\n\nscale=2\n\n\n\n\n\nscale=4\n\n\n\n\n\n\n\n\n\n\nRandom configuration\nFor generative art purposes it’s good to generate random configuration and noise based on it.\n\nrandom-noise-cfg, create random configuration\nrandom-noise-fn, create random noise from random configuration\n\nOptional parameter is a map with values user wants to fix.\n\n(r/random-noise-cfg)\n\n\n{:interpolation :hermite, :warp-scale 0.0, :seed 665844940, :normalize? true, :noise-type :simplex, :lacunarity 1.90872786462909, :gain 0.24908653479282347, :generator :billow, :warp-depth 1, :octaves 6}\n\n\n(r/random-noise-cfg {:seed 1})\n\n\n{:interpolation :hermite, :warp-scale 4.0, :seed 1, :normalize? true, :noise-type :value, :lacunarity 1.8874943472437105, :gain 0.508204375355278, :generator :billow, :warp-depth 1, :octaves 3}\n\n\n(def some-random-noise (r/random-noise-fn {:seed 1}))\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(some-random-noise 0.2) ;; =&gt; 0.22782144922035752\n(some-random-noise 0.2 0.3) ;; =&gt; 0.12119809880961607\n(some-random-noise 0.2 0.3 0.4) ;; =&gt; 0.5632473316430661\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete noise\ndiscrete-noise is a 1d or 2d hashing function which hashes long or two longs and converts it to a double from [0,1] range.\n\n\n\n\n\n\nExamples\n\n\n\n\n(r/discrete-noise 100) ;; =&gt; 0.07493987729537295\n(r/discrete-noise 101) ;; =&gt; 0.9625321542669703\n(r/discrete-noise 200 100) ;; =&gt; 0.6713155553076955\n(r/discrete-noise 200 101) ;; =&gt; 0.22706653793671472",
    "crumbs": [
      "Random"
    ]
  },
  {
    "objectID": "random.html#reference",
    "href": "random.html#reference",
    "title": "Random",
    "section": "Reference",
    "text": "Reference\n\nfastmath.random\nVarious random and noise functions.\nNamespace defines various random number generators (RNGs), different types of random functions, sequence generators and noise functions.\n### RNGs\nYou can use a selection of various RNGs defined in Apache Commons Math library.\nCurrently supported RNGs:\n\n:jdk - default java.util.Random\n:mersenne - MersenneTwister\n:isaac - ISAAC\n:well512a, :well1024a, :well19937a, :well19937c, :well44497a, :well44497b - several WELL variants\n\nTo create your RNG use rng multimethod. Pass RNG name and (optional) seed. Returned RNG is equipped with RNGProto protocol with methods: irandom, lrandom, frandom drandom, grandom, brandom which return random primitive value with given RNG.\n(let [rng (rng :isaac 1337)]\n  (irandom rng))\nFor conveniency default RNG (:jdk) with following functions are created: irand, lrand, frand, drand, grand, brand.\nEach prefix denotes returned type:\n\ni - int\nl - long\nf - float\nd - double\ng - gaussian (double)\nb - boolean\n\nCheck individual function for parameters description.\n### Random Vector Sequences\nCouple of functions to generate sequences of numbers or vectors.\nTo create generator call sequence-generator with generator name and vector size. Following generators are available:\n\n:halton - Halton low-discrepancy sequence; range [0,1]\n:sobol - Sobol low-discrepancy sequence; range [0,1]\n:r2 - R2 low-discrepancy sequence; range [0,1], more…\n:sphere - uniformly random distributed on unit sphere\n:ball - uniformly random distributed from unit ball\n:gaussian - gaussian distributed (mean=0, stddev=1)\n:default - uniformly random; range:[0,1]\n\n:halton, :sobol and :r2 can be also randomly jittered according to this article. Call jittered-sequence-generator.\nAfter creation you get lazy sequence\n### Noise\nList of continuous noise functions (1d, 2d and 3d):\n\n:value - value noise\n:gradient - gradient noise (improved Ken Perlin version)\n:simplex - simplex noise\n\nFirst two (:value and :gradient) can use 4 different interpolation types: :none, :linear, :hermite (cubic) and :quintic.\nAll can be combined in following variants:\n\nNoise - pure noise value, create with single-noise\nFBM - fractal brownian motion, create with fbm-noise\nBillow - billow noise, billow-noise\nRidgedMulti - ridged multi, ridgedmulti-noise\n\nNoise creation requires detailed configuration which is simple map of following keys:\n\n:seed - seed as integer\n:noise-type - type of noise: :value, :gradient (default), :simplex\n:interpolation - type of interpolation (for value and gradient): :none, :linear, :hermite (default) or :quintic\n:octaves - number of octaves for combined noise (like FBM), default: 6\n:lacunarity - scaling factor for combined noise, default: 2.00\n:gain - amplitude scaling factor for combined noise, default: 0.5\n:normalize? - should be normalized to [0,1] range (true, default) or to [-1,1] range (false)\n\nFor usage convenience 3 ready to use functions are prepared. Returning value from [0,1] range:\n\nnoise - Perlin Noise (gradient noise, 6 octaves, quintic interpolation)\nvnoise - Value Noise (as in Processing, 6 octaves, hermite interpolation)\nsimplex - Simplex Noise (6 octaves)\n\nFor random noise generation you can use random-noise-cfg and random-noise-fn. Both can be feed with configuration. Additional configuration:\n\n:generator can be set to one of the noise variants, defaults to :fbm\n:warp-scale - 0.0 - do not warp, &gt;0.0 warp\n:warp-depth - depth for warp (default 1.0, if warp-scale is positive)\n\n#### Discrete Noise\ndiscrete-noise is a 1d or 2d hash function for given integers. Returns double from [0,1] range.\n### Distribution\nVarious real and integer distributions. See DistributionProto and RNGProto for functions.\nTo create distribution call distribution multimethod with name as a keyword and map as parameters.\n\n\n-&gt;seq\n\n(-&gt;seq)\n(-&gt;seq rng)\n(-&gt;seq rng n)\n(-&gt;seq rng n sampling-method)\n\nReturns lazy sequence of random samples (can be limited to optional n values).\nAdditionally one of the sampling methods can be provided, ie: :uniform, :antithetic, :systematic and :stratified.\nsource\n\n\n\nar\n\n(ar)\n(ar phi)\n(ar phi signal)\n\nsource\n\n\n\narfima\n\n(arfima)\n(arfima phi d theta)\n(arfima phi d theta signal)\n(arfima phi d dlimit theta signal)\n\nsource\n\n\n\narma\n\n(arma)\n(arma phi theta)\n(arma phi theta signal)\n\nsource\n\n\n\nball-random\n\n(ball-random dims)\n(ball-random rng dims)\n\nReturn random vector from a ball\nsource\n\n\n\nbillow-noise\n\n(billow-noise)\n(billow-noise cfg__75429__auto__)\n\nCreate billow-noise function with optional configuration.\nsource\n\n\n\nbrand\nRandom boolean with default RNG.\nReturns true or false with equal probability. You can set p probability for true\nsource\n\n\n\nbrandom\n\n(brandom rng)\n(brandom rng p)\n\nRandom boolean with provided RNG\nsource\n\n\n\nccdf\n\n(ccdf d v)\n\nComplementary cumulative probability.\nsource\n\n\n\ncdf\n\n(cdf d v)\n(cdf d v1 v2)\n\nCumulative probability.\nsource\n\n\n\ncontinuous?\n\n(continuous? d)\n\nDoes distribution support continuous domain?\nsource\n\n\n\ncovariance\n\n(covariance d)\n\nDistribution covariance matrix (for multivariate distributions)\nsource\n\n\n\ndefault-normal\nDefault normal distribution (u=0.0, sigma=1.0).\nsource\n\n\n\ndefault-rng\nDefault RNG - JDK\nsource\n\n\n\ndimensions\n\n(dimensions d)\n\nDistribution dimensionality\nsource\n\n\n\ndiscrete-noise\n\n(discrete-noise X Y)\n(discrete-noise X)\n\nDiscrete noise. Parameters:\n\nX (long)\nY (long, optional)\n\nReturns double value from [0,1] range\nsource\n\n\n\ndistribution\nCreate distribution object.\n\nFirst parameter is distribution as a :key.\nSecond parameter is a map with configuration.\n\nAll distributions accept rng under :rng key (default: default-rng) and some of them accept inverse-cumm-accuracy (default set to 1e-9).\nsource\n\n\n\ndistribution-id\n\n(distribution-id d)\n\nDistribution identifier as keyword.\nsource\n\n\n\ndistribution-parameters\n\n(distribution-parameters d)\n(distribution-parameters d all?)\n\nDistribution highest supported value.\nWhen all? is true, technical parameters are included, ie: :rng and :inverser-cumm-accuracy.\nsource\n\n\n\ndistribution?\n\n(distribution? distr)\n\nChecks if distr is a distribution object.\nsource\n\n\n\ndistributions-list\nList of distributions.\nsource\n\n\n\ndrand\n\n(drand)\n(drand mx)\n(drand mn mx)\n\nRandom double number with default RNG.\nAs default returns random double from [0,1) range. When mx is passed, range is set to [0, mx). When mn is passed, range is set to [mn, mx).\nsource\n\n\n\ndrandom\n\n(drandom rng)\n(drandom rng mx)\n(drandom rng mn mx)\n\nRandom double number with provided RNG\nsource\n\n\n\nfbm-noise\n\n(fbm-noise)\n(fbm-noise cfg__75429__auto__)\n\nCreate fbm-noise function with optional configuration.\nsource\n\n\n\nfi\n\n(fi)\n(fi d)\n(fi d signal)\n(fi d dlimit signal)\n\nsource\n\n\n\nflip\n\n(flip p)\n(flip)\n\nReturns 1 with given probability, 0 otherwise\nsource\n\n\n\nflip-rng\n\n(flip-rng rng p)\n(flip-rng rng)\n\nReturns 1 with given probability, 0 otherwise, for given rng\nsource\n\n\n\nflipb\n\n(flipb p)\n(flipb)\n\nReturns true with given probability, false otherwise\nsource\n\n\n\nflipb-rng\n\n(flipb-rng rng p)\n(flipb-rng rng)\n\nReturns true with given probability, false otherwise, for given rng\nsource\n\n\n\nfrand\n\n(frand)\n(frand mx)\n(frand mn mx)\n\nRandom double number with default RNG.\nAs default returns random float from [0,1) range. When mx is passed, range is set to [0, mx). When mn is passed, range is set to [mn, mx).\nsource\n\n\n\nfrandom\n\n(frandom rng)\n(frandom rng mx)\n(frandom rng mn mx)\n\nRandom double number with provided RNG\nsource\n\n\n\ngrand\n\n(grand)\n(grand stddev)\n(grand mean stddev)\n\nRandom gaussian double number with default RNG.\nAs default returns random double from N(0,1). When std is passed, N(0,std) is used. When mean is passed, distribution is set to N(mean, std).\nsource\n\n\n\ngrandom\n\n(grandom rng)\n(grandom rng stddev)\n(grandom rng mean stddev)\n\nRandom gaussian double number with provided RNG\nsource\n\n\n\nicdf\n\n(icdf d v)\n\nInverse cumulative probability\nsource\n\n\n\nintegrate-pdf\n\n(integrate-pdf pdf-func mn mx steps)\n(integrate-pdf pdf-func {:keys [mn mx steps interpolator], :or {mn 0.0, mx 1.0, steps 1000, interpolator :linear}, :as options})\n\nIntegrate PDF function, returns CDF and iCDF\nParameters: * pdf-func - univariate function * mn - lower bound for integration, value of pdf-func should be 0.0 at this point * mx - upper bound for integration * steps - how much subintervals to integrate (default 1000) * interpolator - interpolation method between integrated points (default :linear)\nAlso other integration related parameters are accepted (:gauss-kronrod integration is used).\nPossible interpolation methods: :linear (default), :spline, :monotone or any function from fastmath.interpolation\nsource\n\n\n\nirand\n\n(irand)\n(irand mx)\n(irand mn mx)\n\nRandom integer number with default RNG.\nAs default returns random integer from full integer range. When mx is passed, range is set to [0, mx). When mn is passed, range is set to [mn, mx).\nsource\n\n\n\nirandom\n\n(irandom rng)\n(irandom rng mx)\n(irandom rng mn mx)\n\nRandom integer number with provided RNG\nsource\n\n\n\njittered-sequence-generator\n\n(jittered-sequence-generator seq-generator dimensions)\n(jittered-sequence-generator seq-generator dimensions jitter)\n\nCreate jittered sequence generator.\nSuitable for :r2, :sobol and :halton sequences.\njitter parameter range is from 0 (no jitter) to 1 (full jitter). Default: 0.25.\nSee also sequence-generator.\nsource\n\n\n\nlikelihood\n\n(likelihood d vs)\n\nLikelihood of samples\nsource\n\n\n\nlog-likelihood\n\n(log-likelihood d vs)\n\nLog likelihood of samples\nsource\n\n\n\nlower-bound\n\n(lower-bound d)\n\nDistribution lowest supported value\nsource\n\n\n\nlpdf\n\n(lpdf d v)\n\nLog density\nsource\n\n\n\nlrand\n\n(lrand)\n(lrand mx)\n(lrand mn mx)\n\nRandom long number with default RNG.\nAs default returns random long from full integer range. When mx is passed, range is set to [0, mx). When mn is passed, range is set to [mn, mx).\nsource\n\n\n\nlrandom\n\n(lrandom rng)\n(lrandom rng mx)\n(lrandom rng mn mx)\n\nRandom long number with provided RNG\nsource\n\n\n\nma\n\n(ma)\n(ma theta)\n(ma theta signal)\n\nGenerates a Moving Average (MA) stochastic process.\nAn MA(q) process is a time series where the current value is a linear combination of past white noise error terms. This function generates a first-order MA(1) process by default, or an MA(q) process if a collection of theta coefficients is provided.\nThis function generates the process based on a provided sequence of white noise.\nParameters:\n\ntheta (optional, number or sequence of numbers): The MA coefficient(s) (θ). If a single number, generates an MA(1) process Yt = εt + θ₁εt-₁. If a sequence of numbers [θ₁, θ₂, ..., θq], generates an MA(q) process. The sequence is interpreted such that theta[i] is the coefficient for εt-(i+1). Defaults to 0.0 (pure white noise).\nsignal (optional, sequence of numbers): The underlying white noise series (εt). Defaults to a sequence of standard normal random numbers (white-noise).\n\nReturns a lazy sequence representing the generated MA process.\nSee also ar (Autoregressive process), arma (ARMA process), arfima (ARFIMA process), white-noise.\nsource\n\n\n\nmean\n\n(mean d)\n\nDistribution mean\nsource\n\n\n\nmeans\n\n(means d)\n\nDistribution means (for multivariate distributions)\nsource\n\n\n\nnoise\n\n(noise x)\n(noise x y)\n(noise x y z)\n\nImproved Perlin Noise.\n6 octaves, quintic interpolation.\nsource\n\n\n\nnoise-generators\nList of possible noise generators as a map of names and functions.\nsource\n\n\n\nnoise-interpolations\nList of possible noise interpolations as a map of names and values.\nsource\n\n\n\nnoise-types\nList of possible noise types as a map of names and values.\nsource\n\n\n\nobserve MACRO\n\n(observe d vs)\n\nLog likelihood of samples. Alias for log-likelihood.\nsource\n\n\n\nobserve1\n\n(observe1 d v)\n\nLog of probability/density of the value. Alias for lpdf.\nsource\n\n\n\npdf\n\n(pdf d v)\n\nDensity\nsource\n\n\n\nprobability\n\n(probability d v)\n\nProbability (PMF)\nsource\n\n\n\nrandom-noise-cfg\n\n(random-noise-cfg pre-config)\n(random-noise-cfg)\n\nCreate random noise configuration.\nOptional map with fixed values.\nsource\n\n\n\nrandom-noise-fn\n\n(random-noise-fn cfg)\n(random-noise-fn)\n\nCreate random noise function from all possible options.\nOptionally provide own configuration cfg. In this case one of 4 different blending methods will be selected.\nsource\n\n\n\nrandval MACRO\n\n(randval v1 v2)\n(randval prob v1 v2)\n(randval prob)\n(randval)\n\nReturn value with given probability (default 0.5)\nsource\n\n\n\nrandval-rng MACRO\n\n(randval-rng rng v1 v2)\n(randval-rng rng prob v1 v2)\n(randval-rng rng prob)\n(randval-rng rng)\n\nReturn value with given probability (default 0.5), for given rng\nsource\n\n\n\nridgedmulti-noise\n\n(ridgedmulti-noise)\n(ridgedmulti-noise cfg__75429__auto__)\n\nCreate ridgedmulti-noise function with optional configuration.\nsource\n\n\n\nrng\nCreate RNG for given name (as keyword) and optional seed. Return object enhanced with RNGProto. See: rngs-list for names.\nsource\n\n\n\nrngs-list\nList of all possible RNGs.\nsource\n\n\n\nroll-a-dice\n\n(roll-a-dice sides)\n(roll-a-dice dices sides)\n\nRoll a dice with given sides\nsource\n\n\n\nroll-a-dice-rng\n\n(roll-a-dice-rng rng sides)\n(roll-a-dice-rng rng dices sides)\n\nRoll a dice with given sides and given rng\nsource\n\n\n\nsample\n\n(sample d)\n\nRandom sample\nsource\n\n\n\nsequence-generator\nCreate Sequence generator. See sequence-generators-list for names.\nValues:\n\n:r2, :halton, :sobol, :default/:uniform - range [0-1] for each dimension\n:gaussian - from N(0,1) distribution\n:sphere - from surface of unit sphere (ie. euclidean distance from origin equals 1.0)\n:ball - from an unit ball\n\nPossible dimensions:\n\n:r2 - 1-15\n:halton - 1-40\n:sobol - 1-1000\nthe rest - 1+\n\nSee also jittered-sequence-generator.\nsource\n\n\n\nsequence-generators-list\nList of random sequence generator. See sequence-generator.\nsource\n\n\n\nset-seed\n\n(set-seed)\n(set-seed v)\n(set-seed rng v)\n\nCreate and return new RNG\nsource\n\n\n\nset-seed!\n\n(set-seed!)\n(set-seed! v)\n(set-seed! rng v)\n\nSets seed.\nsource\n\n\n\nsimplex\n\n(simplex x)\n(simplex x y)\n(simplex x y z)\n\nSimplex noise. 6 octaves.\nsource\n\n\n\nsingle-noise\n\n(single-noise)\n(single-noise cfg__75429__auto__)\n\nCreate single-noise function with optional configuration.\nsource\n\n\n\nsource-object\n\n(source-object d)\n\nReturns Java or proxy object from backend library (if available)\nsource\n\n\n\nsynced-rng\n\n(synced-rng m)\n(synced-rng m seed)\n\nCreate synchronized RNG for given name and optional seed. Wraps rng method.\nsource\n\n\n\nupper-bound\n\n(upper-bound d)\n\nDistribution highest supported value\nsource\n\n\n\nvariance\n\n(variance d)\n\nDistribution variance\nsource\n\n\n\nvnoise\n\n(vnoise x)\n(vnoise x y)\n(vnoise x y z)\n\nValue Noise.\n6 octaves, Hermite interpolation (cubic, h01).\nsource\n\n\n\nwarp-noise-fn\n\n(warp-noise-fn noise scale depth)\n(warp-noise-fn noise scale)\n(warp-noise-fn noise)\n(warp-noise-fn)\n\nCreate warp noise (see Inigo Quilez article).\nParameters:\n\nnoise function, default: vnoise\nscale factor, default: 4.0\ndepth (1 or 2), default 1\n\nNormalization of warp noise depends on normalization of noise function.\nsource\n\n\n\nwhite-noise\n\n(white-noise)\n(white-noise sigma)\n(white-noise rng sigma)\n\nGenerates Gaussian white noise.\nThe generated sequence consists of independent random numbers drawn from a normal distribution with mean 0.0 and a specified standard deviation.\nParameters:\n\nsigma (double, optional): The standard deviation of the generated noise. Defaults to 1.0 (standard normal distribution).\nrng (fastmath.random.IRandomGenerator, optional): The random number generator to use. Defaults to a new JDK generator (fastmath.random/rng :jdk).\n\nReturns a lazy sequence of random numbers.\nsource\n\nsource: clay/random.clj",
    "crumbs": [
      "Random"
    ]
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "Statistics",
    "section": "",
    "text": "Datasets\nThis notebook provides a comprehensive overview and examples of the statistical functions available in the fastmath.stats namespace. It covers a wide array of descriptive statistics, measures of spread, quantiles, moments, correlation, distance metrics, contingency table analysis, binary classification metrics, effect sizes, various statistical tests (normality, binomial, t/z, variance, goodness-of-fit, ANOVA, autocorrelation), time series analysis tools (ACF, PACF), data transformations, and histogram generation. Each section introduces the relevant concepts and demonstrates the use of fastmath.stats functions with illustrative datasets (mtcars, iris, winequality).\nTo illustrate statistical functions three dataset will be used:\nA fastmath.dev.dataset as ds will be used to access data.\nSelect the :mpg column from mtcars\nSelect the :mpg column with a row predicate\nGroup by the :am column and select :mpg",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#datasets",
    "href": "stats.html#datasets",
    "title": "Statistics",
    "section": "",
    "text": "mtcars\niris\nwinequality\n\n\n\n\n(ds/mtcars :mpg)\n\n\n(21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26 30.4 15.8 19.7 15 21.4)\n\n\n\n(ds/mtcars :mpg (fn [row] (and (&lt; (row :mpg) 20.0) (zero? (row :am)))))\n\n\n(18.7 18.1 14.3 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 13.3 19.2)\n\n\n\n(ds/by ds/mtcars :am :mpg)\n\n\n{1 (21 21 22.8 32.4 30.4 33.9 27.3 26 30.4 15.8 19.7 15 21.4), 0 (21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 21.5 15.5 15.2 13.3 19.2)}\n\n\nmtcars\n11 attributes of 32 different cars comparison\n\n\nnameqseccylamgeardispwtdrathpmpgvscarbMazda RX416.466141602.623.91102104Mazda RX4 Wag17.026141602.8753.91102104Datsun 71018.614141082.323.859322.811Hornet 4 Drive19.446032583.2153.0811021.411Hornet Sportabout17.028033603.443.1517518.702Valiant20.226032253.462.7610518.111Duster 36015.848033603.573.2124514.304Merc 240D20404146.73.193.696224.412Merc 23022.9404140.83.153.929522.812Merc 28018.3604167.63.443.9212319.214Merc 280C18.9604167.63.443.9212317.814Merc 450SE17.4803275.84.073.0718016.403Merc 450SL17.6803275.83.733.0718017.303Merc 450SLC18803275.83.783.0718015.203Cadillac Fleetwood17.988034725.252.9320510.404Lincoln Continental17.828034605.424321510.404Chrysler Imperial17.428034405.3453.2323014.704Fiat 12819.4741478.72.24.086632.411Honda Civic18.5241475.71.6154.935230.412Toyota Corolla19.941471.11.8354.226533.911Toyota Corona20.01403120.12.4653.79721.511Dodge Challenger16.878033183.522.7615015.502AMC Javelin17.38033043.4353.1515015.202Camaro Z2815.418033503.843.7324513.304Pontiac Firebird17.058034003.8453.0817519.202Fiat X1-918.9414791.9354.086627.311Porsche 914-216.7415120.32.144.43912602Lotus Europa16.941595.11.5133.7711330.412Ford Pantera L14.58153513.174.2226415.804Ferrari Dino15.56151452.773.6217519.706Maserati Bora14.68153013.573.543351508Volvo 142E18.64141212.784.1110921.412\n\n\n\n\niris\nSepal and Petal iris species comparison\n\n\nsepal-lengthsepal-widthpetal-lengthpetal-widthspecies5.13.51.40.2setosa4.931.40.2setosa4.73.21.30.2setosa4.63.11.50.2setosa53.61.40.2setosa5.43.91.70.4setosa4.63.41.40.3setosa53.41.50.2setosa4.42.91.40.2setosa4.93.11.50.1setosa5.43.71.50.2setosa4.83.41.60.2setosa4.831.40.1setosa4.331.10.1setosa5.841.20.2setosa5.74.41.50.4setosa5.43.91.30.4setosa5.13.51.40.3setosa5.73.81.70.3setosa5.13.81.50.3setosa5.43.41.70.2setosa5.13.71.50.4setosa4.63.610.2setosa5.13.31.70.5setosa4.83.41.90.2setosa531.60.2setosa53.41.60.4setosa5.23.51.50.2setosa5.23.41.40.2setosa4.73.21.60.2setosa4.83.11.60.2setosa5.43.41.50.4setosa5.24.11.50.1setosa5.54.21.40.2setosa4.93.11.50.1setosa53.21.20.2setosa5.53.51.30.2setosa4.93.11.50.1setosa4.431.30.2setosa5.13.41.50.2setosa53.51.30.3setosa4.52.31.30.3setosa4.43.21.30.2setosa53.51.60.6setosa5.13.81.90.4setosa4.831.40.3setosa5.13.81.60.2setosa4.63.21.40.2setosa5.33.71.50.2setosa53.31.40.2setosa73.24.71.4versicolor6.43.24.51.5versicolor6.93.14.91.5versicolor5.52.341.3versicolor6.52.84.61.5versicolor5.72.84.51.3versicolor6.33.34.71.6versicolor4.92.43.31versicolor6.62.94.61.3versicolor5.22.73.91.4versicolor523.51versicolor5.934.21.5versicolor62.241versicolor6.12.94.71.4versicolor5.62.93.61.3versicolor6.73.14.41.4versicolor5.634.51.5versicolor5.82.74.11versicolor6.22.24.51.5versicolor5.62.53.91.1versicolor5.93.24.81.8versicolor6.12.841.3versicolor6.32.54.91.5versicolor6.12.84.71.2versicolor6.42.94.31.3versicolor6.634.41.4versicolor6.82.84.81.4versicolor6.7351.7versicolor62.94.51.5versicolor5.72.63.51versicolor5.52.43.81.1versicolor5.52.43.71versicolor5.82.73.91.2versicolor62.75.11.6versicolor5.434.51.5versicolor63.44.51.6versicolor6.73.14.71.5versicolor6.32.34.41.3versicolor5.634.11.3versicolor5.52.541.3versicolor5.52.64.41.2versicolor6.134.61.4versicolor5.82.641.2versicolor52.33.31versicolor5.62.74.21.3versicolor5.734.21.2versicolor5.72.94.21.3versicolor6.22.94.31.3versicolor5.12.531.1versicolor5.72.84.11.3versicolor6.33.362.5virginica5.82.75.11.9virginica7.135.92.1virginica6.32.95.61.8virginica6.535.82.2virginica7.636.62.1virginica4.92.54.51.7virginica7.32.96.31.8virginica6.72.55.81.8virginica7.23.66.12.5virginica6.53.25.12virginica6.42.75.31.9virginica6.835.52.1virginica5.72.552virginica5.82.85.12.4virginica6.43.25.32.3virginica6.535.51.8virginica7.73.86.72.2virginica7.72.66.92.3virginica62.251.5virginica6.93.25.72.3virginica5.62.84.92virginica7.72.86.72virginica6.32.74.91.8virginica6.73.35.72.1virginica7.23.261.8virginica6.22.84.81.8virginica6.134.91.8virginica6.42.85.62.1virginica7.235.81.6virginica7.42.86.11.9virginica7.93.86.42virginica6.42.85.62.2virginica6.32.85.11.5virginica6.12.65.61.4virginica7.736.12.3virginica6.33.45.62.4virginica6.43.15.51.8virginica634.81.8virginica6.93.15.42.1virginica6.73.15.62.4virginica6.93.15.12.3virginica5.82.75.11.9virginica6.83.25.92.3virginica6.73.35.72.5virginica6.735.22.3virginica6.32.551.9virginica6.535.22virginica6.23.45.42.3virginica5.935.11.8virginica\n\n\n\n\nwinequality\nWhite Portugese Vinho Verde wine consisting set of quality parameters (physicochemical and sensory).\n\n\nData\nLet’s bind some common columns to a global vars\n\nmtcars\nMiles per US gallon\n\n(def mpg (ds/mtcars :mpg))\n\nHorsepower\n\n(def hp (ds/mtcars :hp))\n\nWeight of the car\n\n(def wt (ds/mtcars :wt))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niris\nSepal lengths\n\n(def sepal-lengths (ds/by ds/iris :species :sepal-length))\n\n\n(def setosa-sepal-length (sepal-lengths :setosa))\n\n\n(def virginica-sepal-length (sepal-lengths :virginica))\n\n\n\n\n\n\n\n\n\n\n\n\n\nSepal widths\n\n(def sepal-widths (ds/by ds/iris :species :sepal-width))\n\n\n(def setosa-sepal-width (sepal-widths :setosa))\n\n\n(def virginica-sepal-width (sepal-widths :virginica))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwinequality\n\n(def residual-sugar (ds/winequality \"residual sugar\"))\n\n\n(def alcohol (ds/winequality \"alcohol\"))",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#basic-descriptive-statistics",
    "href": "stats.html#basic-descriptive-statistics",
    "title": "Statistics",
    "section": "Basic Descriptive Statistics",
    "text": "Basic Descriptive Statistics\nGeneral summary statistics for describing the center and range of data.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nminimum, maximum\nsum\nmean\ngeomean, harmean, powmean\nmode, modes\nwmode, wmodes\nstats-map\n\n\n\n\nBasic\nThis section covers fundamental descriptive statistics including finding the smallest (minimum) and largest (maximum) values, and calculating the total (sum).\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/minimum mpg) ;; =&gt; 10.4\n(stats/maximum mpg) ;; =&gt; 33.9\n(stats/sum mpg) ;; =&gt; 642.8999999999999\n\n\n\nCompensated summation can be used to reduce numerical error. There are three algorithms implemented:\n\n:kahan: The classic algorithm using a single correction variable to reduce numerical error.\n:neumayer: An improvement on Kahan, also using one correction variable but often providing better accuracy.\n:klein: A higher-order method using two correction variables, typically offering the highest accuracy at a slight computational cost.\n\nAs you can see below, all compensated summation give accurate result for mpg data.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/sum mpg) ;; =&gt; 642.8999999999999\n(stats/sum mpg :kahan) ;; =&gt; 642.9\n(stats/sum mpg :neumayer) ;; =&gt; 642.9\n(stats/sum mpg :klein) ;; =&gt; 642.9\n\n\n\nBut here is the example in which normal summation and :kahan fails.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/sum [1.0 1.0E100 1.0 -1.0E100]) ;; =&gt; 0.0\n(stats/sum [1.0 1.0E100 1.0 -1.0E100] :kahan) ;; =&gt; 0.0\n(stats/sum [1.0 1.0E100 1.0 -1.0E100] :neumayer) ;; =&gt; 2.0\n(stats/sum [1.0 1.0E100 1.0 -1.0E100] :klein) ;; =&gt; 2.0\n\n\n\n\n\nMean\nThe mean is a measure of central tendency. fastmath.stats provides several types of means:\n\nArithmetic Mean (mean): The sum of values divided by their count. It’s the most common type of average.\n\n\\[\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\n\nGeometric Mean (geomean): The n-th root of the product of n numbers. Suitable for averaging ratios, growth rates, or values that are multiplicative in nature. Requires all values to be positive. It’s less affected by extreme large values than the arithmetic mean, but more affected by extreme small values.\n\n\\[G = \\left(\\prod_{i=1}^{n} x_i\\right)^{1/n} = \\exp\\left(\\frac{1}{n}\\sum_{i=1}^{n} \\ln(x_i)\\right)\\]\n\nHarmonic Mean (harmean): The reciprocal of the arithmetic mean of the reciprocals of the observations. Appropriate for averaging rates (e.g., speeds). It is sensitive to small values and requires all values to be positive and non-zero.\n\n\\[H = \\frac{n}{\\sum_{i=1}^{n} \\frac{1}{x_i}}\\]\n\nPower Mean (powmean): Also known as the generalized mean or Hölder mean. It generalizes the arithmetic, geometric, and harmonic means. Defined by a power parameter \\(p\\).\n\n\\[M_p = \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_i^p\\right)^{1/p} \\text{ for } (p \\neq 0)\\]\nSpecial cases:\n\n\\(p \\to 0\\): Geometric Mean\n\\(p = 1\\): Arithmetic Mean\n\\(p = -1\\): Harmonic Mean\n\\(p = 2\\): Root Mean Square (RMS)\n\\(p \\to \\infty\\): Maximum value\n\\(p \\to -\\infty\\): Minimum value\n\nThe behavior depends on \\(p\\): higher \\(p\\) gives more weight to larger values, lower \\(p\\) gives more weight to smaller values.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mean residual-sugar) ;; =&gt; 6.391414863209474\n(stats/geomean residual-sugar) ;; =&gt; 4.397022390150047\n(stats/harmean residual-sugar) ;; =&gt; 2.9295723194226815\n(stats/powmean residual-sugar ##-Inf) ;; =&gt; 0.6\n(stats/powmean residual-sugar -4.5) ;; =&gt; 1.5790401256763393\n(stats/powmean residual-sugar -1.0) ;; =&gt; 2.9295723194226815\n(stats/powmean residual-sugar 0.0) ;; =&gt; 4.397022390150047\n(stats/powmean residual-sugar 1.0) ;; =&gt; 6.391414863209474\n(stats/powmean residual-sugar 4.5) ;; =&gt; 12.228524559326921\n(stats/powmean residual-sugar ##Inf) ;; =&gt; 65.8\n\n\n\nAll values of power mean for residual-sugar data and range of the power from -5 to 5.\n\n\n\n\nWeighted\nEvery mean function accepts optional weights vector. Formulas for weighted means are as follows.\n\nArithmetic Mean (mean):\n\n\\[\\mu_w = \\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^{n} w_i}\\]\n\nGeometric Mean (geomean):\n\n\\[G_w = \\left(\\prod_{i=1}^{n} x_i^{w_i}\\right)^{1/\\sum w_i} = \\exp\\left(\\frac{\\sum_{i=1}^{n} w_i \\ln(x_i)}{\\sum_{i=1}^{n} w_i}\\right)\\]\n\nHarmonic Mean (harmean):\n\n\\[H_w = \\frac{\\sum_{i=1}^{n} w_i}{\\sum_{i=1}^{n} \\frac{w_i}{x_i}}\\]\n\nPower Mean (powmean):\n\n\\[M_{w,p} = \\left(\\frac{\\sum_{i=1}^{n} w_i x_i^p}{\\sum_{i=1}^{n} w_i}\\right)^{1/p} \\text{ for } (p \\neq 0)\\]\nLet’s calculate mean of hp (horsepower) weighted by wt (car weight).\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mean hp) ;; =&gt; 146.6875\n(stats/mean hp wt) ;; =&gt; 159.99440515968607\n(stats/geomean hp) ;; =&gt; 131.88367883954564\n(stats/geomean hp wt) ;; =&gt; 145.7870847521906\n(stats/harmean hp) ;; =&gt; 118.2288915187372\n(stats/harmean hp wt) ;; =&gt; 131.56286792601395\n(stats/powmean hp -4.5) ;; =&gt; 87.4433284945516\n(stats/powmean hp wt -4.5) ;; =&gt; 95.2308358812506\n(stats/powmean hp 4.5) ;; =&gt; 193.97486455878905\n(stats/powmean hp wt 4.5) ;; =&gt; 201.37510347727357\n\n\n\n\n\nExpectile\nThe expectile is a measure of location, related to both the [mean] and [quantile]. For a given level τ (tau, a value between 0 and 1), the τ-th expectile is the value t that minimizes an asymmetrically weighted sum of squared differences from t. This is distinct from quantiles, which minimize an asymmetrically weighted sum of absolute differences.\nA key property of expectiles is that the 0.5-expectile is identical to the arithmetic [mean]. As τ varies from 0 to 1, expectiles span a range of values, typically from the minimum (τ=0) to the maximum (τ=1) of the dataset. Like the mean, expectiles are influenced by the magnitude of all data points, making them generally more sensitive to outliers than corresponding quantiles (e.g., the median).\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/expectile residual-sugar 0.1) ;; =&gt; 2.794600951101003\n(stats/expectile residual-sugar 0.5) ;; =&gt; 6.391414863209474\n(stats/mean residual-sugar) ;; =&gt; 6.391414863209474\n(stats/expectile residual-sugar 0.9) ;; =&gt; 11.368419035291927\n(stats/expectile hp wt 0.25) ;; =&gt; 130.66198036981677\n\n\n\nPlotting expectiles for residual-sugar data across the range of τ from 0.0 to 1.0.\n\n\n\n\n\n\nMode\nThe mode is the value that appears most frequently in a dataset.\nFor numeric data, mode returns the first mode encountered, while modes returns a sequence of all modes (in increasing order for the default method).\nLet’s see that mode returns elements with the highest frequency of hp (showing only first 10 values)\n\n^:kind/hidden\n(-&gt;&gt; (frequencies hp)\n     (sort-by second &gt;)\n     (take 10))\n\n\n\n\n\n\nvalue\nfrequency\n\n\n\n\n110\n3\n\n\n175\n3\n\n\n180\n3\n\n\n150\n2\n\n\n66\n2\n\n\n245\n2\n\n\n123\n2\n\n\n65\n1\n\n\n62\n1\n\n\n205\n1\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mode hp) ;; =&gt; 110.0\n(stats/modes hp) ;; =&gt; (110.0 175.0 180.0)\n\n\n\nWhen dealing with data potentially from a continuous distribution, these functions can estimate the mode using different methods:\n\n:histogram: Mode(s) based on the peak(s) of a histogram.\n:pearson: Mode estimated using Pearson’s second skewness coefficient (mode ≈ 3 * median - 2 * mean).\n:kde: Mode(s) based on Kernel Density Estimation, finding original data points with the highest estimated density.\nThe default method finds the exact most frequent value(s), suitable for discrete data.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mode residual-sugar) ;; =&gt; 1.2\n(stats/mode residual-sugar :histogram) ;; =&gt; 1.700695134061569\n(stats/mode residual-sugar :pearson) ;; =&gt; 2.8171702735810538\n(stats/mode residual-sugar :kde) ;; =&gt; 1.65\n\n\n\nFor weighted data, or data of any type (not just numeric), use wmode and wmodes. wmode returns the first weighted mode (the one with the highest total weight encountered first), and wmodes returns all values that share the highest total weight. If weights are omitted, they default to 1.0 for each value, effectively calculating unweighted modes for any data type.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/wmode [:a :b :c :d] [1 2.5 1 2.5]) ;; =&gt; :b\n(stats/wmodes [:a :b :c :d] [1 2.5 1 2.5]) ;; =&gt; (:b :d)\n\n\n\n\n\nStats\nThe stats-map function provides a comprehensive summary of descriptive statistics for a given dataset. It returns a map where keys are statistic names (as keywords) and values are their calculated measures. This function is a convenient way to get a quick overview of the data’s characteristics.\nThe resulting map contains the following key-value pairs:\n\n:Size - The number of data points in the sequence.\n:Min - The smallest value in the sequence (see [minimum]).\n:Max - The largest value in the sequence (see [maximum]).\n:Range - The difference between the maximum and minimum values.\n:Mean - The arithmetic average of the values (see [mean]).\n:Median - The middle value of the sorted sequence (see [median]).\n:Mode - The most frequently occurring value(s) (see [mode]).\n:Q1 - The first quartile (25th percentile) of the data (see [percentile]).\n:Q3 - The third quartile (75th percentile) of the data (see [percentile]).\n:Total - The sum of all values in the sequence (see [sum]).\n:SD - The sample standard deviation, a measure of data dispersion around the mean.\n:Variance - The sample variance, the square of the standard deviation.\n:MAD - The Median Absolute Deviation, a robust measure of variability (see [median-absolute-deviation]).\n:SEM - The Standard Error of the Mean, an estimate of the standard deviation of the sample mean.\n:LAV - The Lower Adjacent Value, the smallest observation that is not an outlier (see [adjacent-values]).\n:UAV - The Upper Adjacent Value, the largest observation that is not an outlier (see [adjacent-values]).\n:IQR - The Interquartile Range, the difference between Q3 and Q1.\n:LOF - The Lower Outer Fence, a threshold for identifying extreme low outliers (Q1 - 3.0 * IQR).\n:UOF - The Upper Outer Fence, a threshold for identifying extreme high outliers (Q3 + 3.0 * IQR).\n:LIF - The Lower Inner Fence, a threshold for identifying mild low outliers (Q1 - 1.5 * IQR).\n:UIF - The Upper Inner Fence, a threshold for identifying mild high outliers (Q3 + 1.5 * IQR).\n:Outliers - A list of data points considered outliers (values outside the inner fences, see [outliers]).\n:Kurtosis - A measure of the “tailedness” or “peakedness” of the distribution (see [kurtosis]).\n:Skewness - A measure of the asymmetry of the distribution (see [skewness]).\n\n\n(stats/stats-map residual-sugar)\n\n\n{:IQR 8.200000000000001,\n :Kurtosis 3.4698201025634363,\n :LAV 0.6,\n :LIF -10.600000000000001,\n :LOF -22.900000000000002,\n :MAD 3.6,\n :Max 65.8,\n :Mean 6.391414863209486,\n :Median 5.2,\n :Min 0.6,\n :Mode 1.2,\n :Outliers (22.6 23.5 26.05 26.05 31.6 31.6 65.8),\n :Q1 1.7,\n :Q3 9.9,\n :Range 65.2,\n :SD 5.072057784014863,\n :SEM 0.07247276021182479,\n :Size 4898,\n :Skewness 1.0770937564241123,\n :Total 31305.150000000063,\n :UAV 22.0,\n :UIF 22.200000000000003,\n :UOF 34.5,\n :Variance 25.72577016438576}",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#quantiles-and-percentiles",
    "href": "stats.html#quantiles-and-percentiles",
    "title": "Statistics",
    "section": "Quantiles and Percentiles",
    "text": "Quantiles and Percentiles\nStatistics related to points dividing the distribution of data.\n\n\n\n\n\n\nDefined functions\n\n\n\n\npercentile, percentiles\nquantile, quantiles\nwquantile, wquantiles\nmedian, median-3\nwmedian\n\n\n\nQuantiles and percentiles are statistics that divide the range of a probability distribution into continuous intervals with equal probabilities, or divide the observations in a sample in the same way.\nfastmath.stats provides several functions to calculate these measures:\n\npercentile: Calculates the p-th percentile (a value from 0 to 100) of a sequence.\npercentiles: Calculates multiple percentiles for a sequence.\nquantile: Calculates the q-th quantile (a value from 0.0 to 1.0) of a sequence. This is equivalent to (percentile vs (* q 100.0)).\nquantiles: Calculates multiple quantiles for a sequence.\nmedian: Calculates the median (0.5 quantile or 50th percentile) of a sequence.\nmedian-3: A specialized function that calculates the median of exactly three numbers.\n\nAll percentile, quantiles, quantile, quantiles, and median functions accept an optional estimation-strategy keyword. This parameter determines how the quantile is estimated, particularly how interpolation is handled when the desired quantile falls between data points in the sorted sequence.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/quantile mpg 0.25) ;; =&gt; 15.274999999999999\n(stats/quantiles mpg [0.1 0.25 0.5 0.75 0.9]) ;; =&gt; [13.600000000000001 15.274999999999999 19.2 22.8 30.4]\n(stats/percentile mpg 50.0) ;; =&gt; 19.2\n(stats/percentiles residual-sugar [10 25 50 75 90]) ;; =&gt; [1.2 1.7 5.2 9.9 14.0]\n(stats/median mpg) ;; =&gt; 19.2\n(stats/median-3 15 5 10) ;; =&gt; 10.0\n\n\n\nThe available estimation-strategy values are :legacy (default), :r1, :r2, :r3, :r4, :r5, :r6, :r7, :r8 and :r9. Formulas for all of them can be found on this Wikipedia article. :legacy uses estimate of the form: \\(Q_p = x_{\\lceil p(N+1) - 1/2 \\rceil}\\)\nThe plot below illustrates the differences between these estimation strategies for the sample vs = [1 10 10 30].\n\n(def vs [1 10 10 30])\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :legacy) ;; =&gt; [1.0 3.25 10.0 25.0 30.0]\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :r1) ;; =&gt; [1.0 1.0 10.0 10.0 30.0]\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :r2) ;; =&gt; [1.0 5.5 10.0 20.0 30.0]\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :r3) ;; =&gt; [1.0 1.0 10.0 10.0 30.0]\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :r4) ;; =&gt; [1.0 1.0 10.0 10.0 22.0]\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :r5) ;; =&gt; [1.0 5.5 10.0 20.0 30.0]\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :r6) ;; =&gt; [1.0 3.25 10.0 25.0 30.0]\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :r7) ;; =&gt; [3.7 7.75 10.0 15.0 24.000000000000004]\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :r8) ;; =&gt; [1.0 4.749999999999998 10.0 21.66666666666667 30.0]\n(stats/quantiles vs [0.1 0.25 0.5 0.75 0.9] :r9) ;; =&gt; [1.0 4.9375 10.0 21.25 30.0]\n\n\n\n\nWeighted\nThere are also functions to calculate weighted quantiles and medians. These are useful when individual data points have different levels of importance or contribution.\n\nwquantile: Calculates the q-th weighted quantile for a sequence vs with corresponding weights.\nwquantiles: Calculates multiple weighted quantiles for a sequence vs with weights.\nwmedian: Calculates the weighted median (0.5 weighted quantile) for vs with weights.\n\nAll these functions accept an optional method keyword argument that specifies the interpolation strategy when a quantile falls between points in the weighted empirical cumulative distribution function (ECDF). The available methods are:\n\n:linear (Default): Performs linear interpolation between the data values corresponding to the cumulative weights surrounding the target quantile.\n:step: Uses a step function (specifically, step-before interpolation) based on the weighted ECDF. The result is the data value whose cumulative weight range includes the target quantile.\n:average: Computes the average of the step-before and step-after interpolation methods. This can be useful when a quantile corresponds exactly to a cumulative weight boundary.\n\nLet’s define a sample dataset and weights:\n\n(def sample-data [10 15 30 50 100])\n\n\n(def sample-weights [1 2 5 1 1])\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/wquantile sample-data sample-weights 0.25) ;; =&gt; 13.75\n(stats/wquantile sample-data sample-weights 0.5) ;; =&gt; 21.0\n(stats/wmedian sample-data sample-weights) ;; =&gt; 21.0\n(stats/wquantile sample-data sample-weights 0.75) ;; =&gt; 28.5\n(stats/wmedian sample-data sample-weights :linear) ;; =&gt; 21.0\n(stats/wmedian sample-data sample-weights :step) ;; =&gt; 30.0\n(stats/wmedian sample-data sample-weights :average) ;; =&gt; 22.5\n(stats/wquantiles sample-data sample-weights [0.2 0.5 0.8]) ;; =&gt; [12.5 21.0 30.0]\n(stats/wquantiles sample-data sample-weights [0.2 0.5 0.8] :step) ;; =&gt; [15.0 30.0 30.0]\n(stats/wquantiles sample-data sample-weights [0.2 0.5 0.8] :average) ;; =&gt; [12.5 22.5 30.0]\n\n\n\nUsing mpg data and wt (car weight) as weights:\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/wmedian mpg wt) ;; =&gt; 17.702906976744185\n(stats/wquantile mpg wt 0.25) ;; =&gt; 14.894033613445378\n(stats/wquantiles mpg wt [0.1 0.25 0.5 0.75 0.9] :average) ;; =&gt; [10.4 14.85 17.55 21.2 25.2]\n\n\n\nWhen weights are equal to 1.0, then:\n\n:linear method is the same as :r4 estimation strategy in quantiles\n:step is the same as :r1\n:average has no corresponding strategy\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/quantiles mpg [0.1 0.25 0.5 0.75 0.9] :r4) ;; =&gt; [13.5 15.2 19.2 22.8 29.78]\n(stats/wquantiles mpg (repeat (count mpg) 1.0) [0.1 0.25 0.5 0.75 0.9]) ;; =&gt; [13.5 15.2 19.2 22.8 29.78]\n(stats/quantiles mpg [0.1 0.25 0.5 0.75 0.9] :r1) ;; =&gt; [14.3 15.2 19.2 22.8 30.4]\n(stats/wquantiles mpg (repeat (count mpg) 1.0) [0.1 0.25 0.5 0.75 0.9] :step) ;; =&gt; [14.3 15.2 19.2 22.8 30.4]\n(stats/wquantiles mpg (repeat (count mpg) 1.0) [0.1 0.25 0.5 0.75 0.9] :average) ;; =&gt; [13.8 15.2 19.2 22.8 28.85]",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#measures-of-dispersiondeviation",
    "href": "stats.html#measures-of-dispersiondeviation",
    "title": "Statistics",
    "section": "Measures of Dispersion/Deviation",
    "text": "Measures of Dispersion/Deviation\nStatistics describing the spread or variability of data.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nvariance, population-variance\nstddev, population-stddev\nwvariance, population-wvariance\nwstddev, population-wstddev\npooled-variance, pooled-stddev\nvariation, l-variation\nmean-absolute-deviation\nmedian-absolute-deviation, mad\npooled-mad\nsem\n\n\n\n\nVariance and standard deviation\nVariance and standard deviation are fundamental measures of the dispersion or spread of a dataset around its mean.\n\nVariance quantifies the average squared difference of each data point from the mean. A higher variance indicates that the data points are more spread out, while a lower variance indicates they are clustered closer to the mean.\nStandard Deviation is the square root of the variance. It is expressed in the same units as the data, making it more interpretable than variance as a measure of spread.\nSample Variance (variance) and Sample Standard Deviation (stddev): These are estimates of the population variance and standard deviation, calculated from a sample of data. They use a denominator of \\(N-1\\) (Bessel’s correction) to provide an unbiased estimate of the population variance.\n\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\]\n\\[s = \\sqrt{s^2}\\]\nBoth functions can optionally accept a pre-calculated mean (mu) as a second argument.\n\nPopulation Variance (population-variance) and Population Standard Deviation (population-stddev): These are used when the data represents the entire population of interest, or when a biased estimate (maximum likelihood estimate) from a sample is desired. They use a denominator of \\(N\\).\n\n\\[\\sigma^2 = \\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}\\]\n\\[\\sigma = \\sqrt{\\sigma^2}\\]\nThese also accept an optional pre-calculated population mean (mu).\n\nWeighted Variance (wvariance, population-wvariance) and Weighted Standard Deviation (wstddev, population-wstddev): These calculate variance and standard deviation when each data point has an associated weight. For weighted sample variance (unbiased form, where \\(w_i\\) are weights):\n\n\\[\\bar{x}_w = \\frac{\\sum w_i x_i}{\\sum w_i}\\]\n\\[s_w^2 = \\frac{\\sum w_i (x_i - \\bar{x}_w)^2}{(\\sum w_i) - 1}\\]\nFor weighted population variance:\n\\[\\sigma_w^2 = \\frac{\\sum w_i (x_i - \\bar{x}_w)^2}{\\sum w_i}\\]\nWeighted standard deviations are the square roots of their respective variances.\n\nPooled Variance (pooled-variance) and Pooled Standard Deviation (pooled-stddev): These are used to estimate a common variance when data comes from several groups that are assumed to have the same population variance. Following methods can be used (where \\(k\\) is the number of groups, each with \\(n_i\\) number of observations and sample variance \\(s_i^2\\))\n\n:unbiased (default) \\[s_p^2 = \\frac{\\sum_{i=1}^{k} (n_i-1)s_i^2}{\\sum_{i=1}^{k} n_i - k}\\]\n:biased \\[s_p^2 = \\frac{\\sum_{i=1}^{k} (n_i-1)s_i^2}{\\sum_{i=1}^{k} n_i}\\]\n:avg - simple average of group variances. \\[s_p^2 = \\frac{\\sum_{i=1}^{k} s_i^2}{k}\\]\n\n\nPooled standard deviation is \\(\\sqrt{s_p^2}\\).\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/variance mpg) ;; =&gt; 36.32410282258065\n(stats/stddev mpg) ;; =&gt; 6.026948052089105\n(stats/population-variance mpg) ;; =&gt; 35.188974609375\n(stats/population-stddev mpg) ;; =&gt; 5.932029552301219\n(stats/variance mpg (stats/mean mpg)) ;; =&gt; 36.32410282258065\n(stats/population-variance mpg (stats/mean mpg)) ;; =&gt; 35.188974609375\n\n\n\nWeighted variance and standard deviation\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/wvariance hp wt) ;; =&gt; 4477.350800154701\n(stats/wstddev hp wt) ;; =&gt; 66.91300919966685\n(stats/population-wvariance hp wt) ;; =&gt; 4433.861107869416\n(stats/population-wstddev hp wt) ;; =&gt; 66.58724433305088\n\n\n\nPooled variance and standard deviation\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/pooled-variance [setosa-sepal-length virginica-sepal-length]) ;; =&gt; 0.264295918367347\n(stats/pooled-stddev [setosa-sepal-length virginica-sepal-length]) ;; =&gt; 0.5140971876672221\n(stats/pooled-variance [setosa-sepal-length virginica-sepal-length] :biased) ;; =&gt; 0.2590100000000001\n(stats/pooled-variance [setosa-sepal-length virginica-sepal-length] :avg) ;; =&gt; 0.264295918367347\n\n\n\nBeyond variance and standard deviation, we have three additional functions:\n\nCoefficient of Variation (variation): This is a standardized measure of dispersion, calculated as the ratio of the standard deviation \\(s\\) to the mean \\(\\bar{x}\\).\n\n\\[CV = \\frac{s}{\\bar{x}}\\]\nThe CV is unitless, making it useful for comparing the variability of datasets with different means or units. It’s most meaningful for data measured on a ratio scale (i.e., with a true zero point) and where all values are positive.\n\nStandard Error of the Mean (sem): The SEM estimates the standard deviation of the sample mean if you were to draw multiple samples from the same population. It indicates how precisely the sample mean estimates the true population mean.\n\n\\[SEM = \\frac{s}{\\sqrt{n}}\\]\nwhere \\(s\\) is the sample standard deviation and \\(n\\) is the sample size. A smaller SEM suggests a more precise estimate of the population mean.\n\nL-variation (l-variation): Calculates the coefficient of L-variation. This is a dimensionless measure of dispersion, analogous to the coefficient of variation.\n\n\\[\\tau_2 = \\lambda_2 / \\lambda_1\\]\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/variation mpg) ;; =&gt; 0.29998808160966145\n(stats/variation residual-sugar) ;; =&gt; 0.7935735502339006\n(stats/l-variation mpg) ;; =&gt; 0.1691378280874466\n(stats/l-variation residual-sugar) ;; =&gt; 0.43403938821477966\n(stats/sem mpg) ;; =&gt; 1.0654239593728148\n(stats/sem residual-sugar) ;; =&gt; 0.07247276021182479\n\n\n\n\n\nMAD\nMAD typically refers to Median Absolute Deviation, a robust measure of statistical dispersion. fastmath.stats also provides the Mean Absolute Deviation.\n\nMedian Absolute Deviation (median-absolute-deviation or mad): This is a robust measure of the variability of a univariate sample. It is defined as the median of the absolute deviations from the data’s median.\n\n\\[MAD = \\text{median}(|X_i - \\text{median}(X)|)\\]\nIf a specific center \\(c\\) is provided, it’s \\(MAD_c = \\text{median}(|X_i - c|)\\). Also, different estimation strategies can be used, see [median] MAD is less sensitive to outliers than the standard deviation.\n\nMean Absolute Deviation (mean-absolute-deviation): This measures variability as the average of the absolute deviations from a central point, typically the data’s mean.\n\n\\[MeanAD = \\frac{1}{n} \\sum_{i=1}^{n} |X_i - \\text{mean}(X)|\\]\nIf a specific center \\(c\\) is provided, it’s \\(MeanAD_c = \\frac{1}{n} \\sum_{i=1}^{n} |X_i - c|\\). MeanAD is more sensitive to outliers than MAD but less sensitive than the standard deviation.\n\nPooled MAD (pooled-mad): This function calculates a pooled estimate of the Median Absolute Deviation when data comes from several groups. For each group \\(i\\), absolute deviations from its median \\(M_i\\) are calculated: \\(Y_{ij} = |X_{ij} - M_i|\\). The pooled MAD is then the median of all such \\(Y_{ij}\\) values, scaled by a constant const (which defaults to approximately 1.4826, to make it comparable to the standard deviation for normal data).\n\n\\[PooledMAD = \\text{const} \\cdot \\text{median}(\\{Y_{ij} \\mid \\text{for all groups } i \\text{ and observations } j \\text{ in group } i\\})\\]\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mad mpg) ;; =&gt; 3.6500000000000004\n(stats/median-absolute-deviation mpg) ;; =&gt; 3.6500000000000004\n(stats/median-absolute-deviation mpg (stats/median mpg) :r3) ;; =&gt; 3.6000000000000014\n(stats/median-absolute-deviation mpg (stats/mean mpg)) ;; =&gt; 4.299999999999999\n(stats/mean-absolute-deviation mpg) ;; =&gt; 4.714453125\n(stats/mean-absolute-deviation mpg (stats/median mpg)) ;; =&gt; 4.634375\n(stats/pooled-mad [setosa-sepal-length virginica-sepal-length]) ;; =&gt; 0.4447806655516804\n(stats/pooled-mad [setosa-sepal-length virginica-sepal-length] 1.0) ;; =&gt; 0.2999999999999998",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#moments-and-shape",
    "href": "stats.html#moments-and-shape",
    "title": "Statistics",
    "section": "Moments and Shape",
    "text": "Moments and Shape\nMoments and shape statistics describe the form of a dataset’s distribution, particularly its symmetry and peakedness.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nmoment\nskewness\nkurtosis\nl-moment\n\n\n\n\nConventional Moments (moment)\nThe moment function calculates statistical moments of a dataset. Moments can be central (around the mean), raw (around zero), or around a specified center. They can also be absolute and/or normalized.\n\nk-th Central Moment: \\(\\mu_k = E[(X - \\mu)^k] \\approx \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^k\\). Calculated when center is nil (default) and :mean? is true (default).\nk-th Raw Moment (about origin): \\(\\mu'_k = E[X^k] \\approx \\frac{1}{n} \\sum_{i=1}^{n} x_i^k\\). Calculated if center is 0.0.\nk-th Absolute Central Moment: \\(E[|X - \\mu|^k] \\approx \\frac{1}{n} \\sum_{i=1}^{n} |x_i - \\bar{x}|^k\\). Calculated if :absolute? is true.\nNormalization: If :normalize? is true, the moment is divided by \\(\\sigma^k\\) (where \\(\\sigma\\) is the standard deviation), yielding a scale-invariant measure. For example, the 3rd normalized central moment is related to skewness, and the 4th to kurtosis.\nPower of sum of differences: If :mean? is false, the function returns the sum \\(\\sum (x_i - c)^k\\) (or sum of absolute values) instead of the mean.\n\nThe order parameter specifies \\(k\\). For example, the 2nd central moment for \\(k=2\\) is the variance.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/moment mpg 2) ;; =&gt; 35.188974609374995\n(stats/variance mpg) ;; =&gt; 36.32410282258065\n(stats/moment mpg 3) ;; =&gt; 133.68672198486328\n(stats/moment mpg 4 {:normalize? true}) ;; =&gt; 2.6272339701791085\n(stats/moment mpg 1 {:absolute? true, :center (stats/median mpg)}) ;; =&gt; 4.634375\n\n\n\n\n\nSkewness\nSkewness measures the asymmetry of a probability distribution about its mean. fastmath.stats/skewness offers several types:\nMoment-based (sensitive to outliers):\n\n:G1 (Default): Sample skewness based on the 3rd standardized moment, adjusted for sample bias (via Apache Commons Math).\n\n\\[G_1 = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n \\left(\\frac{x_i - \\bar{x}}{s}\\right)^3\\]\n\n:g1 or :pearson: Pearson’s moment coefficient of skewness, another bias-adjusted version of the 3rd standardized central moment \\(m_3\\).\n\n\\[g_1 = \\frac{m_3}{m_2^{3/2}}\\]\n\n:b1: Sample skewness coefficient, related to \\(g_1\\).\n\n\\[b_1 = \\frac{m_3}{s^3}\\]\n\n:skew: Skewness used in BCa\n\n\\[SKEW = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^3}{(\\sum_{i=1}^n (x_i - \\bar{x})^2)^{3/2}}  = \\frac{g_1}{\\sqrt{n}}\\]\nRobust (less sensitive to outliers):\n\n:median: Median Skewness / Pearson’s first skewness coefficient.\n\n\\[S_P = 3 \\frac{\\text{mean} - \\text{median}}{\\text{stddev}}\\]\n\n:mode: Pearson’s second skewness coefficient. Mode estimation method can be specified.\n\n\\[S_K = \\frac{\\text{mean} - \\text{mode}}{\\text{stddev}}\\]\n\n:bowley or :yule (with \\(u=0.25\\)): Based on quartiles \\(Q_1, Q_2, Q_3\\).\n\n\\[S_B = \\frac{(Q_3 - Q_2) - (Q_2 - Q_1)}{Q_3 - Q_1} = \\frac{Q_3 + Q_1 - 2Q_2}{Q_3 - Q_1}\\]\n\n:yule or :B1 (Yule’s coefficient): Generalization of Bowley’s, using quantiles \\(Q_u, Q_{0.5}, Q_{1-u}\\).\n\n\\[B_1 = S_Y(u) = \\frac{(Q_{1-u} - Q_{0.5}) - (Q_{0.5} - Q_u)}{Q_{1-u} - Q_u}\\]\n\n:B3: Robust measure by Groeneveld and Meeden\n\n\\[B_3 = \\frac{\\text{mean} - \\text{median}}{\\text{mean}(|X_i - \\text{median}|)}\\]\n\n:hogg: Based on comparing trimmed means (\\(U_{0.05}\\): mean of top 5%, \\(L_{0.05}\\): mean of bottom 5%, \\(M_{0.25}\\): 25% trimmed mean).\n\n\\[S_H = \\frac{U_{0.05} - M_{0.25}}{M_{0.25} - L_{0.05}}\\]\n\n:l-skewness: L-moments based skewness.\n\n\\[\\tau_3 = \\lambda_3 / \\lambda_2\\]\nPositive skewness indicates a tail on the right side of the distribution; negative skewness indicates a tail on the left. Zero indicates symmetry.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/skewness residual-sugar) ;; =&gt; 1.0770937564240939\n(stats/skewness residual-sugar :G1) ;; =&gt; 1.0770937564240939\n(stats/skewness residual-sugar :g1) ;; =&gt; 1.0767638711454521\n(stats/skewness residual-sugar :pearson) ;; =&gt; 1.0767638711454521\n(stats/skewness residual-sugar :b1) ;; =&gt; 1.07643413178962\n(stats/skewness residual-sugar :skew) ;; =&gt; 0.01538548123095513\n(stats/skewness residual-sugar :median) ;; =&gt; 0.7046931919610692\n(stats/skewness residual-sugar :mode) ;; =&gt; 1.0235322790625094\n(stats/skewness residual-sugar [:mode :histogram]) ;; =&gt; 0.9248159088272246\n(stats/skewness residual-sugar [:mode :kde]) ;; =&gt; 0.9348108923665123\n(stats/skewness residual-sugar :bowley) ;; =&gt; 0.1463414634146341\n(stats/skewness residual-sugar :yule) ;; =&gt; 0.1463414634146341\n(stats/skewness residual-sugar [:yule 0.1]) ;; =&gt; 0.37499999999999994\n(stats/skewness residual-sugar :B3) ;; =&gt; 0.2864727410181956\n(stats/skewness residual-sugar :hogg) ;; =&gt; 3.0343529984131266\n(stats/skewness residual-sugar :l-skewness) ;; =&gt; 0.22296648073302056\n\n\n\nEffect of an outlier is visible for moment based skewness, while has no effect when robust method is used.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/skewness (conj residual-sugar -1000)) ;; =&gt; -58.659786835804155\n(stats/skewness (conj residual-sugar -1000) :l-skewness) ;; =&gt; 0.13914390517414776\n\n\n\n\n\nKurtosis\nKurtosis measures the “tailedness” or “peakedness” of a distribution. High kurtosis means heavy tails (more outliers) and a sharp peak (leptokurtic); low kurtosis means light tails and a flatter peak (platykurtic). fastmath.stats/kurtosis offers several types:\nMoment-based (sensitive to outliers):\n\n:G2 (Default): Sample kurtosis (Fisher’s definition, not excess), adjusted for sample bias (via Apache Commons Math). For a normal distribution, this is approximately 3.\n\n\\[G_2 = \\frac{(n+1)n}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n \\left(\\frac{x_i - \\bar{x}}{s}\\right)^4 - 3\\frac{(n-1)^2}{(n-2)(n-3)}\\]\n\n:g2 or :excess: Sample excess kurtosis. For a normal distribution, this is approximately 0.\n\n\\[g_2 = \\frac{m_4}{m_2^2}-3\\]\n\n:kurt: Kurtosis defined as \\(g_2 + 3\\).\n\n\\[g_{kurt} = \\frac{m_4}{m_2^2} = g_2 + 3\\]\n\n:b2: Sample kurtosis\n\n\\[b_2 = \\frac{m_4}{s^4}-3\\]\nRobust (less sensitive to outliers):\n\n:geary: Geary’s ‘g’ measure of kurtosis. Normal \\(\\approx \\sqrt{2/\\pi} \\approx 0.798\\).\n\n\\[g = \\frac{MeanAD}{\\sigma^2}\\]\n\n:moors: Based on octiles \\(E_i\\) (quantiles \\(i/8\\)) and centered by subtracting \\(1.233\\) (Moors’ constant for normality).\n\n\\[M_0 = \\frac{(E_7-E_5) + (E_3-E_1)}{E_6-E_2}-1.233\\]\n\n:crow (Crow-Siddiqui): Based on quantiles \\(Q_\\alpha, Q_{1-\\alpha}, Q_\\beta, Q_{1-\\beta}\\) and centered for normality (\\(c\\) is based on \\(\\alpha\\) and \\(\\beta\\)). By default \\(\\alpha=0.025\\) and \\(\\beta=0.25\\).\n\n\\[CS(\\alpha, \\beta) = \\frac{Q_{1-\\alpha} - Q_{\\alpha}}{Q_{1-\\beta} - Q_{\\beta}}-c\\]\n\n:hogg: Based on trimmed means \\(U_p\\) (mean of top \\(p\\%\\)) and \\(L_p\\) (mean of bottom \\(p\\%\\)) and centered by subtracting \\(2.585\\). By default \\(\\alpha=0.005\\) and \\(\\beta=0.5\\).\n\n\\[K_H(\\alpha, \\beta) = \\frac{U_{\\alpha} - L_{\\alpha}}{U_{\\beta} - L_{\\beta}}-2.585\\]\n\n:l-kurtosis: L-moments based kurtosis.\n\n\\[\\tau_4 = \\lambda_4 / \\lambda_2\\]\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/kurtosis residual-sugar) ;; =&gt; 3.4698201025636317\n(stats/kurtosis residual-sugar :G2) ;; =&gt; 3.4698201025636317\n(stats/kurtosis residual-sugar :g1) ;; =&gt; 3.4698201025636317\n(stats/kurtosis residual-sugar :excess) ;; =&gt; 3.4650542966048463\n(stats/kurtosis residual-sugar :kurt) ;; =&gt; 6.465054296604846\n(stats/kurtosis residual-sugar :b2) ;; =&gt; 3.4624146909177034\n(stats/kurtosis residual-sugar :geary) ;; =&gt; 0.8336299967214688\n(stats/kurtosis residual-sugar :moors) ;; =&gt; -0.35495121951219544\n(stats/kurtosis residual-sugar :crow) ;; =&gt; -0.8936518297189449\n(stats/kurtosis residual-sugar [:crow 0.05 0.25]) ;; =&gt; -0.6581758315571915\n(stats/kurtosis residual-sugar :hogg) ;; =&gt; -0.5329102382273203\n(stats/kurtosis residual-sugar [:hogg 0.025 0.45]) ;; =&gt; -0.5505736317691476\n(stats/kurtosis residual-sugar :l-kurtosis) ;; =&gt; 0.02007386147996773\n\n\n\nEffect of an outlier is visible for moment based kurtosis, while has no effect when robust method is used.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/kurtosis (conj residual-sugar -1000 1000)) ;; =&gt; 2167.8435026710868\n(stats/kurtosis (conj residual-sugar -1000 1000) :l-kurtosis) ;; =&gt; 0.1447670840462904\n\n\n\n\n\nL-moment\nL-moments are summary statistics analogous to conventional moments but are computed from linear combinations of order statistics (sorted data). They are more robust to outliers and provide better estimates for small samples compared to conventional moments.\n\nl-moment vs order: Calculates the L-moment of a specific order.\n\n\\(\\lambda_1\\): L-location (identical to the mean).\n\\(\\lambda_2\\): L-scale (a measure of dispersion).\nHigher orders relate to shape.\n\nTrimmed L-moments (TL-moments) can be calculated by specifying :s (left trim) and :t (right trim) as number of trimmed samples\nL-moment Ratios: If :ratio? true, normalized L-moments are returned.\n\n\\(\\tau_3 = \\lambda_3 / \\lambda_2\\): Coefficient of L-skewness (same as (stats/skewness vs :l-skewness)).\n\\(\\tau_4 = \\lambda_4 / \\lambda_2\\): Coefficient of L-kurtosis (same as (stats/kurtosis vs :l-kurtosis)).\n\n\nL-moments often provide more reliable inferences about the underlying distribution shape, especially when data may contain outliers or come from heavy-tailed distributions.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/l-moment mpg 1) ;; =&gt; 20.090624999999996\n(stats/mean mpg) ;; =&gt; 20.090625\n(stats/l-moment mpg 2) ;; =&gt; 3.3980846774193565\n(stats/l-moment mpg 3) ;; =&gt; 0.534375\n(stats/l-moment residual-sugar 3) ;; =&gt; 0.6185370660798765\n(stats/l-moment residual-sugar 3 {:s 10}) ;; =&gt; 0.3017960335908669\n(stats/l-moment residual-sugar 3 {:t 10}) ;; =&gt; 0.029252613288362147\n(stats/l-moment residual-sugar 3 {:s 10, :t 10}) ;; =&gt; 0.003909359312292547\n\n\n\nRelation to skewness and kurtosis\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/l-moment residual-sugar 3 {:ratio? true}) ;; =&gt; 0.22296648073302056\n(stats/skewness residual-sugar :l-skewness) ;; =&gt; 0.22296648073302056\n(stats/l-moment residual-sugar 4 {:ratio? true}) ;; =&gt; 0.02007386147996773\n(stats/kurtosis residual-sugar :l-kurtosis) ;; =&gt; 0.02007386147996773",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#intervals-and-extents",
    "href": "stats.html#intervals-and-extents",
    "title": "Statistics",
    "section": "Intervals and Extents",
    "text": "Intervals and Extents\nThis section provides functions to describe the spread or define specific ranges and intervals within a dataset.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nspan, iqr\nextent,\nstddev-extent, mad-extent, sem-extent\npercentile-extent, quantile-extent\npi, pi-extent\nhpdi-extent\nadjacent-values\ninner-fence-extent, outer-fence-extent\npercentile-bc-extent, percentile-bca-extent\nci\n\n\n\n\nBasic Range: Functions like span (\\(max - min\\)) and extent (providing \\([min, max]\\) and optionally the mean) offer simple measures of the total spread of the data.\nInterquartile Range: iqr (\\(Q_3 - Q_1\\)) specifically measures the spread of the middle 50% of the data, providing a robust alternative to the total range.\nSymmetric Spread Intervals: Functions ending in -extent such as stddev-extent, mad-extent, and sem-extent define intervals typically centered around the mean or median. They represent a range defined by adding/subtracting a multiple (usually 1) of a measure of dispersion (Standard Deviation, Median Absolute Deviation, or Standard Error of the Mean) from the central point.\nQuantile-Based Intervals: percentile-extent, quantile-extent, pi, pi-extent, and hpdi-extent define intervals based on quantiles or percentiles of the data. These functions capture specific ranges containing a certain percentage of the data points (e.g., the middle 95% defined by quantiles 0.025 and 0.975). hpdi-extent calculates the shortest interval containing a given proportion of data, based on empirical density.\nBox Plot Boundaries: adjacent-values (LAV, UAV) and fence functions (inner-fence-extent, outer-fence-extent) calculate specific bounds based on quartiles and multiples of the IQR. These are primarily used in box plot visualization and as a conventional method for identifying potential outliers.\nConfidence and Prediction Intervals: ci, percentile-bc-extent, and percentile-bca-extent provide inferential intervals. ci estimates a confidence interval for the population mean using the t-distribution. percentile-bc-extent and percentile-bca-extent (Bias-Corrected and Bias-Corrected Accelerated) are advanced bootstrap methods for estimating confidence intervals for statistics, offering robustness against non-normality and bias.\n\nNote that:\n\n\\(IQR = Q_3-Q_1\\)\n\\(LIF=Q_1-1.5 \\times IQR\\)\n\\(UIF=Q_3+1.5 \\times IQR\\)\n\\(LOF=Q_1-3\\times IQR\\)\n\\(UOF=Q_3+3\\times IQR\\)\n\\(CI=\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\n\n\n\n\n\nFunction\nReturned value\n\n\n\n\nspan\n\\(max-min\\)\n\n\niqr\n\\(Q_3-Q_1\\)\n\n\nextent\n[min, max, mean] or [min, max] (when :mean? is false)\n\n\nstddev-extent\n[mean - stddev, mean + stddev, mean]\n\n\nmad-extent\n[median - mad, median + mad, median]\n\n\nsem-extent\n[mean - sem, mean + sem, mean]\n\n\npercentile-extent\n[p1-val, p2-val, median] with default p1=25 and p2=75\n\n\nquantile-extent\n[q1-val, q2-val, median] with default q1=0.25 and q2=0.75\n\n\npi\n{p1 p1-val p2 p2-val} defined by size=p2-p1\n\n\npi-extent\n[p1-val, p2-val, median] defined by size=p2-p1\n\n\nhdpi-extent\n[p1-val, p2-val, median] defined by size=p2-p1\n\n\nadjacent-values\n[LAV, UAV, median]\n\n\ninner-fence-extent\n[LIF, UIF, median]\n\n\nouter-fence-extent\n[LOF, UOF, median]\n\n\nci\n[lower upper mean]\n\n\npercentile-bc-extent\n[lower upper mean]\n\n\npercentile-bca-extent\n[lower upper mean]\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/span mpg) ;; =&gt; 23.5\n(stats/iqr mpg) ;; =&gt; 7.525000000000002\n(stats/extent mpg) ;; =&gt; #vec3 [10.4, 33.9, 20.090625]\n(stats/extent mpg false) ;; =&gt; #vec2 [10.4, 33.9]\n(stats/stddev-extent mpg) ;; =&gt; [14.063676947910896 26.117573052089103 20.090625]\n(stats/mad-extent mpg) ;; =&gt; [15.549999999999999 22.85 19.2]\n(stats/sem-extent mpg) ;; =&gt; [19.025201040627184 21.156048959372814 20.090625]\n(stats/percentile-extent mpg) ;; =&gt; [15.274999999999999 22.8 19.2]\n(stats/percentile-extent mpg 2.5 97.5) ;; =&gt; [10.4 33.9 19.2]\n(stats/percentile-extent mpg 2.5 97.5 :r9) ;; =&gt; [10.4 33.628125 19.2]\n(stats/quantile-extent mpg) ;; =&gt; [15.274999999999999 22.8 19.2]\n(stats/quantile-extent mpg 0.025 0.975) ;; =&gt; [10.4 33.9 19.2]\n(stats/quantile-extent mpg 0.025 0.975 :r9) ;; =&gt; [10.4 33.628125 19.2]\n(stats/pi mpg 0.95) ;; =&gt; {2.5 10.4, 97.5 33.9}\n(stats/pi-extent mpg 0.95) ;; =&gt; [10.4 33.9 19.2]\n(stats/hpdi-extent mpg 0.95) ;; =&gt; [10.4 32.4 19.2]\n(stats/adjacent-values mpg) ;; =&gt; [10.4 33.9 19.2]\n(stats/inner-fence-extent mpg) ;; =&gt; [3.9874999999999954 34.087500000000006 19.2]\n(stats/outer-fence-extent mpg) ;; =&gt; [-7.300000000000008 45.37500000000001 19.2]\n(stats/ci mpg) ;; =&gt; [17.917678508746246 22.263571491253753 20.090625]\n(stats/ci mpg 0.1) ;; =&gt; [18.284178665508097 21.8970713344919 20.090625]\n(stats/percentile-bc-extent mpg) ;; =&gt; [10.4 33.9 20.090625]\n(stats/percentile-bc-extent mpg 10.0) ;; =&gt; [14.85121570396848 32.66635783408668 20.090625]\n(stats/percentile-bca-extent mpg) ;; =&gt; [10.4 33.9 20.090625]\n(stats/percentile-bca-extent mpg 10.0) ;; =&gt; [14.79162798537463 32.44980004741413 20.090625]",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#outlier-detection",
    "href": "stats.html#outlier-detection",
    "title": "Statistics",
    "section": "Outlier Detection",
    "text": "Outlier Detection\nOutlier detection involves identifying data points that are significantly different from other observations. Outliers can distort statistical analyses and require careful handling. fastmath.stats provides functions to find and optionally remove such values based on the Interquartile Range (IQR) method.\n\n\n\n\n\n\nDefined functions\n\n\n\n\noutliers\n\n\n\noutliers function use the inner fence rule based on the IQR and returns a sequence containing only the data points identified as outliers.\n\nLower Inner Fence (LIF): \\(Q_1 - 1.5 \\times IQR\\)\nUpper Inner Fence (UIF): \\(Q_3 + 1.5 \\times IQR\\)\n\nWhere \\(Q_1\\) is the first quartile (25th percentile) and \\(Q_3\\) is the third quartile (75th percentile). Points falling below the LIF or above the UIF are considered outliers.\nFunction accepts an optional estimation-strategy keyword (see [quantile]) to control how quartiles are calculated, which affects the fence boundaries.\nLet’s find the outliers in the residual-sugar data.\n\n(stats/outliers residual-sugar)\n\n\n(23.5 31.6 31.6 65.8 26.05 26.05 22.6)",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#data-transformation",
    "href": "stats.html#data-transformation",
    "title": "Statistics",
    "section": "Data Transformation",
    "text": "Data Transformation\nFunctions to modify data (scaling, normalizing, transforming).\n\n\n\n\n\n\nDefined functions\n\n\n\n\nstandardize, robust-standardize, demean\nrescale\nremove-outliers\ntrim, trim-lower, trim-upper, winsor\nbox-cox-infer-lambda, box-cox-transformation\nyeo-johnson-infer-lambda, yeo-johnson-transformation\n\n\n\nData transformations are often necessary preprocessing steps in statistical analysis and machine learning. They can help meet the assumptions of certain models (e.g., normality, constant variance), improve interpretability, or reduce the influence of outliers. fastmath.stats offers several functions for these purposes, broadly categorized into linear scaling/centering, outlier handling, and power transformations for normality.\nLet’s demonstrate some of these transformations using the residual-sugar data from the wine quality dataset.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mean residual-sugar) ;; =&gt; 6.391414863209474\n(stats/stddev residual-sugar) ;; =&gt; 5.072057784014863\n(stats/median residual-sugar) ;; =&gt; 5.2\n(stats/mad residual-sugar) ;; =&gt; 3.6\n(stats/extent residual-sugar false) ;; =&gt; #vec2 [0.6, 65.8]\n(count residual-sugar) ;; =&gt; 4898\n\n\n\n\n\n\nLinear Transformations: standardize, robust-standardize, demean, and rescale linearly transform data, preserving its shape but changing its location and/or scale.\n\ndemean centers the data by subtracting the mean, resulting in a dataset with a mean of zero.\nstandardize scales the demeaned data by dividing by the standard deviation, resulting in data with mean zero and standard deviation one (z-score normalization). This makes the scale of different features comparable.\nrobust-standardize provides a version less sensitive to outliers by centering around the median and scaling by the Median Absolute Deviation (MAD) or a quantile range (like the IQR).\nrescale linearly maps the data to a specific target range (e.g., [0, 1]), useful for algorithms sensitive to input scale.\n\n\n(def residual-sugar-demeaned (-&gt; residual-sugar stats/demean))\n\n\n(def residual-sugar-standardized (-&gt; residual-sugar stats/standardize))\n\n\n(def residual-sugar-robust-standardized (-&gt; residual-sugar stats/robust-standardize))\n\n\n(def residual-sugar-rescaled (-&gt; residual-sugar stats/rescale))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mean residual-sugar-demeaned) ;; =&gt; -3.2386416038881656E-16\n(stats/stddev residual-sugar-demeaned) ;; =&gt; 5.072057784014863\n(stats/median residual-sugar-demeaned) ;; =&gt; -1.1914148632094737\n(stats/mad residual-sugar-demeaned) ;; =&gt; 3.5999999999999996\n(stats/extent residual-sugar-demeaned false) ;; =&gt; #vec2 [-5.791414863209474, 59.40858513679052]\n(stats/mean residual-sugar-standardized) ;; =&gt; -1.5386267642212256E-16\n(stats/stddev residual-sugar-standardized) ;; =&gt; 1.0000000000000004\n(stats/median residual-sugar-standardized) ;; =&gt; -0.23489773065368974\n(stats/mad residual-sugar-standardized) ;; =&gt; 0.7097710935679375\n(stats/extent residual-sugar-standardized false) ;; =&gt; #vec2 [-1.1418274613238324, 11.712915677739927]\n(stats/mean residual-sugar-robust-standardized) ;; =&gt; 0.3309485731137426\n(stats/stddev residual-sugar-robust-standardized) ;; =&gt; 1.4089049400041327\n(stats/median residual-sugar-robust-standardized) ;; =&gt; 0.0\n(stats/mad residual-sugar-robust-standardized) ;; =&gt; 1.0\n(stats/extent residual-sugar-robust-standardized false) ;; =&gt; #vec2 [-1.277777777777778, 16.833333333333332]\n(stats/mean residual-sugar-rescaled) ;; =&gt; 0.0888253813375686\n(stats/stddev residual-sugar-rescaled) ;; =&gt; 0.07779229730084207\n(stats/median residual-sugar-rescaled) ;; =&gt; 0.07055214723926381\n(stats/mad residual-sugar-rescaled) ;; =&gt; 0.055214723926380375\n(stats/extent residual-sugar-rescaled false) ;; =&gt; #vec2 [0.0, 1.0]\n\n\n\nOutlier Handling: remove-outliers, trim, trim-lower, trim-upper, and winsor address outliers based on quantile fences.\n\nremove-outliers returns a sequence containing the data points from the original sequence excluding those identified as outliers.\ntrim removes values outside a specified quantile range (defaulting to 0.2 quantile, removing the bottom and top 20%). trim-lower and trim-upper remove only below or above a single quantile.\nwinsor caps values outside a quantile range to the boundary values instead of removing them. This retains the sample size but reduces the influence of extreme values.\n\n\n(def residual-sugar-no-outliers (stats/remove-outliers residual-sugar))\n\n\n(def residual-sugar-trimmed (stats/trim residual-sugar))\n\n\n(def residual-sugar-winsorized (stats/winsor residual-sugar))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mean residual-sugar-no-outliers) ;; =&gt; 6.354109589041096\n(stats/stddev residual-sugar-no-outliers) ;; =&gt; 4.950545246813552\n(stats/median residual-sugar-no-outliers) ;; =&gt; 5.2\n(stats/mad residual-sugar-no-outliers) ;; =&gt; 3.6\n(stats/extent residual-sugar-no-outliers false) ;; =&gt; #vec2 [0.6, 22.0]\n(count residual-sugar-no-outliers) ;; =&gt; 4891\n(stats/mean residual-sugar-trimmed) ;; =&gt; 5.27940414507772\n(stats/stddev residual-sugar-trimmed) ;; =&gt; 2.9594599192772546\n(stats/median residual-sugar-trimmed) ;; =&gt; 5.0\n(stats/mad residual-sugar-trimmed) ;; =&gt; 2.7\n(stats/extent residual-sugar-trimmed false) ;; =&gt; #vec2 [1.5, 11.2]\n(count residual-sugar-trimmed) ;; =&gt; 3088\n(stats/mean residual-sugar-winsorized) ;; =&gt; 5.789893834218048\n(stats/stddev residual-sugar-winsorized) ;; =&gt; 3.8241868005472104\n(stats/median residual-sugar-winsorized) ;; =&gt; 5.2\n(stats/mad residual-sugar-winsorized) ;; =&gt; 3.6\n(stats/quantiles residual-sugar [0.2 0.8]) ;; =&gt; [1.5 11.2]\n(stats/extent residual-sugar-winsorized false) ;; =&gt; #vec2 [1.5, 11.2]\n(count residual-sugar-winsorized) ;; =&gt; 4898\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower Transformations: box-cox-transformation and yeo-johnson-transformation (and their infer-lambda counterparts) are non-linear transformations that can change the shape of the distribution to be more symmetric or normally distributed. They are particularly useful for data that is skewed or violates assumptions of linear models. Both are invertable.\n\nbox-cox-transformation works in general for strictly positive data. It includes the log transformation as a special case (when lambda is \\(0.0\\)) and generalizes square root, reciprocal, and other power transformations. box-cox-infer-lambda helps find the optimal lambda parameter. Optional parameters:\n\n:negative? (default: false), when set to true specific transformation is performed to keep information about sign.\n:scaled? (default: false), when set to true, scale data by geometric mean, when is a number, this number is used as a scale.\n\n\n\\[y_{BC}^{(\\lambda)}=\\begin{cases}\n\\frac{y^\\lambda-1}{\\lambda} & \\lambda\\neq 0 \\\\\n\\log(y) & \\lambda = 0\n\\end{cases}\\]\nScaled version, with default scale set to geometric mean (GM):\n\\[y_{BC}^{(\\lambda, s)}=\\begin{cases}\n\\frac{y^\\lambda-1}{\\lambda s^{\\lambda - 1}} & \\lambda\\neq 0 \\\\\ns\\log(y) & \\lambda = 0\n\\end{cases}\\]\nWhen :negative? is set to true, formula takes the following form:\n\\[y_{BCneg}^{(\\lambda)}=\\begin{cases}\n\\frac{\\operatorname{sgn}(y)|y|^\\lambda-1}{\\lambda} & \\lambda\\neq 0 \\\\\n\\operatorname{sgn}(y)\\log(|y|+1) & \\lambda = 0\n\\end{cases}\\]\n\nyeo-johnson-transformation extends Box-Cox to handle zero and negative values. yeo-johnson-infer-lambda finds the optimal lambda for this transformation.\n\n\\[y_{YJ}^{(\\lambda)}=\\begin{cases}\n\\frac{(y+1)^\\lambda - 1}{\\lambda} & \\lambda \\neq 0, y\\geq 0 \\\\\n\\log(y+1) & \\lambda = 0, y\\geq 0 \\\\\n\\frac{(1-y)^{2-\\lambda} - 1}{\\lambda - 2} & \\lambda \\neq 2, y\\geq 0 \\\\\n-\\log(1-y) & \\lambda = 2, y\\geq 0  \n\\end{cases}\\]\nBoth fuctions accept additional parameters:\n\n:alpha (dafault: 0.0): perform dataset shift by value of the :alpha before transformation.\n:inversed? (default: false): perform inverse transformation for given lambda.\n\nWhen lambda is set to nil optimal lambda will be calculated (only when :inversed? is false).\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/box-cox-transformation [0 1 10] 0.0) ;; =&gt; (##-Inf 0.0 2.302585092994046)\n(stats/box-cox-transformation [0 1 10] 2.0) ;; =&gt; (-0.5 0.0 49.5)\n(stats/box-cox-transformation [0 1 10] -2.0 {:alpha 2}) ;; =&gt; (0.375 0.4444444444444444 0.4965277777777778)\n(stats/box-cox-transformation [0.375 0.444 0.497] -2.0 {:alpha 2, :inverse? true}) ;; =&gt; (0.0 0.9880715233359845 10.90994448735805)\n(stats/box-cox-transformation [0 1 10] nil {:alpha 1}) ;; =&gt; (-0.0 0.5989997131047903 1.493287747539177)\n(stats/box-cox-transformation [0 1 10] nil {:scaled? true, :alpha 1}) ;; =&gt; (-0.0 2.619403814271841 6.530092646346955)\n(stats/box-cox-transformation [0 1 10] nil {:alpha -5, :negative? true}) ;; =&gt; (-42.0 -21.666666666666668 41.333333333333336)\n(stats/box-cox-transformation [0 1 10] 2.0 {:alpha -5, :negative? true, :scaled? 2}) ;; =&gt; (-6.5 -4.25 6.0)\n(stats/box-cox-transformation [-6.5 -4.25 6.0] 2.0 {:alpha -5, :negative? true, :scaled? 2, :inverse? true}) ;; =&gt; (0.0 1.0 10.0)\n(stats/yeo-johnson-transformation [0 1 10]) ;; =&gt; (-0.0 0.5989997131047903 1.493287747539177)\n(stats/yeo-johnson-transformation [0 1 10] 0.0) ;; =&gt; (0.0 0.6931471805599453 2.3978952727983707)\n(stats/yeo-johnson-transformation [0 1 10] 2.0 {:alpha -5}) ;; =&gt; (-1.791759469228055 -1.6094379124341003 17.5)\n(stats/yeo-johnson-transformation [-1.79 -1.61 17.5] 2.0 {:alpha -5, :inverse? true}) ;; =&gt; (0.010547533616885651 0.9971887721664103 10.0)\n\n\n\nLet’s illustrate how real data look after transformation. We’ll start with finding an optimal lambda parameter for both transformations.\n\n(stats/box-cox-infer-lambda residual-sugar)\n\n\n0.12450565747077313\n\n\n(stats/yeo-johnson-infer-lambda residual-sugar)\n\n\n-0.004232775028107413\n\n\n(def residual-sugar-box-cox (stats/box-cox-transformation residual-sugar nil))\n\n\n(def residual-sugar-yeo-johnson (stats/yeo-johnson-transformation residual-sugar nil))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mean residual-sugar-box-cox) ;; =&gt; 1.6895596029306466\n(stats/stddev residual-sugar-box-cox) ;; =&gt; 1.1032589652895741\n(stats/median residual-sugar-box-cox) ;; =&gt; 1.83006349249072\n(stats/mad residual-sugar-box-cox) ;; =&gt; 1.0527777588806666\n(stats/extent residual-sugar-box-cox false) ;; =&gt; #vec2 [-0.4949201739139414, 5.494888687185149]\n(stats/mean residual-sugar-yeo-johnson) ;; =&gt; 1.7445922710465025\n(stats/stddev residual-sugar-yeo-johnson) ;; =&gt; 0.7180340465417893\n(stats/median residual-sugar-yeo-johnson) ;; =&gt; 1.8175219821484885\n(stats/mad residual-sugar-yeo-johnson) ;; =&gt; 0.6888246769156978\n(stats/extent residual-sugar-yeo-johnson false) ;; =&gt; #vec2 [0.46953642189900135, 4.164560241279393]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the Yeo-Johnson transformation with the inferred lambda has made the residual-sugar distribution appear more symmetric and perhaps closer to a normal distribution shape.\nBoth power transformation can work on negative data as well. When Box-Cox is used, :negative? option should be set to true.\n\n(stats/box-cox-infer-lambda residual-sugar\n                            nil {:alpha (- (stats/mean residual-sugar)) :negative? true})\n\n\n1.0967709597378346\n\n\n(stats/yeo-johnson-infer-lambda residual-sugar nil {:alpha (- (stats/mean residual-sugar))})\n\n\n0.7290829398083033\n\n\n(def residual-sugar-box-cox-demeaned\n  (stats/box-cox-transformation\n   residual-sugar nil {:alpha (- (stats/mean residual-sugar)) :negative? true}))\n\n\n(def residual-sugar-yeo-johnson-demeaned (stats/yeo-johnson-transformation\n                                        residual-sugar nil\n                                        {:alpha (- (stats/mean residual-sugar))}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mean residual-sugar-box-cox-demeaned) ;; =&gt; -0.817408751945805\n(stats/stddev residual-sugar-box-cox-demeaned) ;; =&gt; 5.608979218510315\n(stats/median residual-sugar-box-cox-demeaned) ;; =&gt; -2.0166286943919243\n(stats/mad residual-sugar-box-cox-demeaned) ;; =&gt; 3.9790383049427582\n(stats/extent residual-sugar-box-cox-demeaned false) ;; =&gt; #vec2 [-7.1704674516316125, 79.51310067615422]\n(stats/mean residual-sugar-yeo-johnson-demeaned) ;; =&gt; -1.3556628530573829\n(stats/stddev residual-sugar-yeo-johnson-demeaned) ;; =&gt; 4.783153826599674\n(stats/median residual-sugar-yeo-johnson-demeaned) ;; =&gt; -1.3457965294079925\n(stats/mad residual-sugar-yeo-johnson-demeaned) ;; =&gt; 4.669926884717297\n(stats/extent residual-sugar-yeo-johnson-demeaned false) ;; =&gt; #vec2 [-8.192317681811963, 25.905106824581896]",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#correlation-and-covariance",
    "href": "stats.html#correlation-and-covariance",
    "title": "Statistics",
    "section": "Correlation and Covariance",
    "text": "Correlation and Covariance\nMeasures of the relationship between two or more variables.\n\n\n\n\n\n\nDefined functions\n\n\n\n\ncovariance, correlation\npearson-correlation, spearman-correlation, kendall-correlation\ncoefficient-matrix, correlation-matrix, covariance-matrix\n\n\n\nCovariance vs. Correlation:\n\ncovariance measures the extent to which two variables change together. A positive covariance means they tend to increase or decrease simultaneously. A negative covariance means one tends to increase when the other decreases. A covariance near zero suggests no linear relationship. The magnitude of covariance depends on the scales of the variables, making it difficult to compare covariances between different pairs of variables. The sample covariance between two sequences \\(X = \\{x_1, \\dots, x_n\\}\\) and \\(Y = \\{y_1, \\dots, y_n\\}\\) is calculated as:\n\n\\[ \\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\]\nwhere \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means.\n\ncorrelation standardizes the covariance, resulting in a unitless measure that ranges from -1 to +1. It indicates both the direction and strength of a relationship. A correlation of +1 indicates a perfect positive relationship, -1 a perfect negative relationship, and 0 no linear relationship. The correlation function in fastmath.stats defaults to computing the Pearson correlation coefficient.\n\nTypes of Correlation:\n\npearson-correlation: The most common correlation coefficient, also known as the Pearson product-moment correlation coefficient (\\(r\\)). It measures the strength and direction of a linear relationship between two continuous variables. It assumes the variables are approximately normally distributed and that the relationship is linear. It is sensitive to outliers. The formula for the sample Pearson correlation coefficient is:\n\n\\[r = \\frac{\\text{Cov}(X, Y)}{s_x s_y} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\\]\nwhere \\(s_x\\) and \\(s_y\\) are the sample standard deviations.\n\nspearman-correlation: Spearman’s rank correlation coefficient (\\(\\rho\\)) is a non-parametric measure of the strength and direction of a monotonic relationship between two variables. A monotonic relationship is one that is either consistently increasing or consistently decreasing, but not necessarily linear. Spearman’s correlation is calculated by applying the Pearson formula to the ranks of the data values rather than the values themselves. This makes it less sensitive to outliers than Pearson correlation and suitable for ordinal data or when the relationship is monotonic but non-linear.\nkendall-correlation: Kendall’s Tau rank correlation coefficient (\\(\\tau\\)) is another non-parametric measure of the strength and direction of a monotonic relationship. It is based on the number of concordant and discordant pairs of observations. A pair of data points is concordant if their values move in the same direction (both increase or both decrease) relative to each other, and discordant if they move in opposite directions. Kendall’s Tau is generally preferred over Spearman’s Rho for smaller sample sizes or when there are many tied ranks. One common formulation, Kendall’s Tau-a, is:\n\n\\[\\tau_A = \\frac{N_c - N_d}{n(n-1)/2}\\]\nwhere \\(N_c\\) is the number of concordant pairs and \\(N_d\\) is the number of discordant pairs.\nComparison of Correlation Methods:\n\nUse Pearson for measuring linear relationships between continuous, normally distributed variables.\nUse Spearman or Kendall for measuring monotonic relationships (linear or non-linear) between variables, especially when data is not normally distributed, contains outliers, or is ordinal. Kendall is often more robust with ties and smaller samples.\n\nMatrix Functions for Multiple Variables:\n\ncoefficient-matrix: A generic function that computes a specified pairwise measure (defined by a function passed as an argument) between all pairs of sequences in a collection. Useful for generating matrices of custom similarity, distance, or correlation measures.\ncovariance-matrix: A specialization that computes the pairwise covariance for all sequences in a collection. The output is a symmetric matrix where the element at row i, column j is the covariance between sequence i and sequence j. The diagonal elements are the variances of the individual sequences.\ncorrelation-matrix: A specialization that computes the pairwise correlation (Pearson by default, or specified via keyword like :spearman or :kendall) for all sequences in a collection. The output is a symmetric matrix where the element at row i, column j is the correlation between sequence i and sequence j. The diagonal elements are always 1.0 (a variable is perfectly correlated with itself).\n\nLet’s examine the correlations between the numerical features in the iris dataset.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/covariance virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.03007346938775512\n(stats/correlation virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.13417210385493564\n(stats/pearson-correlation virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.1341721038549354\n(stats/spearman-correlation virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.038837958926489176\n(stats/kendall-correlation virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.030531668042830747\n\n\n\nTo generate matrices we’ll use three sepal lengths samples. The last two examples use custom measure function: Euclidean distance between samples and Glass’ delta.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/covariance-matrix (vals sepal-lengths)) ;; =&gt; ([0.12424897959183674 -0.014710204081632658 0.03007346938775512] [-0.014710204081632658 0.2664326530612246 -0.04649795918367346] [0.03007346938775512 -0.04649795918367346 0.4043428571428573])\n(stats/correlation-matrix (vals sepal-lengths)) ;; =&gt; ([1.0 -0.08084972701756978 0.13417210385493492] [-0.08084972701756978 0.9999999999999999 -0.14166588513698952] [0.13417210385493492 -0.14166588513698952 1.0])\n(stats/correlation-matrix (vals sepal-lengths) :kendall) ;; =&gt; ([1.0 -0.06357129445203882 0.030531668042830747] [-0.06357129445203882 1.0 -0.10454171307909799] [0.030531668042830747 -0.10454171307909799 1.0])\n(stats/correlation-matrix (vals sepal-lengths) :spearman) ;; =&gt; ([1.0 -0.10163684956357029 0.03883795892648921] [-0.10163684956357029 1.0 -0.14067854670792204] [0.03883795892648921 -0.14067854670792204 1.0])\n(stats/coefficient-matrix (vals sepal-lengths) stats/L2 true) ;; =&gt; ([0.0 7.989367934949548 12.16922347563722] [7.989367934949548 0.0 7.660287200882224] [12.16922347563722 7.660287200882224 0.0])\n(stats/coefficient-matrix (vals sepal-lengths) stats/glass-delta) ;; =&gt; ([0.0 -1.8017279836157427 -2.4878923883271122] [2.6383750609896146 0.0 -1.0253513509413892] [4.488074566113517 1.263146930448887 0.0])",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#distance-and-similarity-metrics",
    "href": "stats.html#distance-and-similarity-metrics",
    "title": "Statistics",
    "section": "Distance and Similarity Metrics",
    "text": "Distance and Similarity Metrics\nMeasures of distance, error, or similarity between sequences or distributions.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nme, mae, mape\nrss, mse, rmse\nr2\ncount=, L0, L1, L2sq, L2, LInf\npsnr\ndissimilarity, similarity\n\n\n\nDistance metrics quantify how far apart or different two data sequences or probability distributions are. Similarity metrics, conversely, measure how close or alike they are, often being the inverse or a transformation of a distance. fastmath.stats provides a range of these measures suitable for comparing numerical sequences, observed counts (histograms), or theoretical probability distributions.\n\nError Metrics\nThese functions typically quantify the difference between an observed sequence and a predicted or reference sequence, focusing on the magnitude of errors. All can accept a constant as a second argument.\n\nme (Mean Error): The average of the differences between corresponding elements. \\[ ME = \\frac{1}{n} \\sum_{i=1}^n (x_i - y_i) \\]\nmae (Mean Absolute Error): The average of the absolute differences. More robust to outliers than squared error. \\[ MAE = \\frac{1}{n} \\sum_{i=1}^n |x_i - y_i| \\]\nmape (Mean Absolute Percentage Error): The average of the absolute percentage errors. Useful for relative error assessment, but undefined if the reference value \\(x_i\\) is zero. \\[ MAPE = \\frac{1}{n} \\sum_{i=1}^n \\left| \\frac{x_i - y_i}{x_i} \\right| \\times 100\\% \\]\nrss (Residual Sum of Squares): The sum of the squared differences. Used in least squares regression. \\[ RSS = \\sum_{i=1}^n (x_i - y_i)^2 \\]\nmse (Mean Squared Error): The average of the squared differences. Penalizes larger errors more heavily. \\[ MSE = \\frac{1}{n} \\sum_{i=1}^n (x_i - y_i)^2 \\]\nrmse (Root Mean Squared Error): The square root of the MSE. Has the same units as the original data. \\[ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - y_i)^2} \\]\nr2 (Coefficient of Determination): Measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Calculated as \\(1 - (RSS / TSS)\\), where TSS is the Total Sum of Squares. It ranges from 0 to 1 for linear regression. \\[ R^2 = 1 - \\frac{\\sum (x_i - y_i)^2}{\\sum (x_i - \\bar{x})^2} \\]\nadjusted r2: A modified version of \\(R^2\\) that has been adjusted for the number of predictors in the model. It increases only if the new term improves the model more than would be expected by chance. \\[ R^2_{adj} = 1 - (1 - R^2) \\frac{n-1}{n-p-1} \\]\n\nLet’s use setosa-sepal-length as observed and virginica-sepal-length as predicted (though they are independent samples, not predictions) to illustrate error measures.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/me setosa-sepal-length virginica-sepal-length) ;; =&gt; -1.582\n(stats/mae setosa-sepal-length virginica-sepal-length) ;; =&gt; 1.582\n(stats/mape setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.3211321155719454\n(stats/rss setosa-sepal-length virginica-sepal-length) ;; =&gt; 148.09000000000006\n(stats/mse setosa-sepal-length virginica-sepal-length) ;; =&gt; 2.9618\n(stats/rmse setosa-sepal-length virginica-sepal-length) ;; =&gt; 1.720988088279521\n(stats/r2 setosa-sepal-length virginica-sepal-length) ;; =&gt; -23.324102361946068\n(stats/r2 setosa-sepal-length virginica-sepal-length 2) ;; =&gt; -24.359170547560794\n(stats/r2 setosa-sepal-length virginica-sepal-length 5) ;; =&gt; -26.0882049030763\n\n\n\nAlso we can compare an observed sequence to a constant value. For example to a mean of the virginica sepal length.\n\n(def vsl-mean (stats/mean virginica-sepal-length))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/me setosa-sepal-length vsl-mean) ;; =&gt; -1.582\n(stats/mae setosa-sepal-length vsl-mean) ;; =&gt; 1.582\n(stats/mape setosa-sepal-length vsl-mean) ;; =&gt; 0.32244506843926823\n(stats/rss setosa-sepal-length vsl-mean) ;; =&gt; 131.2244000000001\n(stats/mse setosa-sepal-length vsl-mean) ;; =&gt; 2.6244880000000004\n(stats/rmse setosa-sepal-length vsl-mean) ;; =&gt; 1.6200271602661482\n(stats/r2 setosa-sepal-length vsl-mean) ;; =&gt; -20.55389113366842\n(stats/r2 setosa-sepal-length vsl-mean 2) ;; =&gt; -21.471077990420266\n(stats/r2 setosa-sepal-length vsl-mean 5) ;; =&gt; -23.003196944312556\n\n\n\n\n\nDistance Metrics (L-p Norms and others)\nThese functions represent common distance measures, often related to L-p norms between vectors (sequences).\n\ncount=, L0: Counts the number of elements that are equal in both sequences. While related to the L0 “norm” (which counts non-zero elements), this implementation counts equal elements after subtraction. \\[ Count= = \\sum_{i=1}^n \\mathbb{I}(x_i = y_i) \\]\nL1 (Manhattan/City Block Distance): The sum of the absolute differences. \\[ L_1 = \\sum_{i=1}^n |x_i - y_i| \\]\nL2sq (Squared Euclidean Distance): The sum of the squared differences. Equivalent to rss. \\[ L_2^2 = \\sum_{i=1}^n (x_i - y_i)^2 \\]\nL2 (Euclidean Distance): The square root of the sum of the squared differences. The most common distance metric. \\[ L_2 = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} \\]\nLInf (Chebyshev Distance): The maximum absolute difference between corresponding elements. \\[ L_\\infty = \\max_{i} |x_i - y_i| \\]\npsnr (Peak Signal-to-Noise Ratio): A measure of signal quality often used in image processing, derived from the MSE. Higher PSNR indicates better quality (less distortion). Calculated based on the maximum possible value of the data and the MSE. \\[ PSNR = 10 \\cdot \\log_{10} \\left( \\frac{MAX^2}{MSE} \\right) \\]\n\nUsing the sepal length samples again:\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/count= setosa-sepal-length virginica-sepal-length) ;; =&gt; 1\n(stats/L0 setosa-sepal-length virginica-sepal-length) ;; =&gt; 1\n(stats/L1 setosa-sepal-length virginica-sepal-length) ;; =&gt; 79.10000000000005\n(stats/L2sq setosa-sepal-length virginica-sepal-length) ;; =&gt; 148.09000000000006\n(stats/L2 setosa-sepal-length virginica-sepal-length) ;; =&gt; 12.16922347563722\n(stats/LInf setosa-sepal-length virginica-sepal-length) ;; =&gt; 3.1000000000000005\n(stats/psnr setosa-sepal-length virginica-sepal-length) ;; =&gt; 13.236984537937193\n\n\n\n\n\nDissimilarity and Similarity\ndissimilarity and similarity functions provide measures for comparing probability distributions or frequency counts (like histograms). They quantify how ‘far apart’ or ‘alike’ two data sequences, interpreted as distributions, are. They take a method keyword specifying the desired measure. Many methods exist, each with different properties and interpretations. They can accept raw data sequences, automatically creating histograms for comparison (controlled by :bins), or they can take pre-calculated frequency sequences or a data sequence and a fastmath.random distribution object.\nParameters:\n\nmethod - The specific distance or similarity method to use.\nP-observed - Frequencies, probabilities, or raw data (when Q-expected is a distribution or :bins is set).\nQ-expected - Frequencies, probabilities, or a distribution object (when P-observed is raw data or :bins is set).\nopts (map, optional) - Configuration options, including:\n\n:probabilities? (boolean, default: true): If true, input sequences are normalized to sum to 1.0 before calculating the measure, treating them as probability distributions.\n:epsilon (double, default: 1.0e-6): A small number used to replace zero values in denominators or logarithms to avoid division-by-zero or log-of-zero errors.\n:log-base (double, default: m/E): The base for logarithms in information-theoretic measures.\n:power (double, default: 2.0): The exponent for the :minkowski distance.\n:remove-zeros? (boolean, default: false): Removes pairs where both P and Q are zero before calculation.\n:bins (number, keyword, or seq): Used for comparisons involving raw data or distributions. Specifies the number of histogram bins, an estimation method (see [histogram]), or explicit bin edges for histogram creation.\n\n\n\nDissimilarity Methods\nHigher values generally indicate greater difference.\n\nL-p Norms and Related:\n\n:euclidean: Euclidean distance (\\(L_2\\) norm) between the frequency/probability vectors. \\[ D(P, Q) = \\sqrt{\\sum_i (P_i - Q_i)^2} \\]\n:city-block / :manhattan: Manhattan distance (\\(L_1\\) norm). \\[ D(P, Q) = \\sum_i |P_i - Q_i| \\]\n:chebyshev: Chebyshev distance (\\(L_\\infty\\) norm), the maximum absolute difference. \\[ D(P, Q) = \\max_i |P_i - Q_i| \\]\n:minkowski: Minkowski distance (generalized \\(L_p\\) norm, controlled by :power). \\[ D(P, Q) = \\left(\\sum_i |P_i - Q_i|^p\\right)^{1/p} \\]\n:euclidean-sq / :squared-euclidean: Squared Euclidean distance. \\[ D(P, Q) = \\sum_i (P_i - Q_i)^2 \\]\n:squared-chord: Squared chord distance, related to Hellinger distance. \\[ D(P, Q) = \\sum_i (\\sqrt{P_i} - \\sqrt{Q_i})^2 \\]\n\nSet-based/Overlap: Measures derived from the concept of set overlap applied to frequencies/probabilities.\n\n:sorensen: Sorensen-Dice dissimilarity (1 - Dice similarity). \\[ D(P, Q) = \\frac{\\sum_i |P_i - Q_i|}{\\sum_i (P_i + Q_i)} \\]\n:gower: Gower distance (average Manhattan distance). \\[ D(P, Q) = \\frac{1}{N} \\sum_i |P_i - Q_i| \\]\n:soergel: Soergel distance (1 - Jaccard similarity). \\[ D(P, Q) = \\frac{\\sum_i |P_i - Q_i|}{\\sum_i \\max(P_i, Q_i)} \\]\n:kulczynski: Kulczynski dissimilarity (1 - Kulczynski similarity, can be &gt; 1). \\[ D(P, Q) = \\frac{\\sum_i |P_i - Q_i|}{\\sum_i \\min(P_i, Q_i)} \\]\n:canberra: Canberra distance, sensitive to small values. \\[ D(P, Q) = \\sum_i \\frac{|P_i - Q_i|}{P_i + Q_i} \\]\n:lorentzian: Lorentzian distance. \\[ D(P, Q) = \\sum_i \\ln(1 + |P_i - Q_i|) \\]\n:non-intersection: Non-intersection measure. \\[ D(P, Q) = \\frac{1}{2} \\sum_i |P_i - Q_i| \\]\n:wave-hedges: Wave Hedges distance. \\[ D(P, Q) = \\sum_i \\frac{|P_i - Q_i|}{\\max(P_i, Q_i)} \\]\n:czekanowski: Czekanowski dissimilarity (same as Sorensen).\n:motyka: Motyka dissimilarity. \\[ D(P, Q) = 1 - \\frac{\\sum_i \\min(P_i, Q_i)}{\\sum_i (P_i + Q_i)} \\]\n:tanimoto: Tanimoto dissimilarity (extended Jaccard or Dice). \\[ D(P, Q) = \\frac{\\sum_i (\\max(P_i, Q_i) - \\min(P_i, Q_i))}{\\sum_i \\max(P_i, Q_i)} \\]\n:jaccard: Jaccard dissimilarity (1 - Jaccard similarity). \\[ D(P, Q) = \\frac{\\sum_i (P_i - Q_i)^2}{\\sum_i P_i^2 + \\sum_i Q_i^2 - \\sum_i P_i Q_i} \\]\n:dice: Dice dissimilarity (1 - Dice similarity). \\[ D(P, Q) = \\frac{\\sum_i (P_i - Q_i)^2}{\\sum_i P_i^2 + \\sum_i Q_i^2} \\]\n:bhattacharyya: Bhattacharyya distance. \\[ D(P, Q) = -\\ln \\left( \\sum_i \\sqrt{P_i Q_i} \\right) \\]\n:hellinger: Hellinger distance, derived from Bhattacharyya coefficient. \\[ D(P, Q) = \\sqrt{2 \\sum_i (\\sqrt{P_i} - \\sqrt{Q_i})^2} \\]\n:matusita: Matusita distance. \\[ D(P, Q) = \\sqrt{\\sum_i (\\sqrt{P_i} - \\sqrt{Q_i})^2} \\]\n\nChi-squared based:\n\n:pearson-chisq / :chisq: Pearson’s Chi-squared statistic. \\[ D(P, Q) = \\sum_i \\frac{(P_i - Q_i)^2}{Q_i} \\]\n:neyman-chisq: Neyman’s Chi-squared statistic. \\[ D(P, Q) = \\sum_i \\frac{(P_i - Q_i)^2}{P_i} \\]\n:squared-chisq: Squared Chi-squared distance. \\[ D(P, Q) = \\sum_i \\frac{(P_i - Q_i)^2}{P_i + Q_i} \\]\n:symmetric-chisq: Symmetric Chi-squared distance. \\[ D(P, Q) = 2 \\sum_i \\frac{(P_i - Q_i)^2}{P_i + Q_i} \\]\n:divergence: Divergence statistic. \\[ D(P, Q) = 2 \\sum_i \\frac{(P_i - Q_i)^2}{(P_i + Q_i)^2} \\]\n:clark: Clark distance. \\[ D(P, Q) = \\sqrt{\\sum_i \\left(\\frac{P_i - Q_i}{P_i + Q_i}\\right)^2} \\]\n:additive-symmetric-chisq: Additive Symmetric Chi-squared distance. \\[ D(P, Q) = \\sum_i \\frac{(P_i - Q_i)^2 (P_i + Q_i)}{P_i Q_i} \\]\n\nInformation Theory based (Divergences): Measure the difference in information content.\n\n:kullback-leibler: Kullback-Leibler divergence (not symmetric, \\(KL(P||Q)\\)). \\[ D(P, Q) = \\sum_i P_i \\ln\\left(\\frac{P_i}{Q_i}\\right) \\]\n:jeffreys: Jeffreys divergence (symmetric KL). \\[ D(P, Q) = \\sum_i (P_i - Q_i) \\ln\\left(\\frac{P_i}{Q_i}\\right) \\]\n:k-divergence: K divergence (related to KL). \\[ D(P, Q) = \\sum_i P_i \\ln\\left(\\frac{2 P_i}{P_i + Q_i}\\right) \\]\n:topsoe: Topsoe divergence. \\[ D(P, Q) = \\sum_i \\left( P_i \\ln\\left(\\frac{2 P_i}{P_i + Q_i}\\right) + Q_i \\ln\\left(\\frac{2 Q_i}{P_i + Q_i}\\right) \\right) \\]\n:jensen-shannon: Jensen-Shannon divergence (symmetric, finite, based on KL). \\[ D(P, Q) = \\frac{1}{2} \\left( KL(P || M) + KL(Q || M) \\right), \\text{ where } M = \\frac{P+Q}{2} \\]\n:jensen-difference: Jensen difference divergence. \\[ D(P, Q) = \\sum_i \\left( \\frac{P_i \\ln P_i + Q_i \\ln Q_i}{2} - \\frac{(P_i+Q_i)}{2} \\ln\\left(\\frac{P_i+Q_i}{2}\\right) \\right) \\]\n:taneja: Taneja divergence. \\[ D(P, Q) = \\sum_i \\frac{P_i + Q_i}{2} \\ln\\left(\\frac{(P_i+Q_i)/2}{\\sqrt{P_i Q_i}}\\right) \\]\n:kumar-johnson: Kumar-Johnson divergence. \\[ D(P, Q) = \\sum_i \\frac{(P_i^2 - Q_i^2)^2}{2 (P_i Q_i)^{3/2}} \\]\n\nOther:\n\n:avg: Average of Manhattan and Chebyshev distances. \\[ D(P, Q) = \\frac{1}{2} \\left( \\sum_i |P_i - Q_i| + \\max_i |P_i - Q_i| \\right) \\]\n\n\nLet’s use the sepal length samples from the iris dataset.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/dissimilarity :euclidean setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.015621495942713016\n(stats/dissimilarity :manhattan setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.09134539463390746\n(stats/dissimilarity :chebyshev setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.005564421661826087\n(stats/dissimilarity :minkowski setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.015621495942713016\n(stats/dissimilarity :minkowski setosa-sepal-length virginica-sepal-length {:power 0.5}) ;; =&gt; 3.931956786218106\n(stats/dissimilarity :euclidean-sq setosa-sepal-length virginica-sepal-length) ;; =&gt; 2.4403113548819921E-4\n(stats/dissimilarity :squared-chord setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.0030351358423127664\n(stats/dissimilarity :sorensen setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.04567269731695373\n(stats/dissimilarity :gower setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.0018269078926781493\n(stats/dissimilarity :kulczynski setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.09571705050991852\n(stats/dissimilarity :canberra setosa-sepal-length virginica-sepal-length) ;; =&gt; 2.2736449691924085\n(stats/dissimilarity :lorentzian setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.09122364351967303\n(stats/dissimilarity :non-intersection setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.04567269731695373\n(stats/dissimilarity :wave-hedges setosa-sepal-length virginica-sepal-length) ;; =&gt; 4.267542427615257\n(stats/dissimilarity :czekanowski setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.04567269731695373\n(stats/dissimilarity :motyka setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.5228363486584767\n(stats/dissimilarity :tanimoto setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.08735562750016011\n(stats/dissimilarity :jaccard setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.012043840252620624\n(stats/dissimilarity :dice setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.0060584033473610865\n(stats/dissimilarity :bhattacharyya setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.001518720593674244\n(stats/dissimilarity :hellinger setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.0779119482789752\n(stats/dissimilarity :matusita setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.05509206696351892\n(stats/dissimilarity :pearson-chisq setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.01230611574919367\n(stats/dissimilarity :neyman-chisq setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.012113936206954518\n(stats/dissimilarity :squared-chisq setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.006058781702682577\n(stats/dissimilarity :symmetric-chisq setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.012117563405365154\n(stats/dissimilarity :divergence setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.30215920027968135\n(stats/dissimilarity :clark setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.38868959355743066\n(stats/dissimilarity :additive-symmetric-chisq setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.024420051956148194\n(stats/dissimilarity :kullback-leibler setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.006089967649043811\n(stats/dissimilarity :jeffreys setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.012148239424007154\n(stats/dissimilarity :k-divergence setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.0015126771903720877\n(stats/dissimilarity :topsoe setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.0030332163488238145\n(stats/dissimilarity :jensen-shannon setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.0015166081744119081\n(stats/dissimilarity :jensen-difference setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.001516608174411918\n(stats/dissimilarity :taneja setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.00152045168158987\n(stats/dissimilarity :kumar-johnson setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.024513334129211098\n(stats/dissimilarity :avg setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.04845490814786678\n\n\n\nWe can compare our data to a distribution. The method used here is based on building histogram for P and quantize distribution for Q.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/dissimilarity :chisq (stats/standardize setosa-sepal-length) r/default-normal) ;; =&gt; 0.11684559542476217\n(stats/dissimilarity :chisq setosa-sepal-length (r/distribution :normal {:mu (stats/mean setosa-sepal-length), :sd (stats/stddev setosa-sepal-length)})) ;; =&gt; 0.11684559542476244\n(stats/dissimilarity :chisq (repeatedly 1000 r/grand) r/default-normal) ;; =&gt; 0.021612937008293368\n\n\n\nIn case when counts of samples are not equal we can use histograms. Also we can bin our data before comparison.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/dissimilarity :gower (repeatedly 1000 r/grand) (repeatedly 800 r/grand) {:bins :auto}) ;; =&gt; 0.007386363636363636\n(stats/dissimilarity :gower setosa-sepal-length virginica-sepal-length {:bins :auto}) ;; =&gt; 0.22499999999999998\n(stats/dissimilarity :gower setosa-sepal-length virginica-sepal-length {:bins 10}) ;; =&gt; 0.18400000000000002\n\n\n\n\n\nSimilarity Methods\nHigher values generally indicate greater similarity.\n\nOverlap/Set-based:\n\n:intersection: Intersection measure (sum of element-wise minimums). \\[ S(P, Q) = \\sum_i \\min(P_i, Q_i) \\]\n:czekanowski: Czekanowski similarity (same as Sorensen-Dice). \\[ S(P, Q) = \\frac{2 \\sum_i \\min(P_i, Q_i)}{\\sum_i (P_i + Q_i)} \\]\n:motyka: Motyka similarity. \\[ S(P, Q) = \\frac{\\sum_i \\min(P_i, Q_i)}{\\sum_i (P_i + Q_i)} \\]\n:kulczynski: Kulczynski similarity (can be &gt; 1). \\[ S(P, Q) = \\frac{\\sum_i \\min(P_i, Q_i)}{\\sum_i |P_i - Q_i|} \\]\n:ruzicka: Ruzicka similarity. \\[ S(P, Q) = \\frac{\\sum_i \\min(P_i, Q_i)}{\\sum_i \\max(P_i, Q_i)} \\]\n:fidelity: Probability fidelity (Bhattacharyya coefficient). \\[ S(P, Q) = \\sum_i \\sqrt{P_i Q_i} \\]\n:squared-chord: Squared chord similarity (1 - Squared Chord dissimilarity). \\[ S(P, Q) = 2 \\sum_i \\sqrt{P_i Q_i} - 1 \\]\n\nInner Product / Angle:\n\n:inner-product: Inner product of the vectors. \\[ S(P, Q) = \\sum_i P_i Q_i \\]\n:cosine: Cosine similarity. \\[ S(P, Q) = \\frac{\\sum_i P_i Q_i}{\\sqrt{\\sum_i P_i^2} \\sqrt{\\sum_i Q_i^2}} \\]\n\nSet-based (adapted):\n\n:jaccard: Jaccard similarity (generalized to distributions). \\[ S(P, Q) = \\frac{\\sum_i P_i Q_i}{\\sum_i P_i^2 + \\sum_i Q_i^2 - \\sum_i P_i Q_i} \\]\n:dice: Dice similarity (generalized to distributions). \\[ S(P, Q) = \\frac{2 \\sum_i P_i Q_i}{\\sum_i P_i^2 + \\sum_i Q_i^2} \\]\n\nHarmonic Mean:\n\n:harmonic-mean: Harmonic mean similarity. \\[ S(P, Q) = 2 \\sum_i \\frac{P_i Q_i}{P_i + Q_i} \\]\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/similarity :intersection setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.9543273026830466\n(stats/similarity :czekanowski setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.9543273026830466\n(stats/similarity :motyka setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.4771636513415233\n(stats/similarity :kulczynski setosa-sepal-length virginica-sepal-length) ;; =&gt; 10.447459409505903\n(stats/similarity :ruzicka setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.9126443724998404\n(stats/similarity :fidelity setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.9984824320788436\n(stats/similarity :squared-chord setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.9969648641576871\n(stats/similarity :inner-product setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.020017872905882722\n(stats/similarity :cosine setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.9939438317187768\n(stats/similarity :jaccard setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.9879561597473809\n(stats/similarity :dice setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.9939415966526397\n(stats/similarity :harmonic-mean setosa-sepal-length virginica-sepal-length) ;; =&gt; 0.9969706091486588\n\n\n\nAs for dissimilarity, we can compare our data to a distribution. The method used here is based on building histogram for P and quantize distribution for Q.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/similarity :dice (stats/standardize setosa-sepal-length) r/default-normal) ;; =&gt; 0.941700344004566\n(stats/similarity :dice setosa-sepal-length (r/distribution :normal {:mu (stats/mean setosa-sepal-length), :sd (stats/stddev setosa-sepal-length)})) ;; =&gt; 0.9417003440045658\n(stats/similarity :dice (repeatedly 10000 r/grand) r/default-normal) ;; =&gt; 0.9984869550039952\n\n\n\nIn case when counts of samples are not equal we can use histograms. Also we can bin our data before comparison.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/similarity :ruzicka (repeatedly 1000 r/grand) (repeatedly 800 r/grand) {:bins :auto}) ;; =&gt; 0.8535681186283597\n(stats/similarity :ruzicka setosa-sepal-length virginica-sepal-length {:bins :auto}) ;; =&gt; 0.052631578947368425\n(stats/similarity :ruzicka setosa-sepal-length virginica-sepal-length {:bins 10}) ;; =&gt; 0.04166666666666666",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#contingency-tables",
    "href": "stats.html#contingency-tables",
    "title": "Statistics",
    "section": "Contingency Tables",
    "text": "Contingency Tables\nFunctions for creating and analyzing contingency tables.\n\n\n\n\n\n\nDefined functions\n\n\n\n\ncontingency-table, rows-&gt;contingency-table, contingency-table-&gt;marginals\ncontingency-2x2-measures-all, contingency-2x2-measures\nmcc\ncramers-c, cramers-v, cramers-v-corrected\ncohens-w, tschuprows-t\ncohens-kappa, weighted-kappa\n\n\n\nContingency tables, also known as cross-tabulations or cross-tabs, are a fundamental tool in statistics for displaying the frequency distribution of two or more categorical variables. They help visualize and analyze the relationship or association between these variables.\nFor two variables, a contingency table typically looks like this:\n\n\n\n\n\n\n\n\n\n\n\nCategory 1 (Col)\nCategory 2 (Col)\n…\nColumn Totals (Marginals)\n\n\n\n\nCat A (Row)\n\\(n_{11}\\)\n\\(n_{12}\\)\n…\n\\(R_1 = \\sum_j n_{1j}\\)\n\n\nCat B (Row)\n\\(n_{21}\\)\n\\(n_{22}\\)\n…\n\\(R_2 = \\sum_j n_{2j}\\)\n\n\n…\n…\n…\n…\n…\n\n\nRow Totals (Marginals)\n\\(C_1 = \\sum_i n_{i1}\\)\n\\(C_2 = \\sum_i n_{i2}\\)\n…\n\\(N = \\sum_i \\sum_j n_{ij}\\)\n\n\n\nWhere \\(n_{ij}\\) is the count of observations falling into row category \\(i\\) and column category \\(j\\). \\(R_i\\) are row marginal totals, \\(C_j\\) are column marginal totals, and \\(N\\) is the grand total number of observations.\nLet’s use data from wikipedia article as the 2x2 example.\n\n\n\n\n\n\nRight-handed\nLeft-handed\nTotal\n\n\nMale\n43\n9\n52\n\n\nFemale\n44\n4\n48\n\n\nTotal\n87\n13\n100\n\n\n\n\n\n\n(def ct-data-1 [[43 9] [44 4]])\n\nAnother example will be from openstax book\n\n\n\n\n\n\nLake Path\nHilly Path\nWooded Path\nTotal\n\n\nYounger\n45\n38\n27\n110\n\n\nOlder\n26\n52\n12\n90\n\n\nTotal\n71\n90\n39\n200\n\n\n\n\n\n\n(def ct-data-2 [[45 38 27] [26 52 12]])\n\nThe last example will consist two sequences for gender and exam outcome\n\n(def gender [:male :female :male :female :male :male :female :female :female :male :male])\n\n\n(def outcome [:pass :fail :pass :pass :fail :pass :fail :pass :pass :pass :pass])\n\nfastmath.stats provides functions for creating and analyzing these tables:\n\ncontingency-table: Creates a frequency map from one or more sequences. If given two sequences of equal length, say vars1 and vars2, it produces a map where keys are pairs [value-from-vars1, value-from-vars2] and values are the counts of these co-occurrences.\nrows-&gt;contingency-table: Takes a sequence of sequences, interpreted as rows of counts in a grid, and converts it into a map format where keys are [row-index, column-index] and values are the non-zero counts. This is useful for inputting tables structured as lists of lists.\ncontingency-table-&gt;marginals: Calculates the row totals (:rows), column totals (:cols), grand total (:n), and diagonal elements (:diag) from a contingency table (either in map format or sequence of sequences format).\n\nLet’s create a simple contingency table from above data.\n\n(def ct-gender-outcome (stats/contingency-table gender outcome))\n\n\n(def contingency-table-1 (stats/rows-&gt;contingency-table ct-data-1))\n\n\n(def contingency-table-2 (stats/rows-&gt;contingency-table ct-data-2))\n\n\n\n\n\n\n\nExamples\n\n\n\n\nct-gender-outcome ;; =&gt; {[:male :pass] 5, [:female :fail] 2, [:female :pass] 3, [:male :fail] 1}\ncontingency-table-1 ;; =&gt; {[0 0] 43, [0 1] 9, [1 0] 44, [1 1] 4}\ncontingency-table-2 ;; =&gt; {[0 0] 45, [0 1] 38, [0 2] 27, [1 0] 26, [1 1] 52, [1 2] 12}\n(stats/contingency-table-&gt;marginals ct-gender-outcome) ;; =&gt; {:rows ([:female 5.0] [:male 6.0]), :cols ([:fail 3.0] [:pass 8.0]), :n 11.0, :diag ()}\n(stats/contingency-table-&gt;marginals contingency-table-1) ;; =&gt; {:rows ([0 52.0] [1 48.0]), :cols ([0 87.0] [1 13.0]), :n 100.0, :diag ([[0 0] 43] [[1 1] 4])}\n(stats/contingency-table-&gt;marginals contingency-table-2) ;; =&gt; {:rows ([0 110.0] [1 90.0]), :cols ([0 71.0] [1 90.0] [2 39.0]), :n 200.0, :diag ([[0 0] 45] [[1 1] 52])}\n\n\n\nMeasures of Association: These statistics quantify the strength and nature of the relationship between the variables in the table. They are often derived from the Pearson’s Chi-squared statistic (\\(\\chi^2\\), obtainable via [chisq-test]), which tests for independence.\n\ncramers-c: Cramer’s C is a measure of association for any \\(R \\times K\\) contingency table. It ranges from 0 to 1, where 0 indicates no association and 1 indicates perfect association. \\[ C = \\sqrt{\\frac{\\chi^2}{N + \\chi^2}} \\]\ncramers-v: Cramer’s V is another measure of association, also ranging from 0 to 1. It is widely used and is often preferred over Tschuprow’s T because it can attain the value 1 even for non-square tables. \\[ V = \\sqrt{\\frac{\\chi^2/N}{\\min(R-1, K-1)}} \\]\ncramers-v-corrected: Corrected Cramer’s V (\\(V^*\\)) applies a bias correction, which is particularly important for small sample sizes or tables with many cells having low expected counts.\ncohens-w: Cohen’s W is a measure of effect size for Chi-squared tests. It quantifies the magnitude of the difference between observed and expected frequencies. It ranges from 0 upwards, with 0 indicating no difference (independence). \\[ W = \\sqrt{\\frac{\\chi^2}{N}} \\]\ntschuprows-t: Tschuprow’s T is a measure of association ranging from 0 to 1. However, it can only reach 1 in square tables (\\(R=K\\)). \\[ T = \\sqrt{\\frac{\\chi^2/N}{\\sqrt{(R-1)(K-1)}}} \\]\n\nfastmath.stats allows you to calculate these measures by providing either the raw sequences or a pre-calculated contingency table:\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/cramers-c ct-gender-outcome) ;; =&gt; 0.25242645609111675\n(stats/cramers-v ct-gender-outcome) ;; =&gt; 0.26087459737497565\n(stats/cramers-v-corrected ct-gender-outcome) ;; =&gt; 0.0\n(stats/cohens-w ct-gender-outcome) ;; =&gt; 0.26087459737497565\n(stats/tschuprows-t ct-gender-outcome) ;; =&gt; 0.26087459737497565\n(stats/cramers-c contingency-table-1) ;; =&gt; 0.13215047155447307\n(stats/cramers-v contingency-table-1) ;; =&gt; 0.13331972997326805\n(stats/cramers-v-corrected contingency-table-1) ;; =&gt; 0.08804224922800516\n(stats/cohens-w contingency-table-1) ;; =&gt; 0.13331972997326805\n(stats/tschuprows-t contingency-table-1) ;; =&gt; 0.13331972997326805\n(stats/cramers-c contingency-table-2) ;; =&gt; 0.22972682313063483\n(stats/cramers-v contingency-table-2) ;; =&gt; 0.23603966869637247\n(stats/cramers-v-corrected contingency-table-2) ;; =&gt; 0.21423142299458464\n(stats/cohens-w contingency-table-2) ;; =&gt; 0.23603966869637247\n(stats/tschuprows-t contingency-table-2) ;; =&gt; 0.19848491126445403\n\n\n\nMeasures of Agreement: These statistics are typically used for square tables (\\(R=K\\)) to assess the consistency of agreement between two independent raters or methods that categorize items.\n\ncohens-kappa: Cohen’s Kappa (\\(\\kappa\\)) measures the agreement between two raters for nominal (or ordinal) categories, correcting for the agreement that would be expected by chance. It ranges from -1 (perfect disagreement) to +1 (perfect agreement), with 0 indicating agreement equivalent to chance. \\[ \\kappa = \\frac{p_0 - p_e}{1 - p_e} \\] where \\(p_0\\) is the observed proportional agreement (sum of diagonal cells divided by N) and \\(p_e\\) is the proportional agreement expected by chance (based on marginal probabilities).\nweighted-kappa: Weighted Kappa (\\(\\kappa_w\\)) is an extension of Cohen’s Kappa that allows for different levels of disagreement penalties, suitable for ordinal categories. Disagreements between categories that are closer together (e.g., “Good” vs “Very Good”) are penalized less than disagreements between categories that are further apart (e.g., “Poor” vs “Excellent”). It requires specifying a weighting scheme (e.g., :equal-spacing, :fleiss-cohen).\n\nExamples for agreement measures (using the [rows-&gt;contingency-table] format for clarity on cell positions):\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/cohens-kappa ct-gender-outcome) ;; =&gt; -1.0862068965517244\n(stats/cohens-kappa contingency-table-1) ;; =&gt; -0.09233305853256403\n(stats/weighted-kappa contingency-table-1 :equal-spacing) ;; =&gt; -0.09233305853256403\n(stats/weighted-kappa contingency-table-1 :fleiss-cohen) ;; =&gt; -0.09233305853256403\n(stats/weighted-kappa contingency-table-1 {[0 0] 0.1, [1 0] 0.2, [0 1] 0.4, [1 1] 0.3}) ;; =&gt; 0.005427145418423201\n(stats/weighted-kappa contingency-table-1 (fn [dim row-id col-id] (/ (max row-id col-id) dim))) ;; =&gt; 0.049513704686118425\n\n\n\n2x2 Specific Measures: These functions are tailored for 2x2 tables.\n\nmcc: Calculates the [mcc], which is equivalent to the Phi coefficient for a 2x2 table. It is a single, balanced measure of classification performance, ranging from -1 to +1.\ncontingency-2x2-measures: A convenience function that returns a map containing a selection of commonly used statistics specifically for 2x2 tables (Chi-squared, Kappa, Phi, Yule’s Q, Odds Ratio, etc.).\ncontingency-2x2-measures-all: Provides a very comprehensive map of measures for a 2x2 table, including various Chi-squared statistics and p-values, measures of association, agreement, and risk/effect size measures like Odds Ratio (OR), Relative Risk (RR), and Number Needed to Treat (NNT). It’s the most detailed summary for a 2x2 table.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/mcc contingency-table-1) ;; =&gt; -0.133319729973268\n\n\n\nThe contingency-2x2-measures-all function accepts the counts a, b, c, d directly, a sequence [a b c d], a matrix [[a b] [c d]], or a map {:a a :b b :c c :d d}.\nThe map returned by contingency-2x2-measures-all is extensive. Here’s a description of its main keys and their contents:\n\n:n: The grand total number of observations in the table (\\(N\\)).\n:table: A map representation of the input counts, typically {:a a, :b b, :c c, :d d} corresponding to the top-left, top-right, bottom-left, bottom-right cells.\n:expected: A map of the expected counts {:a exp_a, :b exp_b, :c exp_c, :d exp_d} for each cell under the assumption of independence.\n:marginals: A map containing the row totals (:row1, :row2), column totals (:col1, :col2), and the grand total (:total).\n:proportions: A nested map providing proportions:\n\n:table: Cell counts divided by the grand total (N).\n:rows: Cell counts divided by their respective row totals (conditional probabilities of columns given rows).\n:cols: Cell counts divided by their respective column totals (conditional probabilities of rows given columns).\n:marginals: Row and column totals divided by the grand total (N).\n\n:p-values: A map containing p-values for various Chi-squared tests:\n\n:chi2: Pearson’s Chi-squared test p-value.\n:yates: Yates’ continuity corrected Chi-squared test p-value.\n:cochran-mantel-haenszel: Cochran-Mantel-Haenszel statistic p-value (useful for stratified data, but calculated for the single table here).\n\n:OR: The Odds Ratio (\\(OR\\)) for the 2x2 table, typically \\((a \\times d) / (b \\times c)\\). Quantifies the strength of association, representing the odds of an outcome occurring in one group compared to the odds in another group.\n:lOR: The natural logarithm of the Odds Ratio (\\(\\ln(OR)\\)). Useful for constructing confidence intervals for the OR.\n:RR: The Relative Risk (\\(RR\\)) for the 2x2 table, typically \\((a/(a+b)) / (c/(c+d))\\). Compares the probability of an outcome in one group to the probability in another group. Requires row marginals to represent the exposed/unexposed or intervention/control groups.\n:risk: A map containing various risk-related measures, especially relevant in epidemiology or clinical trials, derived from the counts assuming a specific row structure (e.g., exposure/outcome):\n\n:RR: Relative Risk (same as top-level :RR).\n:RRR: Relative Risk Reduction (\\(1 - RR\\)).\n:RD: Risk Difference (\\(a/(a+b) - c/(c+d)\\)).\n:ES: Exposure Sample size (row 1 total).\n:CS: Control Sample size (row 2 total).\n:EER: Experimental Event Rate (\\(a/(a+b)\\)).\n:CER: Control Event Rate (\\(c/(c+d)\\)).\n:ARR: Absolute Risk Reduction (\\(CER - EER\\)).\n:NNT: Number Needed to Treat (\\(1 / ARR\\)). The average number of patients who need to be treated to prevent one additional bad outcome.\n:ARI: Absolute Risk Increase (\\(-ARR\\)).\n:NNH: Number Needed to Harm (\\(-NNT\\)).\n:RRI: Relative Risk Increase (\\(RR - 1\\)).\n:AFe: Attributable Fraction among the exposed (\\((EER - CER)/EER\\)).\n:PFu: Prevented Fraction among the unexposed (\\(1 - RR\\)).\n\n:SE: Standard Error related values used in some calculations.\n:measures: A map containing a wide array of statistical measures:\n\n:chi2: Pearson’s Chi-squared statistic (same as :p-values :chi2 but the statistic value).\n:yates: Yates’ continuity corrected Chi-squared statistic.\n:cochran-mantel-haenszel: Cochran-Mantel-Haenszel statistic.\n:cohens-kappa: Cohen’s Kappa coefficient.\n:yules-q: Yule’s Q, a measure of association based on the Odds Ratio. Ranges -1 to +1.\n:holley-guilfords-g: Holley-Guilford’s G.\n:huberts-gamma: Hubert’s Gamma.\n:yules-y: Yule’s Y, based on square roots of cell proportions. Ranges -1 to +1.\n:cramers-v: Cramer’s V measure of association.\n:phi: Phi coefficient (equivalent to MCC for 2x2 tables), calculated as \\(\\frac{ad-bc}{\\sqrt{(a+b)(c+d)(a+c)(b+d)}}\\). Ranges -1 to +1.\n:scotts-pi: Scott’s Pi, another measure of inter-rater reliability, similar to Kappa.\n:cohens-h: Cohen’s H, a measure of effect size for comparing two proportions.\n:PCC: Pearson’s Contingency Coefficient.\n:PCC-adjusted: Adjusted Pearson’s Contingency Coefficient.\n:TCC: Tschuprow’s Contingency Coefficient.\n:F1: F1 Score, common in binary classification, the harmonic mean of precision and recall. Calculated assuming a=TP, b=FP, c=FN, d=TN.\n:bangdiwalas-b: Bangdiwala’s B, a measure of agreement/reproducibility for clustered data, but calculated for the 2x2 table.\n:mcnemars-chi2: McNemar’s Chi-squared test statistic, specifically for paired nominal data (e.g., before/after) to test for changes in proportions. Calculated from off-diagonal elements (\\(b\\) and \\(c\\)).\n:gwets-ac1: Gwet’s AC1, another measure of inter-rater reliability claimed to be more robust to prevalence issues than Kappa.\n\n\n\n(stats/contingency-2x2-measures-all (flatten ct-data-1))\n\n\n{:OR 0.43434343434343436,\n :RR 0.9020979020979021,\n :SE 0.6380393387494788,\n :expected {:a 45.24, :b 6.76, :c 41.76, :d 6.24},\n :lOR -0.8339197344410274,\n :marginals {:col1 87, :col2 13, :row1 52, :row2 48, :total 100},\n :measures {:F1 0.6187050359712231,\n            :PCC 0.13215047155447307,\n            :PCC-adjusted 0.18688898914633573,\n            :TCC -0.3172384169581813,\n            :bangdiwalas-b 0.3622766122766123,\n            :chi2 1.7774150400145103,\n            :cochran-mantel-haenszel 1.7596408896143674,\n            :cohens-h -0.27245415531951833,\n            :cohens-kappa -0.09233305853256389,\n            :cramers-v 0.133319729973268,\n            :gwets-ac1 0.07994097734571652,\n            :holley-guilfords-g -0.06,\n            :huberts-gamma 0.0036,\n            :mcnemars-chi2 23.11320754716981,\n            :phi -0.133319729973268,\n            :scotts-pi -0.2501474230451704,\n            :yates 1.0724852071005921,\n            :youdens-j -0.08974358974358974,\n            :yules-q -0.3943661971830986,\n            :yules-y -0.20551108882876298},\n :n 100,\n :p-values {:chi2 0.1824670652605519,\n            :cochran-mantel-haenszel 0.18466931414678545,\n            :yates 0.3003847703905692},\n :proportions {:cols {:a 0.4942528735632184,\n                      :b 0.6923076923076923,\n                      :c 0.5057471264367817,\n                      :d 0.3076923076923077},\n               :marginals {:col1 0.87, :col2 0.13, :row1 0.52, :row2 0.48},\n               :rows {:a 0.8269230769230769,\n                      :b 0.17307692307692307,\n                      :c 0.9166666666666666,\n                      :d 0.08333333333333333},\n               :table {:a 0.43, :b 0.09, :c 0.44, :d 0.04}},\n :risk {:AFe -0.10852713178294575,\n        :ARI -0.08974358974358976,\n        :ARR 0.08974358974358976,\n        :CER 0.9166666666666666,\n        :CS 48,\n        :EER 0.8269230769230769,\n        :ES 52,\n        :NNH -11.14285714285714,\n        :NNT 11.14285714285714,\n        :PFu 0.09790209790209792,\n        :RD -0.08974358974358976,\n        :RR 0.9020979020979021,\n        :RRI -0.09790209790209792,\n        :RRR 0.09790209790209792},\n :table {:a 43, :b 9, :c 44, :d 4}}\n\n\nBinary Classification Metrics\nMetrics derived from a 2x2 confusion matrix are essential for evaluating the performance of algorithms in binary classification tasks (problems where the outcome belongs to one of two classes, e.g., “positive” or “negative”, “success” or “failure”). These metrics quantify how well a classifier distinguishes between the two classes.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nconfusion-matrix\nbinary-measures-all, binary-measures\n\n\n\nThe foundation for these metrics is the confusion matrix, a 2x2 table summarizing the results by comparing the actual class of each instance to the predicted class:\n\n\n\n\n\n\n\n\n\n\nPredicted Positive (P’)\nPredicted Negative (N’)\nTotal\n\n\n\n\nActual Positive (P)\nTrue Positives (TP)\nFalse Negatives (FN)\nP\n\n\nActual Negative (N)\nFalse Positives (FP)\nTrue Negatives (TN)\nN\n\n\nTotal\nP’\nN’\nTotal\n\n\n\n\nTrue Positives (TP): Instances correctly predicted as positive.\nFalse Negatives (FN): Instances incorrectly predicted as negative (Type II error). These are positive instances missed by the classifier.\nFalse Positives (FP): Instances incorrectly predicted as positive (Type I error). These are negative instances misclassified as positive.\nTrue Negatives (TN): Instances correctly predicted as negative.\n\nThe counts from these four cells form the basis for almost all binary classification metrics.\nfastmath.stats provides functions to generate and analyze these matrices:\n\nconfusion-matrix: This function constructs the 2x2 confusion matrix counts (:tp, :fn, :fp, :tn). It can take the four counts directly, a structured map/sequence representation, or two sequences of actual and predicted outcomes. It can also handle different data types for outcomes using an encode-true parameter.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/confusion-matrix 10 2 5 80) ;; =&gt; {:tp 10, :fn 2, :fp 5, :tn 80}\n(stats/confusion-matrix [10 2 5 80]) ;; =&gt; {:tp 10, :fn 2, :fp 5, :tn 80}\n(stats/confusion-matrix [[10 5] [2 80]]) ;; =&gt; {:tp 10, :fn 5, :fp 2, :tn 80}\n(stats/confusion-matrix {:tp 10, :fn 2, :fp 5, :tn 80}) ;; =&gt; {:tp 10, :fn 2, :fp 5, :tn 80}\n(stats/confusion-matrix [:pos :neg :pos :pos :neg :pos :neg :pos :neg :pos :pos] [:pos :neg :pos :pos :neg :pos :neg :neg :pos :pos :pos] #{:pos}) ;; =&gt; {:tp 6, :tn 3, :fn 1, :fp 1}\n(stats/confusion-matrix [1 0 1 1 0 1 0 1 0 1 1] [1 0 1 1 0 1 0 0 1 1 1]) ;; =&gt; {:tp 6, :tn 3, :fn 1, :fp 1}\n(stats/confusion-matrix [true false true true false true false true false true true] [true false true true false true false false true true true]) ;; =&gt; {:tp 6, :tn 3, :fn 1, :fp 1}\n\n\n\n\nbinary-measures-all: This is the primary function for calculating a wide range of metrics from a 2x2 confusion matrix. It accepts the same input formats as confusion-matrix. It returns a map containing a comprehensive set of derived statistics.\nbinary-measures: A convenience function that calls binary-measures-all but returns a smaller, more commonly used subset of the metrics. It accepts the same input formats.\n\nThe map returned by binary-measures-all contains numerous metrics. Key metrics include:\n\n:tp, :fn, :fp, :tn: The raw counts from the confusion matrix.\n:total: Total number of instances (\\(TP + FN + FP + TN\\)).\n:cp, :cn, :pcp, :pcn: Marginal totals (e.g., :cp is actual positives \\(TP + FN\\)).\n:accuracy: Overall proportion of correct predictions: \\[ Accuracy = \\frac{TP + TN}{TP + FN + FP + TN} \\]\n:sensitivity, :recall, :tpr: True Positive Rate (proportion of actual positives correctly identified): \\[ Sensitivity = \\frac{TP}{TP + FN} \\]\n:specificity, :tnr: True Negative Rate (proportion of actual negatives correctly identified): \\[ Specificity = \\frac{TN}{FP + TN} \\]\n:precision, :ppv: Positive Predictive Value (proportion of positive predictions that were actually positive): \\[ Precision = \\frac{TP}{TP + FP} \\]\n:fdr: False Discovery Rate (\\(1 - Precision\\)).\n:npv: Negative Predictive Value (proportion of negative predictions that were actually negative): \\[ NPV = \\frac{TN}{FN + TN} \\]\n:for: False Omission Rate (\\(1 - NPV\\)).\n:fpr: False Positive Rate (proportion of actual negatives incorrectly identified as positive): \\[ FPR = \\frac{FP}{FP + TN} \\]\n:fnr: False Negative Rate (proportion of actual positives incorrectly identified as negative): \\[ FNR = \\frac{FN}{TP + FN} \\]\n:f1-score, :f-measure: The harmonic mean of Precision and Recall. It provides a balance between the two: \\[ F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} \\]\n:mcc, :phi: Matthews Correlation Coefficient / Phi coefficient. A balanced measure ranging from -1 to +1, considered robust for imbalanced datasets: \\[ MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}} \\]\n:prevalence: Proportion of the positive class in the actual data: \\[ Prevalence = \\frac{TP + FN}{TP + FN + FP + TN} \\]\nOther metrics like :ba (Balanced Accuracy), :lr+, :lr-, :dor, :fm, :pt, :bm, :kappa (Cohen’s Kappa for 2x2), :mk, and :f-beta (a function to calculate F-beta score for any beta value).\n\nbinary-measures returns a map containing :tp, :tn, :fp, :fn, :accuracy, :fdr, :f-measure, :fall-out (FPR), :precision, :recall (Sensitivity/TPR), :sensitivity, :specificity (TNR), and :prevalence. This provides a useful quick summary. For a deeper dive or specific niche metrics, use binary-measures-all.\nLet’s calculate the metrics for a sample confusion matrix {:tp 10 :fn 2 :fp 5 :tn 80}.\n\n(stats/binary-measures-all {:tp 10, :fn 2, :fp 5, :tn 80})\n\n\n{:accuracy 0.9278350515463918,\n :ba 0.8872549019607843,\n :bm 0.7745098039215685,\n :cn 85.0,\n :cp 12.0,\n :dor 80.00000000000003,\n :f-beta #&lt;Fn@2ec64c87 fastmath.stats/binary_measures_all_calc[fn]&gt;,\n :f-measure 0.7407407407407408,\n :f1-score 0.7407407407407408,\n :fall-out 0.058823529411764705,\n :fdr 0.33333333333333337,\n :fm 0.7453559924999299,\n :fn 2,\n :fnr 0.16666666666666663,\n :for 0.024390243902439046,\n :fp 5,\n :fpr 0.058823529411764705,\n :hit-rate 0.8333333333333334,\n :jaccard 0.5882352941176471,\n :kappa 0.6994245241257193,\n :lr+ 14.166666666666668,\n :lr- 0.1770833333333333,\n :mcc 0.7053009189406806,\n :miss-rate 0.16666666666666663,\n :mk 0.6422764227642275,\n :n 85.0,\n :npv 0.975609756097561,\n :p 12.0,\n :pcn 82.0,\n :pcp 15.0,\n :phi 0.7053009189406806,\n :pn 82.0,\n :pp 15.0,\n :ppv 0.6666666666666666,\n :precision 0.6666666666666666,\n :prevalence 0.12371134020618557,\n :pt 0.20991366558572697,\n :recall 0.8333333333333334,\n :selectivity 0.9411764705882353,\n :sensitivity 0.8333333333333334,\n :specificity 0.9411764705882353,\n :tn 80,\n :tnr 0.9411764705882353,\n :total 97.0,\n :tp 10,\n :tpr 0.8333333333333334,\n :ts 0.5882352941176471}\n\n\n(stats/binary-measures {:tp 10, :fn 2, :fp 5, :tn 80})\n\n\n{:accuracy 0.9278350515463918,\n :f-measure 0.7407407407407408,\n :fall-out 0.058823529411764705,\n :fdr 0.33333333333333337,\n :fn 2,\n :fp 5,\n :precision 0.6666666666666666,\n :prevalence 0.12371134020618557,\n :recall 0.8333333333333334,\n :sensitivity 0.8333333333333334,\n :specificity 0.9411764705882353,\n :tn 80,\n :tp 10}",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#effect-size",
    "href": "stats.html#effect-size",
    "title": "Statistics",
    "section": "Effect Size",
    "text": "Effect Size\nMeasures quantifying the magnitude of a phenomenon or relationship.\n\nDifference Family\nMeasures quantifying the magnitude of the difference between two group means, standardized by a measure of variability. These are widely used effect size statistics, particularly in comparing experimental and control groups or two distinct conditions.\n\n\n\n\n\n\nDefined functions\n\n\n\n\ncohens-d, cohens-d-corrected\nhedges-g, hedges-g-corrected, hedges-g*\nglass-delta\n\n\n\nThese functions primarily quantify the difference between the means of two groups (usually group 1 and group 2: \\(\\bar{x}_1 - \\bar{x}_2\\)), scaled by some estimate of the population standard deviation.\n\nCohen’s d (cohens-d): Measures the difference between two means divided by the pooled standard deviation. It assumes equal variances between the groups. The formula is: \\[ d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p} \\] where \\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\) is the unbiased pooled standard deviation. An optional method argument can be passed to select the pooled standard deviation calculation method (see [pooled-stddev]).\nHedges’ g (hedges-g): In fastmath.stats, this function is equivalent to cohens-d using the default (unbiased) pooled standard deviation. Conceptually, Hedges’ g also divides the mean difference by a pooled standard deviation, but often refers to a bias-corrected version for small samples (see below). \\[ g = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p} \\]\nBias-Corrected Cohen’s d (cohens-d-corrected) / Bias-Corrected Hedges’ g (hedges-g-corrected): These functions apply a small-sample bias correction factor to Cohen’s d or Hedges’ g. This correction factor (an approximation) is multiplied by the calculated d or g value to provide a less biased estimate of the population effect size, especially for small sample sizes (\\(n &lt; 20\\)). The correction factor is approximately \\(1 - \\frac{3}{4\\nu - 1}\\), where \\(\\nu = n_1+n_2-2\\) are the degrees of freedom.\nExact Bias-Corrected Hedges’ g (hedges-g*): Applies the precise bias correction factor \\(J(\\nu)\\) (based on the Gamma function) to Hedges’ g (or Cohen’s d with unbiased pooling). This provides the most theoretically accurate bias correction for small samples. \\[ g^* = g \\cdot J(\\nu) \\] where \\(\\nu = n_1+n_2-2\\) and \\(J(\\nu) = \\frac{\\Gamma(\\nu/2)}{\\sqrt{\\nu/2} \\Gamma((\\nu-1)/2)}\\).\nGlass’s Delta (glass-delta): Measures the difference between two means divided by the standard deviation of the control group (conventionally group 2). \\[ \\Delta = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_2} \\] This is useful when the control group’s variance is considered a better estimate of the population variance than a pooled variance, or when the intervention is expected to affect variance.\n\nThe returned value from these functions represents the difference between the means of Group 1 and Group 2, expressed in units of the relevant standard deviation (pooled for Cohen’s/Hedges’, control group’s for Glass’s).\n\nA positive value indicates that the mean of Group 1 is greater than the mean of Group 2.\nA negative value indicates that the mean of Group 1 is less than the mean of Group 2.\nThe magnitude of the value indicates the size of the difference relative to the spread of the data. Cohen’s informal guidelines for interpreting the magnitude of d (and often g) are:\n\n\\(|d| \\approx 0.2\\): small effect\n\\(|d| \\approx 0.5\\): medium effect\n\\(|d| \\approx 0.8\\): large effect\n\n\nComparison:\n\ncohens-d and hedges-g (as implemented here) are standard measures for the mean difference scaled by pooled standard deviation, assuming equal variances. Cohen’s d is more commonly cited, while Hedges’ g is often used when sample size is small.\ncohens-d-corrected, hedges-g-corrected, and hedges-g* are preferred over the standard d/g when sample sizes are small, as they provide less biased estimates of the population effect size. hedges-g* uses the most accurate correction formula.\nglass-delta is distinct in using only the control group’s standard deviation for scaling. Use this when the variance of the control group is more appropriate as a baseline (e.g., comparing an experimental group to a control, or if the intervention might affect variance).\n\nLet’s illustrate these with the sepal lengths from the iris dataset, using ‘virginica’ as group 1 and ‘setosa’ as group 2.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/cohens-d virginica-sepal-length setosa-sepal-length) ;; =&gt; 3.0772391640158845\n(stats/cohens-d-corrected virginica-sepal-length setosa-sepal-length) ;; =&gt; 3.053628633345686\n(stats/cohens-d virginica-sepal-length setosa-sepal-length :biased) ;; =&gt; 3.1084809717263635\n(stats/cohens-d-corrected virginica-sepal-length setosa-sepal-length :biased) ;; =&gt; 3.0851089343449623\n(stats/cohens-d virginica-sepal-length setosa-sepal-length :avg) ;; =&gt; 3.0772391640158845\n(stats/cohens-d-corrected virginica-sepal-length setosa-sepal-length :avg) ;; =&gt; 3.053628633345686\n(stats/hedges-g virginica-sepal-length setosa-sepal-length) ;; =&gt; 3.0772391640158845\n(stats/hedges-g-corrected virginica-sepal-length setosa-sepal-length) ;; =&gt; 3.053628633345686\n(stats/hedges-g* virginica-sepal-length setosa-sepal-length) ;; =&gt; 3.0536185452069557\n(stats/glass-delta virginica-sepal-length setosa-sepal-length) ;; =&gt; 4.488074566113517\n\n\n\n\n\nRatio Family\nEffect size measures in the ratio family quantify the difference between two group means as a multiplicative factor, rather than a difference. This is useful when comparing values that are inherently ratios or when expressing the effect in terms of relative change.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nmeans-ratio, means-ratio-corrected\n\n\n\n\nRatio of Means (means-ratio): Calculates the ratio of the mean of the first group (\\(\\bar{x}_1\\)) to the mean of the second group (\\(\\bar{x}_2\\)). \\[ Ratio = \\frac{\\bar{x}_1}{\\bar{x}_2} \\] A value greater than 1 means the first group’s mean is larger; less than 1 means it’s smaller. A value of 1 indicates equal means.\nCorrected Ratio of Means (means-ratio-corrected): Applies a small-sample bias correction to the simple ratio of means. This provides a less biased estimate of the population ratio, especially for small sample sizes. The correction is based on incorporating the variances of the two groups. This function is equivalent to calling (means-ratio group1 group2 true).\n\nUse means-ratio for a direct, uncorrected ratio. Use means-ratio-corrected for a less biased estimate of the population ratio, particularly advisable with small sample sizes.\nLet’s calculate these for the virginica and setosa sepal lengths from the iris dataset.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/means-ratio virginica-sepal-length setosa-sepal-length) ;; =&gt; 1.316020775069916\n(stats/means-ratio-corrected virginica-sepal-length setosa-sepal-length) ;; =&gt; 1.3160781315136816\n\n\n\n\n\nOrdinal / Non-parametric Family\nEffect size measures in the ordinal or non-parametric family are suitable for data that are not normally distributed, have outliers, or are measured on an ordinal scale. They are often based on comparing pairs of observations between two groups.\n\n\n\n\n\n\nDefined functions\n\n\n\n\ncliffs-delta\nameasure, wmw-odds\n\n\n\n\nCliff’s Delta (cliffs-delta): A non-parametric measure of the amount of separation or overlap between two distributions. It is calculated as the probability that a randomly selected observation from one group is greater than a randomly selected observation from the other group, minus the probability of the reverse:\n\\[\\delta = P(X &gt; Y) - P(Y &gt; X)\\]\nIt ranges from -1 (complete separation, with values in the second group always greater than values in the first) to +1 (complete separation, with values in the first group always greater than values in the second). A value of 0 indicates complete overlap (stochastic equality). It is a robust alternative to Cohen’s d when assumptions for parametric tests are not met.\nVargha-Delaney A (ameasure): A non-parametric measure of stochastic superiority. It quantifies the probability that a randomly chosen value from the first sample (group1) is greater than a randomly chosen value from the second sample (group2).\n\\[A = P(X &gt; Y)\\]\nIt ranges from 0 to 1. A value of 0.5 indicates stochastic equality (distributions are overlapping). Values &gt; 0.5 mean group1 tends to have larger values; values &lt; 0.5 mean group2 tends to have larger values. It is directly related to Cliff’s Delta: \\(A = (\\delta + 1)/2\\).\nWilcoxon-Mann-Whitney Odds (wmw-odds): This non-parametric measure quantifies the odds that a randomly chosen observation from the first group (group1) is greater than a randomly chosen observation from the second group (group2).\n\\[\\psi = \\frac{P(X &gt; Y)}{P(Y &gt; X)} = \\frac{1 + \\delta}{1 - \\delta}\\]\nIt ranges from 0 to infinity. A value of 1 indicates stochastic equality. Values &gt; 1 mean group1 values tend to be larger; values &lt; 1 mean group1 values tend to be smaller. The natural logarithm of ψ is the log-odds that a random observation from group 1 is greater than a random observation from group 2.\n\nThese three measures (cliffs-delta, ameasure, wmw-odds) are non-parametric and robust, making them suitable for ordinal data or when assumptions of normality and equal variances are violated. They are inter-related and can be transformed into one another. Cliff’s Delta is centered around 0, A is centered around 0.5, and WMW Odds is centered around 1 (or 0 on a log scale), offering different interpretations of the effect magnitude and direction.\nLet’s illustrate these with the sepal lengths from the iris dataset, using ‘virginica’ as group 1 and ‘setosa’ as group 2.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/cliffs-delta virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.9692\n(stats/ameasure virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.9846\n(stats/wmw-odds virginica-sepal-length setosa-sepal-length) ;; =&gt; 63.93506493506461\n\n\n\n\n\nOverlap Family\nMeasures quantifying the degree to which two distributions share common values. These metrics assess how much the data from one group tends to blend or overlap with the data from another group. They provide an alternative perspective on effect size compared to mean differences or ratios, which is particularly useful when the focus is on the proportion of scores that are shared or distinct between groups.\n\n\n\n\n\n\nDefined functions\n\n\n\n\np-overlap\ncohens-u1-normal, cohens-u2-normal, cohens-u3-normal\ncohens-u1, cohens-u2, cohens-u3\n\n\n\nThese functions provide different ways to quantify overlap:\n\np-overlap: Estimates the proportion of overlap between two distributions based on Kernel Density Estimation (KDE). It calculates the area under the minimum of the two estimated density functions.\n\\[ \\text{Overlap} = \\int_{-\\infty}^{\\infty} \\min(f_1(x), f_2(x)) \\, dx \\]\nThis measure is non-parametric, meaning it doesn’t assume the data comes from a specific distribution (like normal), and it is symmetric (p-overlap(A, B) == p-overlap(B, A)). The result is a proportion between 0 (no overlap) and 1 (complete overlap, identical distributions).\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/p-overlap virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.12090979659086024\n(stats/p-overlap virginica-sepal-length setosa-sepal-length {:kde :epanechnikov, :bandwidth 0.7}) ;; =&gt; 0.15938870929802704\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCohen’s U measures (cohens-u1-normal, cohens-u2-normal, cohens-u3-normal): These are measures of non-overlap or overlap derived from Cohen’s d, assuming normal distributions with equal variances. They quantify the proportion of one group’s scores that fall beyond a certain point in relation to the other group.\n\ncohens-u1-normal: Quantifies the proportion of scores in the lower-scoring group that overlap with the scores in the higher-scoring group. Calculated from Cohen’s d (\\(d\\)) using the standard normal CDF (\\(\\Phi\\)) as \\((2\\Phi(|d|/2) - 1) / \\Phi(|d|/2)\\). Range is [0, 1]. 0 means no overlap, 1 means complete overlap.\ncohens-u2-normal: Quantifies the proportion of scores in the lower-scoring group that are below the point located halfway between the means of the two groups. Calculated as \\(\\Phi(|d|/2)\\). Range is [0, 1]. 0.5 means perfect overlap (medians are at the same point), 0 or 1 means no overlap (distributions are far apart).\ncohens-u3-normal: Quantifies the proportion of scores in the lower-scoring group that fall below the mean of the higher-scoring group. Calculated as \\(\\Phi(d)\\). Range is [0, 1]. This measure is asymmetric: U3(A, B) is the proportion of B below A’s mean; U3(B, A) is the proportion of A below B’s mean.\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/cohens-u1-normal virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.9339603379976781\n(stats/cohens-u2-normal virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.9380514024419309\n(stats/cohens-u3-normal virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.9989553620117301\n\n\n\n\n\n\n\nNon-parametric Cohen’s U measures (cohens-u1, cohens-u2, cohens-u3): These are analogous measures that do not assume normality or equal variances. They are based on comparing percentiles or medians of the empirical distributions.\n\ncohens-u1: Non-parametric measure of difference/separation, related to [cohens-u2]. Its value indicates greater separation as it increases towards +1, and more overlap as it approaches -1. Symmetric.\ncohens-u2: Quantifies the minimum overlap between corresponding quantiles. It finds the smallest distance between quantiles \\(q\\) and \\(1-q\\) of the two distributions across \\(q \\in [0.5, 1.0]\\). Range is typically [0, max_diff]. It is symmetric.\ncohens-u3: Quantifies the proportion of values in the second group (group2) that are less than the median of the first group (group1). This is a non-parametric version of the concept behind cohens-u3-normal, but is calculated directly from the empirical distributions’ medians and CDFs. Range is [0, 1]. This measure is also asymmetric.\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/cohens-u1 virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.9372815512903497\n(stats/cohens-u2 virginica-sepal-length setosa-sepal-length) ;; =&gt; 0.9409830056250525\n(stats/cohens-u3 virginica-sepal-length setosa-sepal-length) ;; =&gt; 1.0\n\n\n\nComparison:\n\nUse p-overlap for a robust, non-parametric, symmetric measure of direct area overlap based on estimated densities.\nUse cohens-u*-normal functions when you are comfortable assuming normality and equal variances, and want measures directly derived from Cohen’s d. u1, u2, and u3 offer slightly different views of overlap/separation relative to means and standard deviations.\nUse non-parametric cohens-u* functions when assumptions of normality are not met or when working with ordinal data. u2 is a symmetric overlap measure based on quantiles, while u3 is an asymmetric measure based on comparing to the median.\n\n\n\nCorrelation / Association Family\nEffect size measures in the correlation and association family quantify the strength and magnitude of the relationship between variables. Unlike difference-based measures (like Cohen’s d), these focus on how well one variable predicts or is associated with another, often in terms of shared or explained variance. They are widely used in regression, ANOVA, and general correlation analysis.\n\n\n\n\n\n\nDefined functions\n\n\n\n\npearson-r, r2-determination\neta-sq, omega-sq, epsilon-sq\ncohens-f2, cohens-f, cohens-q\nrank-eta-sq, rank-epsilon-sq\n\n\n\n\nPearson r (pearson-r): The Pearson product-moment correlation coefficient. An alias for [pearson-correlation]. Measures the strength and direction of a linear relationship between two continuous variables (\\(r\\)). Ranges from -1 to +1. \\[ r = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}} \\]\nR² Determination (r2-determination): The coefficient of determination. An alias for the standard [r2] calculation between two sequences. For a simple linear relationship, this is the square of the Pearson correlation (\\(R^2 = r^2\\)). It quantifies the proportion of the variance in one variable that is linearly predictable from the other. Ranges from 0 to 1. \\[ R^2 = 1 - \\frac{RSS}{TSS} = \\frac{SS_{regression}}{SS_{total}} \\]\nEta-squared (eta-sq): A measure of the proportion of the variance in a dependent variable that is associated with an independent variable. In the context of two numerical sequences, this function calculates \\(R^2\\) from the simple linear regression of group1 on group2. \\[ \\eta^2 = \\frac{SS_{regression}}{SS_{total}} \\]\nOmega-squared (omega-sq): A less biased estimate of the population Eta-squared (\\(\\omega^2\\)). It estimates the proportion of variance in the dependent variable accounted for by the independent variable in the population. Often preferred over Eta-squared as a population effect size estimate, especially for small sample sizes. \\[ \\omega^2 = \\frac{SS_{regression} - p \\cdot MSE}{SS_{total} + MSE} \\] where \\(p\\) is the number of predictors (1 for simple linear regression) and \\(MSE\\) is the Mean Squared Error of the residuals.\nEpsilon-squared (epsilon-sq): Another less biased estimate of population Eta-squared (\\(\\varepsilon^2\\)), similar to adjusted \\(R^2\\). Also aims to estimate the proportion of variance explained in the population. \\[ \\varepsilon^2 = \\frac{SS_{regression} - MSE}{SS_{total}} \\]\nCohen’s f² (cohens-f2): Measures the ratio of the variance explained by the effect to the unexplained variance. It can be calculated based on Eta-squared, Omega-squared, or Epsilon-squared (controlled by the :type parameter). Ranges from 0 upwards. \\[ f^2 = \\frac{\\text{Proportion of Variance Explained}}{\\text{Proportion of Variance Unexplained}} = \\frac{\\eta^2}{1-\\eta^2} \\]\nCohen’s f (cohens-f): The square root of Cohen’s f² (\\(f = \\sqrt{f^2}\\)). It quantifies the magnitude of the effect size in standardized units. Ranges from 0 upwards.\n\nLet’s look at the relationship between mpg (Miles Per Gallon) and hp (Horsepower) from the mtcars dataset. We expect a negative relationship (higher HP means lower MPG).\nThe Pearson r value shows a strong negative linear correlation (-0.77). Squaring this gives the R² (r2-determination, eta-sq), indicating that about 59% of the variance in MPG can be explained by the linear relationship with HP in this sample. omega-sq and epsilon-sq provide adjusted estimates for the population, which are slightly lower, as expected. Cohen’s f² and f quantify the magnitude of this effect, suggesting a large effect size according to conventional guidelines.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/pearson-r mpg hp) ;; =&gt; -0.7761683718265864\n(stats/r2-determination mpg hp) ;; =&gt; 0.602437341423934\n(stats/eta-sq mpg hp) ;; =&gt; 0.602437341423934\n(stats/omega-sq mpg hp) ;; =&gt; 0.5814794357913807\n(stats/epsilon-sq mpg hp) ;; =&gt; 0.5891852528047318\n(stats/cohens-f2 mpg hp) ;; =&gt; 1.515326775360793\n(stats/cohens-f2 mpg hp :omega) ;; =&gt; 1.3893688519007434\n(stats/cohens-f2 mpg hp :epsilon-sq) ;; =&gt; 1.515326775360793\n(stats/cohens-f mpg hp) ;; =&gt; 1.2309860987682977\n(stats/cohens-f mpg hp :omega) ;; =&gt; 1.178714915448491\n(stats/cohens-f mpg hp :epsilon-sq) ;; =&gt; 1.2309860987682977\n\n\n\n\nCohen’s q (cohens-q): Measures the difference between two correlation coefficients after applying the Fisher z-transformation (atanh). Useful for comparing the strength of correlation between different pairs of variables or in different samples. \\[ q = \\text{atanh}(r_1) - \\text{atanh}(r_2) \\]\n\nLet’s compare correlations using cohens-q. We’ll compare the correlation between mpg and hp against the correlation between mpg and wt (weight). Second test is to compare correlation between setosa sepal widths and lengths against correlation between virginical sepal widths and lengths.\n\n(def r-mpg-hp (stats/pearson-correlation mpg hp))\n\n\n(def r-mpg-wt (stats/pearson-correlation mpg wt))\n\n\n\n\n\n\n\nExamples\n\n\n\n\nr-mpg-hp ;; =&gt; -0.7761683718265864\nr-mpg-wt ;; =&gt; -0.8676593765172276\n(stats/cohens-q r-mpg-hp r-mpg-wt) ;; =&gt; 0.2878712810066375\n(stats/cohens-q mpg hp wt) ;; =&gt; 0.2878712810066375\n(stats/cohens-q setosa-sepal-width setosa-sepal-length virginica-sepal-width virginica-sepal-length) ;; =&gt; 0.4718354551725414\n\n\n\nThe cohens-q value quantifies the difference in the strength of these two (dependent) correlations. A larger absolute value of q suggests a more substantial difference between the correlation coefficients.\n\nRank Eta-squared (rank-eta-sq): Effect size measure for the Kruskal-Wallis test. It represents the proportion of variation in the dependent variable accounted for by group membership, based on ranks. Ranges from 0 to 1. Calculated based on the Kruskal-Wallis H statistic (\\(H\\)), number of groups (\\(k\\)), and total sample size (\\(n\\)). \\[ \\eta^2_H = \\frac{\\max(0, H - (k-1))}{n - k} \\]\nRank Epsilon-squared (rank-epsilon-sq): Another effect size measure for the Kruskal-Wallis test, also based on ranks and providing a less biased estimate than Rank Eta-squared. Ranges from 0 to 1. Calculated based on \\(H\\) and \\(n\\). \\[ \\varepsilon^2_H = \\frac{H}{n-1} \\]\n\nWe can treat the different species (setosa, virginica and versicolor) as groups and compare their sepal lengths using a non-parametric approach conceptually related to Kruskal-Wallis (though the function itself is a generic effect size calculation).\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/rank-eta-sq (vals sepal-lengths)) ;; =&gt; 0.6458328979635933\n(stats/rank-eta-sq (vals sepal-widths)) ;; =&gt; 0.41152809592198036\n(stats/rank-epsilon-sq (vals sepal-lengths)) ;; =&gt; 0.6505868187962968\n(stats/rank-epsilon-sq (vals sepal-widths)) ;; =&gt; 0.41942704765457123\n\n\n\nComparison:\n\npearson-r measures linear relationship direction and strength. r2-determination quantifies the proportion of shared variance (\\(r^2\\)) for a simple linear relationship.\neta-sq, omega-sq, and epsilon-sq all quantify the proportion of variance explained. eta-sq is a sample-based measure (equal to \\(R^2\\) here), while omega-sq and epsilon-sq are less biased population estimates, preferred when generalizing beyond the sample.\ncohens-f2 and cohens-f measure effect magnitude relative to unexplained variance. They are useful for power analysis and interpreting the practical significance of a regression or ANOVA effect. cohens-f is often more interpretable as it’s in the same units as standard deviations (though it’s not directly scaled by a single standard deviation like Cohen’s d).\ncohens-q is specifically for comparing correlation coefficients, assessing if the strength of relationship between two variables differs significantly from the strength between another pair or in another context.\nrank-eta-sq and rank-epsilon-sq are specific to rank-based tests like Kruskal-Wallis, providing effect sizes analogous to Eta-squared/Epsilon-squared but suitable for non-parametric data.\n\nInterpretation Guidelines:\n\nCorrelation (r):\n\n\\(|r| &lt; 0.3\\): weak/small linear relationship\n\\(0.3 \\le |r| &lt; 0.7\\): moderate/medium linear relationship\n\\(|r| \\ge 0.7\\): strong/large linear relationship\n\nProportion of Variance Explained (\\(R^2, \\eta^2, \\omega^2, \\varepsilon^2\\)):\n\n\\(0.01\\): small effect (1% of variance explained)\n\\(0.06\\): medium effect (6% of variance explained)\n\\(0.14\\): large effect (14% of variance explained)\n\nCohen’s f (and f²):\n\n\\(f = 0.1\\) (\\(f^2 = 0.01\\)): small effect\n\\(f = 0.25\\) (\\(f^2 \\approx 0.06\\)): medium effect\n\\(f = 0.4\\) (\\(f^2 = 0.16\\)): large effect",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#statistical-tests",
    "href": "stats.html#statistical-tests",
    "title": "Statistics",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nFunctions for hypothesis testing.\n\nNormality and Shape Tests\nThese tests assess whether a dataset deviates significantly from a normal distribution, or exhibits specific characteristics related to skewness (asymmetry) and kurtosis (tailedness). Deviations from normality are important to check as many statistical methods assume normal data.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nskewness-test\nkurtosis-test\nnormality-test\njarque-bera-test\nbonett-seier-test\n\n\n\nfastmath.stats provides several functions for these tests:\n\nskewness-test: Tests if the sample skewness significantly differs from the zero skewness expected of a normal distribution. It calculates a standardized test statistic (approximately Z) and its p-value. By default, it uses the :g1 type of skewness (Pearson’s moment coefficient).\nkurtosis-test: Tests if the sample kurtosis significantly differs from the value expected of a normal distribution (3 for raw kurtosis, 0 for excess kurtosis). It also calculates a standardized test statistic (approximately Z) and its p-value. By default, it uses the :kurt type of kurtosis (Excess Kurtosis + 3).\nnormality-test: The D’Agostino-Pearson K² test. This is an omnibus test that combines the skewness and kurtosis tests into a single statistic (\\(K^2 = Z_{skewness}^2 + Z_{kurtosis}^2\\)). \\(K^2\\) follows approximately a Chi-squared distribution with 2 degrees of freedom under the null hypothesis of normality. It tests for overall departure from normality. It internally uses the default :g1 skewness and default :kurt kurtosis types from the individual tests.\njarque-bera-test: Another omnibus test for normality based on sample skewness (\\(S\\), specifically type :g1) and excess kurtosis (\\(K\\), specifically type :g2). The test statistic is \\(JB = \\frac{n}{6}(S^2 + \\frac{1}{4}K^2)\\), which also follows approximately a Chi-squared distribution with 2 degrees of freedom under the null hypothesis. Similar to the K² test, it assesses whether the combined deviation in skewness and kurtosis is significant. Note that the specific :g1 and :g2 types are used for the JB statistic formula regardless of any :type option passed to the underlying skewness or kurtosis functions.\nbonett-seier-test: A test for normality based on Geary’s ‘g’ measure of kurtosis, which is more robust to outliers than standard moment-based kurtosis. It calculates a Z-statistic comparing the sample Geary’s ‘g’ to its expected value for a normal distribution (\\(\\sqrt{2/\\pi}\\)). This test specifically uses the :geary kurtosis type.\n\nInterpretation: The output of these functions is typically a map containing:\n\n:stat (or an alias like :Z, :K2, :JB, :F, :chi2): The calculated value of the test statistic.\n:p-value: The probability of observing a test statistic as extreme as, or more extreme than, the one calculated, assuming the null hypothesis (that the data comes from a normal distribution, or has the expected shape property) is true.\nOther keys might include :df (degrees of freedom), :skewness (the value of the type used in the test), :kurtosis (the value of the type used in the test), :n (sample size), and :sides (the alternative hypothesis used).\n\nA small p-value (typically less than the chosen significance level, e.g., 0.05) suggests that the observed data is unlikely to have come from a normal distribution (or satisfy the specific shape property being tested), leading to rejection of the null hypothesis. A large p-value indicates that there is not enough evidence to reject the null hypothesis.\nLet’s apply some of these tests to the residual-sugar data, which we observed earlier to be right-skewed. We’ll show the specific skewness/kurtosis values used by each test for clarity.\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/skewness residual-sugar :g1) ;; =&gt; 1.0767638711454521\n(stats/kurtosis residual-sugar :kurt) ;; =&gt; 6.465054296604846\n(stats/kurtosis residual-sugar :g2) ;; =&gt; 3.4650542966048463\n(stats/kurtosis residual-sugar :geary) ;; =&gt; 0.8336299967214688\n(stats/skewness-test residual-sugar) ;; =&gt; {:p-value 0.0, :Z 25.47884190247254, :skewness 1.0767638711454521}\n(stats/kurtosis-test residual-sugar) ;; =&gt; {:p-value 0.0, :Z 20.004147071664516, :kurtosis 6.465054296604846}\n(stats/normality-test residual-sugar) ;; =&gt; {:p-value 0.0, :Z 1049.3372847559745, :skewness 1.0767638711454521, :kurtosis 6.465054296604846}\n(stats/jarque-bera-test residual-sugar) ;; =&gt; {:p-value 0.0, :Z 3396.8207586928006, :skewness 1.0767638711454521, :kurtosis 3.4650542966048463}\n(stats/bonett-seier-test residual-sugar) ;; =&gt; {:p-value 1.2876717402423177E-30, :stat -11.50208472276902, :Z -11.50208472276902, :kurtosis 0.8336299967214688, :n 4898, :sides :two-sided}\n\n\n\nAs expected for the right-skewed residual-sugar data, the skewness-test, normality-test (K²), and jarque-bera-test yield very small p-values, strongly suggesting that the data is not normally distributed. The kurtosis-test and bonett-seier-test examine tailedness; their p-values indicate whether the deviation from normal kurtosis is significant.\n\n\nBinomial Tests\nThese tests are designed for data representing counts of successes in a fixed number of trials, where each trial has only two possible outcomes (success or failure). They are used to make inferences about the true underlying proportion of successes in the population.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nbinomial-test\nbinomial-ci\n\n\n\nbinomial-test: Performs an exact hypothesis test on a binomial proportion. This test is used to determine if the observed number of successes in a given number of trials is significantly different from what would be expected under a specific hypothesized probability of success (\\(p_0\\)). The null hypothesis is typically \\(H_0: p = p_0\\).\nThe test calculates a p-value based on the binomial probability distribution, assessing the likelihood of observing the sample results (or more extreme) if $p_0$ were the true population proportion.\nLet’s illustrate with some examples. Imagine we check the am column of the mtcars dataset, where 0 represents automatic and 1 represents manual transmission. We want to test if the proportion of cars with manual transmission is significantly different from 0.5 (an equal split). We also want to estimate a confidence interval for this proportion.\nLet’s get the counts:\n\n(def mtcars-am (ds/mtcars :am))\n\n\n(def manual-count (count (filter m/one? mtcars-am)))\n\n\n(def total-count (count mtcars-am))\n\n\n\n\n\n\n\nExamples\n\n\n\n\nmanual-count ;; =&gt; 13\ntotal-count ;; =&gt; 32\n(stats/mean mtcars-am) ;; =&gt; 0.40625\n\n\n\nNow, let’s perform the binomial test:\n\n(stats/binomial-test manual-count total-count)\n\n\n{:alpha 0.05,\n :ci-method :asymptotic,\n :confidence-interval [0.4008057508099545 0.4116942491900455],\n :estimate 0.40625,\n :level 0.95,\n :p 0.5,\n :p-value 0.3770855874754484,\n :stat 13,\n :successes 13,\n :test-type :two-sided,\n :trials 32}\n\n\n(stats/binomial-test manual-count total-count {:p 0.5, :sides :one-sided-greater})\n\n\n{:alpha 0.05,\n :ci-method :asymptotic,\n :confidence-interval [0.39533998822335753 1.0],\n :estimate 0.40625,\n :level 0.95,\n :p 0.5,\n :p-value 0.8923364251386376,\n :stat 13,\n :successes 13,\n :test-type :one-sided-greater,\n :trials 32}\n\n\n(stats/binomial-test manual-count total-count {:p 0.8, :sides :one-sided-less, :alpha 0.01})\n\n\n{:alpha 0.01,\n :ci-method :asymptotic,\n :confidence-interval [0.0 0.40842650129634167],\n :estimate 0.40625,\n :level 0.99,\n :p 0.8,\n :p-value 1.1904334239476455E-6,\n :stat 13,\n :successes 13,\n :test-type :one-sided-less,\n :trials 32}\n\nThe output map from binomial-test contains:\n\n:p-value: The probability of observing the sample result or more extreme outcomes if the true population proportion were equal to :p. A small p-value (typically &lt; 0.05) suggests evidence against the null hypothesis.\n:stat: The observed number of successes.\n:estimate: The observed proportion of successes (:successes / :trials).\n:confidence-interval: A confidence interval for the true population proportion, based on the observed data and using the specified :ci-method and :alpha.\n\n\nBinomial Confidence Intervals\nbinomial-ci: Calculates a confidence interval for a binomial proportion. Given the observed number of successes and trials, this function estimates a range of values that is likely to contain the true population probability of success (\\(p\\)) with a specified level of confidence.\nThe binomial-ci function offers various methods for calculating the confidence interval for a binomial proportion, controlled by the optional method keyword. These methods differ in their underlying assumptions and formulas, leading to intervals that can vary in width and coverage properties, particularly with small sample sizes or proportions close to 0 or 1.\nAvailable method values:\n\n:asymptotic: Normal Approximation (Wald) Interval. Based on the Central Limit Theorem, using the sample proportion and its estimated standard error. Simple to calculate but can have poor coverage (too narrow) for small sample sizes or proportions near 0 or 1.\n:agresti-coull: Agresti-Coull Interval. An adjusted Wald interval that adds ‘pseudo-counts’ (typically 2 successes and 2 failures) to the observed counts. This adjustment improves coverage and performance, especially for small samples.\n:clopper-pearson: Clopper-Pearson Interval. An ‘exact’ method based on inverting binomial tests. It provides guaranteed minimum coverage probability (i.e., the true proportion is included in the interval at least 100 * (1-alpha)% of the time). However, it is often wider than necessary and can be overly conservative.\n:wilson: Wilson Score Interval. Derived from the score test, which is based on the null hypothesis standard error rather than the observed standard error. It performs well across different sample sizes and probabilities and is generally recommended over the Wald interval.\n:prop.test: Continuity-Corrected Wald Interval. Applies a continuity correction to the Wald interval, similar to what is often used with the Chi-squared test for 2x2 tables.\n:cloglog: Complementary Log-log Transformation Interval. Calculates the interval on the clog-log scale and then transforms it back to the probability scale. Can be useful when dealing with skewed data or probabilities close to 0.\n:logit: Logit Transformation Interval. Calculates the interval on the logit scale (log(p/(1-p))) and transforms it back. Also useful for handling probabilities near boundaries.\n:probit: Probit Transformation Interval. Calculates the interval on the probit scale (inverse of the standard normal CDF) and transforms it back.\n:arcsine: Arcsine Transformation Interval. Calculates the interval using the arcsine square root transformation (asin(sqrt(p))) and transforms it back.\n:all: Calculates and returns a map of intervals from all the above methods.\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/binomial-ci manual-count total-count) ;; =&gt; [0.23608446629942265 0.5764155337005774 0.40625]\n(stats/binomial-ci manual-count total-count :asymptotic 0.01) ;; =&gt; [0.18261458036097775 0.6298854196390222 0.40625]\n(stats/binomial-ci manual-count total-count :asymptotic 0.1) ;; =&gt; [0.26344258236512375 0.5490574176348763 0.40625]\n(stats/binomial-ci manual-count total-count :agresti-coull) ;; =&gt; [0.25491682311999536 0.5776792765528302 0.40625]\n(stats/binomial-ci manual-count total-count :clopper-pearson) ;; =&gt; [0.23698410097201839 0.5935507534231756 0.40625]\n(stats/binomial-ci manual-count total-count :wilson) ;; =&gt; [0.25519634842665434 0.5773997512461713 0.40625]\n(stats/binomial-ci manual-count total-count :prop.test) ;; =&gt; [0.24219139863150996 0.5921456974454622 0.40625]\n(stats/binomial-ci manual-count total-count :cloglog) ;; =&gt; [0.238336877315159 0.5678979157795636 0.40625]\n(stats/binomial-ci manual-count total-count :logit) ;; =&gt; [0.25256981246106547 0.5807794598608678 0.40625]\n(stats/binomial-ci manual-count total-count :probit) ;; =&gt; [0.2495476615316991 0.5798499631463624 0.40625]\n(stats/binomial-ci manual-count total-count :arcsine) ;; =&gt; [0.2450397617966424 0.578602376146695 0.40625]\n\n\n\nFor methods other than :all, the function returns a vector [lower-bound, upper-bound, estimated-p]. * lower-bound, upper-bound: The ends of the calculated confidence interval. We are 100 * (1 - alpha)% confident that the true population proportion lies within this range. * estimated-p: The observed proportion of successes in the sample.\nIf the method is :all, a map is returned where keys are the method keywords and values are the [lower, upper, estimate] vectors for each method. Note that different methods can produce different interval widths and positions.\n\n(stats/binomial-ci manual-count total-count :all)\n\n\n{:agresti-coull [0.25491682311999536 0.5776792765528302 0.40625],\n :arcsine [0.2450397617966424 0.578602376146695 0.40625],\n :asymptotic [0.23608446629942265 0.5764155337005774 0.40625],\n :cloglog [0.238336877315159 0.5678979157795636 0.40625],\n :clopper-pearson [0.23698410097201839 0.5935507534231756 0.40625],\n :logit [0.25256981246106547 0.5807794598608678 0.40625],\n :probit [0.2495476615316991 0.5798499631463624 0.40625],\n :prop.test [0.24219139863150996 0.5921456974454622 0.40625],\n :wilson [0.25519634842665434 0.5773997512461713 0.40625]}\n\n\n\n\nLocation Tests (T/Z Tests)\nLocation tests, specifically t-tests and z-tests, are fundamental statistical tools used to compare means. They help determine if the mean of a sample is significantly different from a known or hypothesized value (one-sample tests), or if the means of two different samples are significantly different from each other (two-sample tests). The choice between a t-test and a z-test typically depends on whether the population standard deviation is known and the sample size.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nt-test-one-sample, z-test-one-sample\nt-test-two-samples, z-test-two-samples\np-value\n\n\n\nfastmath.stats provides the following functions for location tests:\n\nt-test-one-sample: Performs a one-sample Student’s t-test. This tests the null hypothesis that the true population mean (\\(\\mu\\)) is equal to a hypothesized value (\\(\\mu_0\\)). It is typically used when the population standard deviation is unknown and estimated from the sample. The test statistic follows a t-distribution with \\(n-1\\) degrees of freedom, where \\(n\\) is the sample size.\n\\[ t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\]\nz-test-one-sample: Performs a one-sample Z-test. This tests the null hypothesis that the true population mean (\\(\\mu\\)) is equal to a hypothesized value (\\(\\mu_0\\)). It is typically used when the population standard deviation is known or when the sample size is large (generally \\(n &gt; 30\\)), allowing the sample standard deviation to be used as a reliable estimate for the population standard deviation, and the test statistic approximates a standard normal distribution.\n\\[ z = \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} \\quad \\text{or} \\quad z \\approx \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\]\nt-test-two-samples: Performs a two-sample t-test to compare the means of two samples. It can perform:\n\nUnpaired tests: For independent samples. By default, it performs Welch’s t-test (:equal-variances? false), which does not assume equal population variances. It can also perform Student’s t-test (:equal-variances? true), assuming equal variances and using a pooled standard deviation.\nPaired tests: For dependent samples (:paired? true), essentially performing a one-sample t-test on the differences between pairs. The null hypothesis is typically that the true difference between population means is zero, or equal to a hypothesized value (\\(\\mu_0\\)). The test statistic follows a t-distribution with degrees of freedom calculated appropriately for the specific variant (pooled for Student’s, Satterthwaite approximation for Welch’s, \\(n-1\\) for paired).\n\nz-test-two-samples: Performs a two-sample Z-test to compare the means of two independent or paired samples. Similar to the one-sample Z-test, it is used when population variances are known or samples are large. It can handle independent samples (with or without assuming equal variances, affecting the standard error calculation) or paired samples (by performing a one-sample Z-test on differences). The test statistic approximates a standard normal distribution.\n\nComparison:\n\nT vs Z: Choose Z-tests primarily when population standard deviations are known or with large samples (often \\(n &gt; 30\\) for each group in two-sample tests), as the sampling distribution of the mean is well-approximated by the normal distribution. Use T-tests when population standard deviations are unknown and estimated from samples, especially with small to moderate sample sizes, as the sampling distribution is better described by the t-distribution.\nOne-Sample vs Two-Sample: Use one-sample tests to compare a single sample mean against a known or hypothesized constant. Use two-sample tests to compare the means of two distinct samples.\nPaired vs Unpaired (Two-Sample): Use paired tests when the two samples consist of paired observations (e.g., measurements before and after an intervention on the same subjects). Use unpaired tests for independent samples (e.g., comparing a treatment group to a control group with different subjects in each).\nWelch’s vs Student’s (Unpaired Two-Sample T-test): Welch’s test is generally recommended as it does not assume equal variances, a common violation in practice. Student’s t-test requires the assumption of equal population variances.\n\nAll these functions return a map containing key results, typically including the test statistic (:t or :z), :p-value, :confidence-interval for the mean (or mean difference), :estimate (sample mean or mean difference), sample size(s) (:n, :nx, :ny), :mu (hypothesized value), :stderr (standard error of the estimate), :alpha (significance level), and :sides (alternative hypothesis type). Two-sample tests also report :paired? and, if unpaired, :equal-variances?.\nArguments:\n\nxs (sequence of numbers): The sample data for one-sample tests or the first sample for two-sample tests.\nys (sequence of numbers): The second sample for two-sample tests.\nparams (map, optional): A map of options. Common keys:\n\n:alpha (double, default 0.05): Significance level. Confidence level is \\(1 - \\alpha\\).\n:sides (keyword, default :two-sided): Alternative hypothesis. Can be :two-sided, :one-sided-greater, or :one-sided-less.\n:mu (double, default 0.0): Hypothesized population mean (one-sample) or hypothesized difference in population means (two-sample).\n:paired? (boolean, default false): For two-sample tests, specifies if samples are paired.\n:equal-variances? (boolean, default false): For unpaired two-sample tests, specifies if equal population variances are assumed (Student’s vs Welch’s).\n\n\nReturn Value (Map):\n\n:stat (double): The calculated test statistic (alias :t or :z).\n:p-value (double): The probability of observing the test statistic or more extreme values under the null hypothesis.\n:confidence-interval (vector of doubles): [lower-bound, upper-bound]. A range likely containing the true population mean (one-sample) or mean difference (two-sample) with \\(1-\\alpha\\) confidence. Includes the estimate as a third value (e.g., [lower upper estimate]).\n:estimate (double): The sample mean (one-sample) or the difference between sample means (two-sample).\n:n (long or vector of longs): Sample size (one-sample) or sample sizes [nx ny] (two-sample).\n:nx, :ny (long): Sample sizes of xs and ys respectively (two-sample unpaired).\n:estimated-mu (vector of doubles): Sample means [mean xs, mean ys] (two-sample unpaired).\n:mu (double): The hypothesized value used in the null hypothesis.\n:stderr (double): The standard error of the sample mean or mean difference.\n:alpha (double): The significance level used.\n:sides / :test-type (keyword): The alternative hypothesis side.\n:df (long or vector of longs): Degrees of freedom (t-tests only).\n:paired? (boolean): Indicates if a paired test was performed (two-sample only).\n:equal-variances? (boolean): Indicates if equal variances were assumed (two-sample unpaired only).\n\nHelper function p-value:\nThe p-value function is a general utility used internally by many statistical tests (including t-tests and z-tests) to calculate the p-value from a given test statistic, its null distribution, and the specified alternative hypothesis (sides). It determines the probability of observing a statistic value as extreme as or more extreme than the one obtained from the sample, assuming the null hypothesis is true. The interpretation of ‘extreme’ depends on whether a two-sided, one-sided-greater, or one-sided-less test is performed. For discrete distributions, a continuity correction is applied.\nLet’s apply these tests. First, one-sample tests on mpg data against a hypothesized mean of 20.0:\n\n(stats/t-test-one-sample mpg {:mu 20.0})\n\n\n{:alpha 0.05,\n :confidence-interval [17.917678508746246 22.263571491253753],\n :df 31,\n :estimate 20.090625,\n :level 0.95,\n :mu 20.0,\n :n 32,\n :p-value 0.9327606409093825,\n :stat 0.08506003568133355,\n :stderr 1.0654239593728148,\n :t 0.08506003568133355,\n :test-type :two-sided}\n\n\n(stats/z-test-one-sample mpg {:mu 20.0, :sides :one-sided-greater})\n\n\n{:alpha 0.05,\n :confidence-interval [18.338158536184626 ##Inf],\n :estimate 20.090625,\n :level 0.95,\n :mu 20.0,\n :n 32,\n :p-value 0.4661068310107286,\n :stat 0.08506003568133355,\n :stderr 1.0654239593728148,\n :test-type :one-sided-greater,\n :z 0.08506003568133355}\n\nNow, two-sample tests comparing setosa-sepal-length and virginica-sepal-length. We’ll use unpaired tests first, comparing Welch’s and Student’s:\n\n(stats/t-test-two-samples setosa-sepal-length virginica-sepal-length)\n\n\n{:alpha 0.05,\n :confidence-interval [-1.7867603323573877 -1.3772396676426117],\n :df 76.51586702413655,\n :equal-variances? false,\n :estimate -1.5819999999999999,\n :estimated-mu [5.006 6.588],\n :level 0.95,\n :mu 0.0,\n :n [50 50],\n :nx 50,\n :ny 50,\n :p-value 3.966867270985708E-25,\n :paired? false,\n :sides :two-sided,\n :stat -15.386195820079422,\n :stderr 0.10281943753344443,\n :t -15.386195820079422,\n :test-type :two-sided}\n\n\n(stats/t-test-two-samples setosa-sepal-length virginica-sepal-length {:equal-variances? true})\n\n\n{:alpha 0.05,\n :confidence-interval [-1.786041827461154 -1.377958172538846],\n :df 98.0,\n :equal-variances? true,\n :estimate -1.5819999999999999,\n :estimated-mu [5.006 6.588],\n :level 0.95,\n :mu 0.0,\n :n [50 50],\n :nx 50,\n :ny 50,\n :p-value 6.892546060673613E-28,\n :paired? false,\n :sides :two-sided,\n :stat -15.386195820079422,\n :stderr 0.10281943753344443,\n :t -15.386195820079422,\n :test-type :two-sided}\n\nAnd the corresponding Z-tests (note the Z-test doesn’t need degrees of freedom):\n\n(stats/z-test-two-samples setosa-sepal-length virginica-sepal-length)\n\n\n{:alpha 0.05,\n :confidence-interval [-1.7835223944762166 -1.380477605523783],\n :equal-variances? false,\n :estimate -1.5819999999999999,\n :estimated-mu [5.006 6.588],\n :level 0.95,\n :mu 0.0,\n :n [50 50],\n :nx 50,\n :ny 50,\n :p-value 2.0259860303505476E-53,\n :paired? false,\n :sides :two-sided,\n :stat -15.386195820079422,\n :stderr 0.10281943753344443,\n :test-type :two-sided,\n :z -15.386195820079422}\n\n\n(stats/z-test-two-samples setosa-sepal-length virginica-sepal-length {:equal-variances? true})\n\n\n{:alpha 0.05,\n :confidence-interval [-1.7835223944762166 -1.380477605523783],\n :equal-variances? true,\n :estimate -1.5819999999999999,\n :estimated-mu [5.006 6.588],\n :level 0.95,\n :mu 0.0,\n :n [50 50],\n :nx 50,\n :ny 50,\n :p-value 2.0259860303505476E-53,\n :paired? false,\n :sides :two-sided,\n :stat -15.386195820079422,\n :stderr 0.10281943753344443,\n :test-type :two-sided,\n :z -15.386195820079422}\n\nIf we had paired data (e.g., hp measurements before and after a modification for the same cars, stored in hp-before and hp-after), we would use the paired option:\nAssume hp-before and hp-after are sequences of the same length\n\n(def hp-before [100 120 150])\n\n\n(def hp-after [110 125 165])\n\n\n(stats/t-test-two-samples hp-before hp-after {:paired? true})\n\n\n{:alpha 0.05,\n :confidence-interval [-22.420688558603995 2.4206885586039952],\n :df 2,\n :estimate -10.0,\n :level 0.95,\n :mu 0.0,\n :n 3,\n :p-value 0.07417990022744848,\n :paired? true,\n :stat -3.4641016151377544,\n :stderr 2.886751345948129,\n :t -3.4641016151377544,\n :test-type :two-sided}\n\n\n(stats/z-test-two-samples hp-before hp-after {:paired? true})\n\n\n{:alpha 0.05,\n :confidence-interval [-15.65792867038086 -4.3420713296191416],\n :estimate -10.0,\n :level 0.95,\n :mu 0.0,\n :n 3,\n :p-value 5.320055051392497E-4,\n :paired? true,\n :stat -3.4641016151377544,\n :stderr 2.886751345948129,\n :test-type :two-sided,\n :z -3.4641016151377544}\n\n\n\nVariance Tests\nVariance tests are used to assess whether the variances of two or more independent samples are statistically different. The null hypothesis for these tests is typically that the population variances are equal (homogeneity of variances, or homoscedasticity). This assumption is important for many statistical procedures, such as ANOVA and Student’s t-test.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nf-test\nlevene-test\nbrown-forsythe-test\nfligner-killeen-test\n\n\n\n\nf-test: Performs an F-test for the equality of variances of two independent samples.\nIt calculates the ratio of the sample variances (\\(s_1^2 / s_2^2\\)) which follows an F-distribution under the null hypothesis of equal population variances, assuming the data in both groups are normally distributed.\n\\[ F = \\frac{s_1^2}{s_2^2} \\]\nParameters:\n\nxs (seq of numbers): The first sample.\nys (seq of numbers): The second sample.\nparams (map, optional): Options map with :sides (default :two-sided) and :alpha (default 0.05).\n\nReturns a map with keys like :F (the F-statistic), :p-value, :df ([numerator-df, denominator-df]), :estimate (the variance ratio), :n ([nx, ny]), and :sides.\nAssumptions: Independent samples, normality within each sample. Sensitive to departures from normality.\nlevene-test: Performs Levene’s test for homogeneity of variances across two or more independent groups.\nThis test is more robust to departures from normality than the F-test. It performs a one-way ANOVA on the absolute deviations of each observation from its group’s mean.\n\\[ W = \\frac{N-k}{k-1} \\frac{\\sum_{i=1}^k n_i (\\bar{Z}_{i\\cdot} - \\bar{Z}_{\\cdot\\cdot})^2}{\\sum_{i=1}^k \\sum_{j=1}^{n_i} (Z_{ij} - \\bar{Z}_{i\\cdot})^2} \\]\nwhere \\(Z_{ij} = |X_{ij} - \\bar{X}_{i\\cdot}|\\) are the absolute deviations from the group means, \\(N\\) is total sample size, \\(k\\) is number of groups, \\(n_i\\) is size of group \\(i\\), \\(\\bar{Z}_{i\\cdot}\\) is mean of absolute deviations in group \\(i\\), and \\(\\bar{Z}_{\\cdot\\cdot}\\) is the grand mean of absolute deviations.\nParameters:\n\nxss (sequence of sequences): Collection of groups.\nparams (map, optional): Options map with :sides (default :one-sided-greater), :statistic (default [mean]), and :scorediff (default [[abs]]).\n\nReturns a map with keys like :W (the F-statistic for the ANOVA on deviations), :p-value, :df ([DFt, DFe]), :n (group sizes), and standard ANOVA output keys.\nAssumptions: Independent samples. Less sensitive to non-normality than F-test.\nbrown-forsythe-test: Performs the Brown-Forsythe test, a modification of Levene’s test using the median as the center.\nThis version is even more robust to non-normality than the standard Levene’s test. It performs an ANOVA on the absolute deviations from group medians.\n\\[ F^* = \\frac{N-k}{k-1} \\frac{\\sum_{i=1}^k n_i (\\tilde{Z}_{i\\cdot} - \\tilde{Z}_{\\cdot\\cdot})^2}{\\sum_{i=1}^k \\sum_{j=1}^{n_i} (Z_{ij} - \\tilde{Z}_{i\\cdot})^2} \\]\nwhere \\(Z_{ij} = |X_{ij} - \\tilde{X}_{i\\cdot}|\\) are the absolute deviations from the group medians, \\(\\tilde{Z}_{i\\cdot}\\) is the median of absolute deviations in group \\(i\\), and \\(\\tilde{Z}_{\\cdot\\cdot}\\) is the grand median of absolute deviations.\nParameters:\n\nxss (sequence of sequences): Collection of groups.\nparams (map, optional): Options map with :sides (default :one-sided-greater) and :scorediff (default [[abs]]). Internally sets :statistic to [median] in [levene-test].\n\nReturns a map similar to levene-test, with an F-statistic derived from the ANOVA on median deviations.\nAssumptions: Independent samples. Most robust to non-normality among parametric-based tests.\nfligner-killeen-test: Performs the Fligner-Killeen test for homogeneity of variances across two or more independent groups.\nThis is a non-parametric test based on ranks of the absolute deviations from group medians. It is generally considered one of the most robust tests for homogeneity of variances when assumptions about the underlying distribution are questionable.\nThe test statistic is based on the ranks of \\(|X_{ij} - \\tilde{X}_{i\\cdot}|\\), where \\(\\tilde{X}_{i\\cdot}\\) is the median of group \\(i\\).\nParameters:\n\nxss (sequence of sequences): Collection of groups.\nparams (map, optional): Options map with :sides (default :one-sided-greater).\n\nReturns a map with keys like :chi2 (the Chi-squared statistic), :p-value, :df (number of groups - 1), :n (group sizes), and standard ANOVA output keys (calculated on transformed ranks).\nAssumptions: Independent samples. Non-parametric, robust to non-normality. Requires distributions to have similar shape for valid inference.\n\nComparison: - The F-test is simple but highly sensitive to non-normality. Use cautiously if data isn’t clearly normal. - Levene’s and Brown-Forsythe tests are more robust to non-normality. Brown-Forsythe (median-based) is generally preferred over standard Levene’s (mean-based) when distributions are potentially heavy-tailed or skewed. - Fligner-Killeen is a non-parametric alternative and the most robust to non-normality, based on ranks.\nThe choice depends on sample size, suspected distribution shape, and the need for robustness.\nFunction-specific keys:\n\nf-test additionally returns:\n\n:F: The calculated F-statistic (ratio of sample variances, Var(xs) / Var(ys)). Alias for :stat.\n:nx, :ny: Sample sizes of the first (xs) and second (ys) groups.\n:estimate: The ratio of the sample variances (same value as :F).\n:confidence-interval: A confidence interval for the true ratio of population variances (Var(xs) / Var(ys)).\n\nlevene-test and brown-forsythe-test additionally return:\n\n:W: The calculated test statistic (which is an F-statistic derived from the ANOVA on deviations). Alias for :stat.\n:F: Alias for :W (as it’s an F-statistic).\n:SSt: Sum of Squares Treatment (between groups of deviations).\n:SSe: Sum of Squares Error (within groups of deviations).\n:DFt: Degrees of Freedom Treatment (number of groups - 1).\n:DFe: Degrees of Freedom Error (total sample size - number of groups).\n:MSt: Mean Square Treatment (SSt / DFt).\n:MSe: Mean Square Error (SSe / DFe).\n\nfligner-killeen-test additionally returns:\n\n:chi2: The calculated Chi-squared statistic. Alias for :stat.\n:SSt: Sum of Squares Treatment, calculated from transformed ranks.\n:SSe: Sum of Squares Error, calculated from transformed ranks.\n:DFt: Degrees of Freedom Treatment (number of groups - 1).\n:DFe: Degrees of Freedom Error (total sample size - number of groups).\n:MSt: Mean Square Treatment (SSt / DFt).\n:MSe: Mean Square Error (SSe / DFe).\n\n\nLet’s apply these tests to compare variances of sepal lengths for ‘setosa’ and ‘virginica’ species, and then across all three species (including ‘versicolor’ from the original sepal-lengths map).\n\n(stats/f-test setosa-sepal-length virginica-sepal-length)\n\n\n{:F 0.30728619882096414,\n :confidence-interval [0.1743775950172501 0.5414962165868265],\n :df [49 49],\n :estimate 0.30728619882096414,\n :n [50 50],\n :nx 50,\n :ny 50,\n :p-value 6.366473152645907E-5,\n :sides :two-sided,\n :stat 0.30728619882096414,\n :test-type :two-sided}\n\n\n(stats/levene-test (vals sepal-lengths))\n\n\n{:DFe 147,\n :DFt 2,\n :MSe 0.09376069877551022,\n :MSt 0.6920563200000005,\n :SSe 13.782822720000002,\n :SSt 1.384112640000001,\n :W 7.381091747801285,\n :df [2 147],\n :n (50 50 50),\n :p-value 8.817887814641656E-4,\n :stat 7.381091747801285}\n\n\n(stats/brown-forsythe-test (vals sepal-lengths))\n\n\n{:DFe 147,\n :DFt 2,\n :MSe 0.10096462585034013,\n :MSt 0.6414000000000003,\n :SSe 14.8418,\n :SSt 1.2828000000000006,\n :W 6.352720020482694,\n :df [2 147],\n :n (50 50 50),\n :p-value 0.0022585277836218998,\n :stat 6.352720020482694}\n\n\n(stats/fligner-killeen-test (vals sepal-lengths))\n\n\n{:DFe 147,\n :DFt 2,\n :MSe 0.3194726743934957,\n :MSt 1.9857373674434182,\n :SSe 46.96248313584387,\n :SSt 3.9714747348868364,\n :chi2 11.617980621101287,\n :df 2,\n :n (50 50 50),\n :p-value 0.0030004580742586384,\n :stat 11.617980621101287}\n\nWe can also compare sepal widths across species using these tests.\n\n(stats/f-test (sepal-widths :setosa) (sepal-widths :virginica))\n\n\n{:F 1.3959028295592801,\n :confidence-interval [0.7921415905767488 2.459843962499585],\n :df [49 49],\n :estimate 1.3959028295592801,\n :n [50 50],\n :nx 50,\n :ny 50,\n :p-value 0.24654059117021854,\n :sides :two-sided,\n :stat 1.3959028295592801,\n :test-type :two-sided}\n\n\n(stats/levene-test (vals sepal-widths))\n\n\n{:DFe 147,\n :DFt 2,\n :MSe 0.04547069387755103,\n :MSt 0.029199786666666727,\n :SSe 6.684192000000001,\n :SSt 0.058399573333333454,\n :W 0.642167166951519,\n :df [2 147],\n :n (50 50 50),\n :p-value 0.5276204473923196,\n :stat 0.642167166951519}\n\n\n(stats/brown-forsythe-test (vals sepal-widths))\n\n\n{:DFe 147,\n :DFt 2,\n :MSe 0.04818367346938773,\n :MSt 0.031199999999999922,\n :SSe 7.082999999999997,\n :SSt 0.062399999999999844,\n :W 0.6475222363405323,\n :df [2 147],\n :n (50 50 50),\n :p-value 0.5248269975064549,\n :stat 0.6475222363405323}\n\n\n(stats/fligner-killeen-test (vals sepal-widths))\n\n\n{:DFe 147,\n :DFt 2,\n :MSe 0.3425246920779396,\n :MSt 0.17247526965670978,\n :SSe 50.35112973545712,\n :SSt 0.34495053931341957,\n :chi2 1.0138383496145384,\n :df 2,\n :n (50 50 50),\n :p-value 0.6023484534454747,\n :stat 1.0138383496145384}\n\n\n\nGoodness-of-Fit and Independence Tests\nThese tests are used to evaluate how well an observed distribution of data matches a hypothesized theoretical distribution (Goodness-of-Fit) or whether there is a statistically significant association between two or more categorical variables (Independence). They are commonly applied to categorical data or data that has been grouped into categories (e.g., histograms).\n\n\n\n\n\n\nDefined functions\n\n\n\n\npower-divergence-test\nchisq-test\nmultinomial-likelihood-ratio-test\nminimum-discrimination-information-test\nneyman-modified-chisq-test\nfreeman-tukey-test\ncressie-read-test\nad-test-one-sample\nks-test-one-sample, ks-test-two-samples\n\n\n\nPower Divergence Tests:\nThe power-divergence-test is a generalized framework for several statistical tests that compare observed frequencies to expected frequencies. The specific test performed is determined by the lambda parameter.\nThe general test statistic is:\n\\[ 2 \\sum_i O_i \\left( \\frac{\\left(\\frac{O_i}{E_i}\\right)^\\lambda - 1}{\\lambda(\\lambda+1)} \\right) \\]\nwhere \\(O_i\\) are the observed counts, \\(E_i\\) are the expected counts, and \\(\\lambda\\) is the power parameter.\nThis test can be used for two main purposes:\n\nGoodness-of-Fit: Testing if a sequence of observed counts matches a set of expected counts (proportions/weights) or if a dataset matches a theoretical distribution.\nIndependence: Testing if there is an association between categorical variables in a contingency table.\n\nThe test statistic approximately follows a Chi-squared distribution with degrees of freedom determined by the specific application (e.g., number of categories - 1 - number of estimated parameters for GOF, \\((rows-1)(cols-1)\\) for independence).\nfastmath.stats provides the general power-divergence-test and aliases for common lambda values:\n\nchisq-test: Pearson’s Chi-squared test (\\(\\lambda=1\\)). The most common test in this family. \\[ \\chi^2 = \\sum_i \\frac{(O_i - E_i)^2}{E_i} \\]\nmultinomial-likelihood-ratio-test: G-test (\\(\\lambda=0\\)). Based on the ratio of likelihoods under the null and alternative hypotheses. \\[ G = 2 \\sum_i O_i \\ln\\left(\\frac{O_i}{E_i}\\right) \\]\nminimum-discrimination-information-test: Minimum Discrimination Information test (\\(\\lambda=-1\\)). Also known as the G-test on expected counts vs observed. \\[ I = 2 \\sum_i E_i \\ln\\left(\\frac{E_i}{O_i}\\right) \\]\nneyman-modified-chisq-test: Neyman Modified Chi-squared test (\\(\\lambda=-2\\)). \\[ NM = \\sum_i \\frac{(O_i - E_i)^2}{O_i} \\]\nfreeman-tukey-test: Freeman-Tukey test (\\(\\lambda=-0.5\\)). \\[ FT = \\sum_i (\\sqrt{O_i} - \\sqrt{E_i})^2 \\]\ncressie-read-test: Cressie-Read test (\\(\\lambda=2/3\\), default for power-divergence-test). A compromise test.\n\nArguments:\n\ncontingency-table-or-xs: For independence tests, a contingency table (sequence of sequences or map). For goodness-of-fit with counts, a sequence of observed counts. For goodness-of-fit with data, a sequence of raw data.\nparams (map, optional): Options including:\n\n:lambda (double): Power parameter (default 2/3).\n:p: For GOF, expected probabilities/weights (seq) or a fastmath.random distribution. Ignored for independence tests.\n:sides (keyword, default :one-sided-greater): For p-value calculation.\n:alpha (double, default 0.05): For confidence intervals.\n:ci-sides (keyword, default :two-sided): For confidence intervals.\n:bootstrap-samples (long, default 1000): For bootstrap CI.\n:ddof (long, default 0): Delta degrees of freedom subtracted from the calculated DF.\n:bins: For GOF with data vs distribution, histogram bins (see [histogram]).\n\n\nReturns: A map with keys:\n\n:stat (or :chi2): The calculated test statistic.\n:df: Degrees of freedom.\n:p-value: P-value.\n:n: Total number of observations.\n:estimate: Observed proportions.\n:expected: Expected counts or proportions.\n:confidence-interval: Bootstrap confidence intervals for observed proportions.\n:lambda, :alpha, :sides, :ci-sides: Input parameters.\n\nExamples:\nGoodness-of-Fit test comparing observed counts to expected proportions (e.g., testing if a six-sided die is fair based on 60 rolls).\n\n(def observed-rolls [10 12 8 11 9 10])\n\n\n(def expected-proportions [1 1 1 1 1 1])\n\nUsing weights proportional to expected, they will be normalized\n\n(stats/chisq-test observed-rolls {:p expected-proportions})\n\n\n{:alpha 0.05,\n :chi2 1.0000000000000009,\n :ci-sides :two-sided,\n :confidence-interval ([0.08333333333333333 0.26666666666666666]\n                       [0.11666666666666667 0.31666666666666665]\n                       [0.05 0.21666666666666667]\n                       [0.08333333333333333 0.2833333333333333]\n                       [0.06666666666666667 0.23333333333333334]\n                       [0.08333333333333333 0.26666666666666666]),\n :df 5,\n :estimate (0.16666666666666666\n            0.2\n            0.13333333333333333\n            0.18333333333333332\n            0.15\n            0.16666666666666666),\n :expected (10.0 10.0 10.0 10.0 10.0 10.0),\n :lambda 1.0,\n :level 0.95,\n :n 60.0,\n :p (0.16666666666666666\n     0.16666666666666666\n     0.16666666666666666\n     0.16666666666666666\n     0.16666666666666666\n     0.16666666666666666),\n :p-value 0.9625657732472963,\n :sides :one-sided-greater,\n :stat 1.0000000000000009,\n :test-type :one-sided-greater}\n\nGoodness-of-Fit test comparing sample data (mpg) to a theoretical distribution (Normal). This implicitly creates a histogram from mpg and compares its counts to the counts expected from a Normal distribution within those bins.\n\n(stats/chisq-test mpg {:p (r/distribution :normal {:mu (stats/mean mpg), :sd (stats/stddev mpg)})})\n\n\n{:alpha 0.05,\n :chi2 5.1376170090363775,\n :ci-sides :two-sided,\n :confidence-interval ([0.0625 0.34375]\n                       [0.21875 0.5625]\n                       [0.09375 0.40625]\n                       [0.0 0.15625]\n                       [0.03125 0.25]),\n :df 4,\n :estimate (0.1875 0.375 0.25 0.0625 0.125),\n :expected (6.522258911613709\n            8.862383891077252\n            9.18485015889101\n            5.339688018565394\n            2.090819019852639),\n :lambda 1.0,\n :level 0.95,\n :n 32.0,\n :p (0.2038205909879284\n     0.27694949659616414\n     0.28702656746534405\n     0.16686525058016857\n     0.06533809437039496),\n :p-value 0.27346634528388103,\n :sides :one-sided-greater,\n :stat 5.1376170090363775,\n :test-type :one-sided-greater}\n\nIndependence test using a contingency table (from the previous section).\n\n(stats/chisq-test ct-data-1)\n\n\n{:alpha 0.05,\n :chi2 1.7774150400145103,\n :ci-sides :two-sided,\n :confidence-interval {[0 0] [0.33 0.52],\n                       [0 1] [0.04 0.15],\n                       [1 0] [0.35000000000000003 0.5397500000000003],\n                       [1 1] [0.01 0.08]},\n :df 1,\n :estimate {[0 0] 0.43, [0 1] 0.09, [1 0] 0.44, [1 1] 0.04},\n :expected {[0 0] 45.24, [0 1] 6.76, [1 0] 41.76, [1 1] 6.24},\n :k 2,\n :lambda 1.0,\n :level 0.95,\n :n 100.0,\n :p-value 0.1824670652605519,\n :r 2,\n :sides :one-sided-greater,\n :stat 1.7774150400145103,\n :test-type :one-sided-greater}\n\n\n(stats/multinomial-likelihood-ratio-test ct-data-2)\n\n\n{:alpha 0.05,\n :chi2 11.250923175273865,\n :ci-sides :two-sided,\n :confidence-interval {[0 0] [0.165 0.28500000000000003],\n                       [0 1] [0.14 0.245],\n                       [0 2] [0.095 0.185],\n                       [1 0] [0.085 0.18],\n                       [1 1] [0.2 0.315],\n                       [1 2] [0.03 0.09]},\n :df 2,\n :estimate\n   {[0 0] 0.225, [0 1] 0.19, [0 2] 0.135, [1 0] 0.13, [1 1] 0.26, [1 2] 0.06},\n :expected\n   {[0 0] 39.05, [0 1] 49.5, [0 2] 21.45, [1 0] 31.95, [1 1] 40.5, [1 2] 17.55},\n :k 2,\n :lambda 0.0,\n :level 0.95,\n :n 200.0,\n :p-value 0.0036048987752140826,\n :r 3,\n :sides :one-sided-greater,\n :stat 11.250923175273865,\n :test-type :one-sided-greater}\n\nDistribution Comparison Tests (AD/KS):\nThese tests directly compare empirical cumulative distribution functions (ECDFs) or density estimates, providing non-parametric ways to assess goodness-of-fit or compare two samples without strong assumptions about the underlying distribution shape.\n\nad-test-one-sample: Anderson-Darling test. Primarily a Goodness-of-Fit test. It is particularly sensitive to differences in the tails of the distributions being compared. It tests if a sample xs comes from a specified distribution or an empirical distribution estimated from ys. \\[ A^2 = -n - \\sum_{i=1}^n \\frac{2i-1}{n} [\\ln(F(X_i)) + \\ln(1-F(X_{n-i+1}))] \\] where \\(X_i\\) are the ordered data, \\(n\\) is the sample size, and \\(F\\) is the CDF of the hypothesized distribution.\nArguments: xs (sample), distribution-or-ys (reference distribution or sample), opts (map: :sides, :kernel, :bandwidth).\nReturns: Map with :A2, :stat, :p-value, :n, :mean, :stddev, :sides.\nks-test-one-sample: One-sample Kolmogorov-Smirnov test. Compares the ECDF of a sample xs to a theoretical CDF or the ECDF of another sample ys. It is sensitive to the largest vertical difference between the two CDFs. \\[ D = \\max_i(|F_n(X_i) - F(X_i)|) \\] where \\(F_n\\) is the ECDF of the sample, \\(F\\) is the reference CDF, and \\(X_i\\) are the ordered data.\nArguments: xs (sample), distribution-or-ys (reference distribution or sample), opts (map: :sides, :kernel, :bandwidth, :distinct?).\nReturns: Map with :n, :dp, :dn, :d, :stat, :KS, :p-value, :sides.\nks-test-two-samples: Two-sample Kolmogorov-Smirnov test. Compares the ECDFs of two independent samples xs and ys. It is sensitive to the largest vertical difference between the two ECDFs. \\[ D_{n,m} = \\max_i(|F_n(X_i) - G_m(X_i)|) \\] where \\(F_n\\) and \\(G_m\\) are the ECDFs of the two samples.\nArguments: xs (sample 1), ys (sample 2), opts (map: :method (exact/approx), :sides, :distinct?, :correct?).\nReturns: Map with :nx, :ny, :n, :dp, :dn, :d, :stat, :KS, :p-value, :sides, :method.\n\nNotes:\n\nThe AD test is generally more sensitive to differences in the tails of the distributions than the KS test.\nThe KS test (both one-sample and two-sample) is based on the maximum difference between CDFs, making it sensitive to any point where the distributions diverge, but perhaps less so specifically in the extreme tails compared to AD.\nThe KS test has issues with discrete data and ties; the distinct? option attempts to mitigate this.\nThe Power Divergence tests are typically applied to counts or binned data, while AD/KS can be applied directly to continuous data (or compared against continuous distributions).\n\nHandling Ties in KS Tests:\nThe Kolmogorov-Smirnov (KS) test is theoretically defined for continuous distributions, where the probability of observing duplicate values (ties) is zero. In practice, real-world data often contains ties, which can affect the accuracy of the KS test, particularly for the exact p-value calculation methods. The ks-test-one-sample and ks-test-two-samples functions in fastmath.stats provide the :distinct? option to address this.\nThe :distinct? option controls how ties are handled:\n\n:ties (default): This is the default behavior. It keeps all observed data points, including ties. If the :method is :exact, information about the ties is passed to the underlying exact calculation algorithm to attempt a correction. The accuracy depends on the specific algorithm’s tie-handling capabilities.\ntrue: This applies the Clojure distinct function separately to each input sequence (xs and ys for the two-sample test) before combining and processing. This removes duplicate values within each sample but does not guarantee that ties between the samples are resolved or correctly handled for the exact method.\n:jitter: This adds a small amount of random noise to each data point to break all ties. This is a common pragmatic approach for dealing with ties in continuous data tests when an exact tie correction is unavailable or complex, but it slightly alters the original data.\nfalse: The data is used exactly as provided, without any specific handling or correction for ties.\n\nThe presence and handling of ties can significantly influence the calculated p-value, especially when using exact calculation methods. Choosing the appropriate method depends on the nature of the data and the required accuracy.\nExamples:\nAD test comparing setosa-sepal-length data to a normal distribution.\n\n(stats/ad-test-one-sample setosa-sepal-length (r/distribution :normal {:mu (stats/mean setosa-sepal-length), :sd (stats/stddev setosa-sepal-length)}))\n\n\n{:A2 0.407985975495866,\n :mean 5.006,\n :n 50,\n :p-value 0.8401982598779278,\n :sides :right,\n :stat 0.407985975495866,\n :stddev 0.3524896872134514}\n\nKS test comparing setosa-sepal-length data to an estimated empirical distribution build from normal samples (KDE based).\n\n(let [d (r/distribution :normal {:mu (stats/mean setosa-sepal-length), :sd (stats/stddev setosa-sepal-length)})] (stats/ks-test-one-sample setosa-sepal-length (r/-&gt;seq d 100) {:kernel :epanechnikov}))\n\n\n{:d 0.2169250973883241,\n :dn 0.15027029483877474,\n :dp 0.2169250973883241,\n :n 15,\n :p-value 0.42077821634367096,\n :sides :two-sided,\n :stat 0.2169250973883241}\n\nTwo-sample KS test comparing setosa-sepal-length and virginica-sepal-length.\n\n(stats/ks-test-two-samples setosa-sepal-length virginica-sepal-length)\n\n\n{:KS 0.9200000000000005,\n :d 0.9200000000000005,\n :dn 6.245004513516506E-17,\n :dp 0.9200000000000005,\n :method :exact,\n :n 100,\n :nx 50,\n :ny 50,\n :p-value 4.719076766639571E-23,\n :sides :two-sided,\n :stat 0.9200000000000005}\n\n\n\nANOVA and Rank Sum Tests\n;; Analysis of Variance (ANOVA) and rank sum tests are used to determine if there are statistically significant differences between the means (ANOVA) or distributions (rank sum tests) of two or more independent groups. One-way tests are used when comparing groups based on a single categorical independent variable.\n\n\n\n\n\n\nDefined functions\n\n\n\n\none-way-anova-test\nkruskal-test\n\n\n\n\nOne-way ANOVA Test (one-way-anova-test): A parametric test used to compare the means of two or more independent groups. It assesses whether the variation among group means is larger than the variation within groups.\n\nNull Hypothesis (\\(H_0\\)): The means of all groups are equal.\nAlternative Hypothesis (\\(H_1\\)): At least one group mean is different from the others.\nAssumptions: Independence of observations, normality within each group, and homogeneity of variances (equal variances across groups).\nTest Statistic (F-statistic): Calculated as the ratio of the variance between groups (Mean Square Treatment, \\(MSt\\)) to the variance within groups (Mean Square Error, \\(MSe\\)). \\[ F = \\frac{MSt}{MSe} \\] Where \\(MSt = SSt / DFt\\) and \\(MSe = SSe / DFe\\). \\(SSt\\) is the Sum of Squares Treatment (between groups), \\(SSe\\) is the Sum of Squares Error (within groups), \\(DFt\\) is the degrees of freedom for the treatment (number of groups - 1), and \\(DFe\\) is the degrees of freedom for the error (total observations - number of groups).\n\nKruskal-Wallis H-Test (kruskal-test): A non-parametric alternative to the one-way ANOVA. It is used to compare the distributions of two or more independent groups when the assumptions of ANOVA (especially normality) are not met. It tests whether the groups come from the same distribution.\n\nNull Hypothesis (\\(H_0\\)): The distributions of all groups are identical (specifically, that the median ranks are equal).\nAlternative Hypothesis (\\(H_1\\)): At least one group’s distribution is different from the others.\nAssumptions: Independence of observations. While it doesn’t assume normality, it assumes that the distributions have similar shapes if the intent is to compare medians.\nTest Statistic (H): Calculated based on the ranks of the combined data from all groups. The sum of ranks is computed for each group, and H is derived from deviations of these rank sums from what would be expected under the null hypothesis.\n\n\nComparison:\n\none-way-anova-test is a parametric test that compares means. It assumes normality and equal variances.\nkruskal-test is a non-parametric test that compares distributions (or median ranks). It does not assume normality or equal variances, making it suitable for ordinal data or data violating ANOVA assumptions.\n\nBoth functions accept a sequence of sequences (xss) where each inner sequence represents a group. Both return a map containing the test statistic, degrees of freedom, and p-value.\nArguments:\n\nxss (sequence of sequences of numbers): The groups of data to compare.\nparams (map, optional): An options map.\n\n:sides (keyword, default :one-sided-greater for both): Specifies the alternative hypothesis side for the test statistic’s distribution.\n\n\nReturn Value (Map):\n\n:stat (double): The calculated test statistic (F for ANOVA, H for Kruskal-Wallis).\n:p-value (double): The probability of observing the test statistic or more extreme values under the null hypothesis.\n:df (long or vector): Degrees of freedom. Vector [DFt, DFe] for ANOVA, scalar DFt for Kruskal-Wallis.\n:n (sequence of longs): Sample sizes of each group.\n:k (long): Number of groups (Kruskal-Wallis only).\n:sides (keyword): The alternative hypothesis side used.\nANOVA only: :F (alias for :stat), :SSt, :SSe, :DFt, :DFe, :MSt, :MSe.\nKruskal-Wallis only: :H (alias for :stat).\n\nLet’s compare the sepal lengths across the three iris species using both tests.\n\n(stats/one-way-anova-test (vals sepal-lengths))\n\n\n{:DFe 147,\n :DFt 2,\n :F 119.26450218450461,\n :MSe 0.2650081632653062,\n :MSt 31.606066666666663,\n :SSe 38.95620000000001,\n :SSt 63.21213333333333,\n :df [2 147],\n :n (50 50 50),\n :p-value 0.0,\n :stat 119.26450218450461}\n\n\n(stats/kruskal-test (vals sepal-lengths))\n\n\n{:df 2, :k 3, :n 150, :p-value 0.0, :sides :right, :stat 96.93743600064822}\n\n\n\nAutocorrelation Tests\nAutocorrelation tests examine whether values in a sequence are correlated with past values in the same sequence. This is particularly important when analyzing time series data or the residuals from a regression analysis, as autocorrelated residuals violate the assumption of independent errors, which can invalidate statistical inferences. fastmath.stats provides the Durbin-Watson test, a common method for detecting first-order (lag-1) autocorrelation in regression residuals.\n\n\n\n\n\n\nDefined functions\n\n\n\n\ndurbin-watson\n\n\n\n\nDurbin-Watson Test (durbin-watson): Calculates the Durbin-Watson statistic (d), which is used to test for the presence of serial correlation, especially first-order (lag-1) autocorrelation, in the residuals of a regression analysis.\nThe Durbin-Watson statistic is calculated as: \\[ d = \\frac{\\sum_{t=2}^T (e_t - e_{t-1})^2}{\\sum_{t=1}^T e_t^2} \\] where \\(e_t\\) are the residuals at time \\(t\\), and \\(T\\) is the total number of observations.\nThe value of the statistic \\(d\\) ranges from 0 to 4.\n\nValues near 2 suggest no first-order autocorrelation.\nValues less than 2 suggest positive autocorrelation (residuals tend to be followed by residuals of the same sign).\nValues greater than 2 suggest negative autocorrelation (residuals tend to be followed by residuals of the opposite sign).\n\nTesting for significance requires comparing the calculated statistic to lower (\\(d_L\\)) and upper (\\(d_U\\)) critical values from Durbin-Watson tables, which depend on the sample size and number of predictors in the regression model.\nParameters:\n\nrs (sequence of numbers): The sequence of residuals from a regression model. The sequence should represent observations ordered by time or sequence index.\n\nReturns the calculated Durbin-Watson statistic as a double.\nNote: This function only calculates the statistic. Determining statistical significance typically requires consulting Durbin-Watson critical value tables based on the sample size and the number of independent variables in the model.\nLet’s calculate the Durbin-Watson statistic for a few example sequences representing different scenarios: no autocorrelation, positive autocorrelation, and negative autocorrelation.\n\n\n(def residuals-no-autocorrelation (repeatedly 100 r/grand))\n\n\n(def residuals-positive-autocorrelation [2.0 1.8 1.5 1.0 -0.3 -0.8 -1.2 -1.5])\n\n\n(def residuals-negative-autocorrelation [1.0 -1.0 0.5 -0.5 0.2 -0.2 0.1 -0.1])\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/durbin-watson residuals-no-autocorrelation) ;; =&gt; 2.064286029995981\n(stats/durbin-watson residuals-positive-autocorrelation) ;; =&gt; 0.17236753856472167\n(stats/durbin-watson residuals-negative-autocorrelation) ;; =&gt; 3.0884615384615386",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#time-series-analysis",
    "href": "stats.html#time-series-analysis",
    "title": "Statistics",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\nFunctions specifically for analyzing sequential data, such as time series. These functions help identify patterns like trends, seasonality, and autocorrelation, which are crucial for understanding the underlying process generating the data and for building forecasting models (like ARIMA).\n\n\n\n\n\n\nDefined functions\n\n\n\n\nacf\npacf\nacf-ci, pacf-ci\n\n\n\nAutocorrelation and Partial Autocorrelation functions are fundamental tools for analyzing the dependence structure of a time series. They measure the correlation between a series and its lagged values.\n\nAutocorrelation Function (ACF) (acf): Measures the linear dependence between a time series and its lagged values. Specifically, the ACF at lag \\(k\\) is the correlation between the series and itself shifted by \\(k\\) time units. It captures both direct and indirect dependencies. For a stationary series, the sample ACF at lag \\(k\\) is estimated as: \\[ \\rho_k = \\frac{\\sum_{t=k+1}^n (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^n (y_t - \\bar{y})^2} \\] where \\(y_t\\) is the observation at time \\(t\\), \\(\\bar{y}\\) is the sample mean, and \\(n\\) is the series length.\nPartial Autocorrelation Function (PACF) (pacf): Measures the linear dependence between a time series and its lagged values after removing the linear dependence from the intermediate lags. The PACF at lag \\(k\\) is the correlation between \\(y_t\\) and \\(y_{t-k}\\), conditional on the intermediate observations \\(y_{t-1}, y_{t-2}, \\dots, y_{t-k+1}\\). It isolates the direct relationship at each lag. The PACF values (\\(\\phi_{kk}\\)) are the last coefficients in a sequence of autoregressive models of increasing order. For example, \\(\\phi_{11}\\) is the correlation at lag 1, \\(\\phi_{22}\\) is the correlation at lag 2 after accounting for lag 1, etc.\n\nComparison:\n\nThe ACF shows correlations at various lags, including those that are simply due to preceding correlations. It tends to decay gradually for autoregressive (AR) processes and cut off sharply for moving average (MA) processes.\nThe PACF shows only the direct correlation at each lag, after removing the effects of shorter lags. It tends to cut off sharply for AR processes and decay gradually for MA processes.\n\nThese patterns in ACF and PACF plots are diagnostic tools for identifying the order of AR or MA components in time series models (e.g., ARIMA).\nConfidence Intervals:\n\nacf-ci, pacf-ci: Calculate ACF and PACF values, respectively, and provide approximate confidence intervals around these estimates. These intervals help determine whether the autocorrelation/partial autocorrelation at a given lag is statistically significant (i.e., unlikely to be zero in the population). For a stationary series, the standard error of the sample ACF at lag \\(k\\) is approximately \\(1/\\sqrt{n}\\) for \\(k&gt;0\\). The PACF has a similar standard error for lags \\(k&gt;0\\). The confidence interval at level \\(\\alpha\\) is typically \\(\\pm z_{\\alpha/2} / \\sqrt{n}\\). acf-ci also provides cumulative confidence intervals (:cis) based on the variance of the sum of squared autocorrelations up to each lag.\n\nArguments:\n\ndata (seq of numbers): The time series data.\nlags (long or seq of longs, optional): The maximum lag to calculate ACF/PACF for (if a number), or a specific list of lags (for acf only). Defaults to (dec (count data)).\nalpha (double, optional, for *-ci functions): Significance level for the confidence intervals (default 0.05 for 95% CI).\n\nReturns:\n\nacf: A sequence of doubles representing the autocorrelation coefficients at lags 0 (always 1.0) up to lags (or specified lags).\npacf: A sequence of doubles representing the partial autocorrelation coefficients at lags 0 (always 0.0) up to lags.\nacf-ci, pacf-ci: A map containing:\n\n:ci (double): The value of the standard confidence interval (e.g., \\(1.96/\\sqrt{n}\\) for 95% CI).\n:acf or :pacf (seq of doubles): The calculated ACF or PACF values.\n:cis (seq of doubles, only for acf-ci): The cumulative confidence intervals for ACF.\n\n\nLet’s illustrate ACF and PACF with the white noise data (\\(sigma=0.5\\)) and AR, MA and ARFIMA processes.\n\n(def white-noise (take 1000 (r/white-noise (r/rng :mersenne 1) 0.5)))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/acf white-noise 10) ;; =&gt; (1.0 -0.018326052355220286 -0.003929181207704636 0.03404395796429751 -0.0437602077864766 -0.019833239371372824 -0.03199281424195406 0.01948375863906099 -0.0358413563927969 -0.03550751208554428 0.005884896971007567)\n(stats/pacf white-noise 10) ;; =&gt; (0.0 -0.018326052355220286 -0.004266458267873071 0.033905460948715424 -0.0425955681599929 -0.021160385359453224 -0.034290284810821074 0.021155121365213258 -0.03604909879044502 -0.0363697118356963 -4.2588688461055706E-4)\n(stats/acf-ci white-noise 10) ;; =&gt; {:ci 0.06197950323045615, :acf (1.0 -0.018326052355220286 -0.003929181207704636 0.03404395796429751 -0.0437602077864766 -0.019833239371372824 -0.03199281424195406 0.01948375863906099 -0.0358413563927969 -0.03550751208554428 0.005884896971007567), :cis (0.06197950323045615 0.06200031519261883 0.062001271732432243 0.06207303866741653 0.06219143491666359 0.062215727186766975 0.062278892765961415 0.06230230372268246 0.062381459961140466 0.06245905092154892 0.062461180879962185)}\n(stats/pacf-ci white-noise 10) ;; =&gt; {:ci 0.06197950323045615, :pacf (0.0 -0.018326052355220286 -0.004266458267873071 0.033905460948715424 -0.0425955681599929 -0.021160385359453224 -0.034290284810821074 0.021155121365213258 -0.03604909879044502 -0.0363697118356963 -4.2588688461055706E-4)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now for AR(30) time series.\n\n(def ar20 (take 10000 (drop 100 (r/ar (v/sq (m/slice-range 0.35 0.001 20)) white-noise))))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/acf ar20 10) ;; =&gt; (1.0 0.29824685915970284 0.2896474498894056 0.311904955709081 0.2564971222020911 0.28562296383470065 0.2594494703108183 0.2895861922578007 0.25082092948344153 0.2345937181515176 0.2551966287040645)\n(stats/pacf ar20 10) ;; =&gt; (0.0 0.29824685915970284 0.22029144703035636 0.2063218881353284 0.10446181152034764 0.13372974449872 0.08364335528216932 0.12094872497857509 0.053209972317661176 0.03814507902442836 0.061857342887178354)\n(stats/acf-ci ar20 10) ;; =&gt; {:ci 0.06683421715111557, :acf (1.0 0.29824685915970284 0.2896474498894056 0.311904955709081 0.2564971222020911 0.28562296383470065 0.2594494703108183 0.2895861922578007 0.25082092948344153 0.2345937181515176 0.2551966287040645), :cis (0.06683421715111557 0.07253598529450296 0.07753039023526297 0.08294616607495518 0.08641652954167832 0.09053521958042302 0.09379757073484547 0.09770956726933358 0.10054443827222243 0.10296037633006667 0.10574802260744404)}\n(stats/pacf-ci ar20 10) ;; =&gt; {:ci 0.06683421715111557, :pacf (0.0 0.29824685915970284 0.22029144703035636 0.2063218881353284 0.10446181152034764 0.13372974449872 0.08364335528216932 0.12094872497857509 0.053209972317661176 0.03814507902442836 0.061857342887178354)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMA(10) time series.\n\n(def ma10 (take 1000 (drop 100 (r/ma [0.1 0.1 0.1 2 1 0.1 0.1 0.1 -1 -2]))))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/acf ma10 10) ;; =&gt; (1.0 0.37147077233680736 -0.043765708819526204 0.023318822754153982 0.13860080451184031 -0.2678164189251497 -0.4100572205312395 -0.03777965531373422 0.0330279723161813 -0.07243034656845905 -0.1860297081683377)\n(stats/pacf ma10 10) ;; =&gt; (0.0 0.37147077233680736 -0.21085179552742053 0.14730099748790157 0.07513153363638297 -0.4368049671356323 -0.11121886566031113 0.18720400008394336 -0.17964413628139067 0.12403661223602583 -0.24381443651528892)\n(stats/acf-ci ma10 10) ;; =&gt; {:ci 0.06197950323045615, :acf (1.0 0.37147077233680736 -0.043765708819526204 0.023318822754153982 0.13860080451184031 -0.2678164189251497 -0.4100572205312395 -0.03777965531373422 0.0330279723161813 -0.07243034656845905 -0.1860297081683377), :cis (0.06197950323045615 0.07001163284832904 0.07011665195137436 0.07014643684284717 0.07119067955638286 0.07496115658384085 0.0831326287685858 0.08319855655252154 0.08324890809938708 0.08349063706391505 0.08506802930674574)}\n(stats/pacf-ci ma10 10) ;; =&gt; {:ci 0.06197950323045615, :pacf (0.0 0.37147077233680736 -0.21085179552742053 0.14730099748790157 0.07513153363638297 -0.4368049671356323 -0.11121886566031113 0.18720400008394336 -0.17964413628139067 0.12403661223602583 -0.24381443651528892)}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nARFIMA(3,0.1,3)\n\n(def arfima (take 1000 (drop 100 (r/arfima [0.1 0.1 -0.2] 0.1 [0.9 0.1 0.01]))))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/acf arfima 10) ;; =&gt; (1.0 0.6679521283341225 0.1665499902186834 -0.08863446891248286 -0.11623042955819972 -0.028619385233849097 0.05672348255511839 0.09759941293311915 0.0975477715018176 0.08072093524707452 0.04671941514558994)\n(stats/pacf arfima 10) ;; =&gt; (0.0 0.6679521283341225 -0.5048571403702115 0.19670224576297463 -0.08245081866106059 0.1050177599739776 -0.01819304597560102 0.06784449991745138 0.005683245143094844 0.04728276026389472 -0.03207213604598272)\n(stats/acf-ci arfima 10) ;; =&gt; {:ci 0.06197950323045615, :acf (1.0 0.6679521283341225 0.1665499902186834 -0.08863446891248286 -0.11623042955819972 -0.028619385233849097 0.05672348255511839 0.09759941293311915 0.0975477715018176 0.08072093524707452 0.04671941514558994), :cis (0.06197950323045615 0.08526001235655144 0.08650078257089443 0.08684896612567833 0.08744447032816513 0.08748044485635242 0.08762162079422252 0.08803824839087476 0.08845247595672698 0.08873500656865348 0.08882944851735629)}\n(stats/pacf-ci arfima 10) ;; =&gt; {:ci 0.06197950323045615, :pacf (0.0 0.6679521283341225 -0.5048571403702115 0.19670224576297463 -0.08245081866106059 0.1050177599739776 -0.01819304597560102 0.06784449991745138 0.005683245143094844 0.04728276026389472 -0.03207213604598272)}",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#histograms",
    "href": "stats.html#histograms",
    "title": "Statistics",
    "section": "Histograms",
    "text": "Histograms\nHistograms are fundamental graphical and statistical tools used to represent the distribution of numerical data. They group data into bins along the x-axis and display the frequency (count or proportion) of data points falling into each bin as bars on the y-axis. Histograms provide a visual summary of the shape, center, and spread of the data, highlighting features like modality, symmetry, and outliers.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nhistogram\nestimate-bins\n\n\n\nfastmath.stats provides functions to construct histograms and assist in choosing appropriate binning strategies:\n\nhistogram: Computes and returns the data structure representing a histogram. It takes the input data and parameters defining the bins (number, estimation method, or explicit intervals). It can also process collections of data sequences (for grouped histograms).\nestimate-bins: A utility function to recommend the number of bins for a given dataset based on various commonly used heuristic rules.\n\nThe histogram function calculates the counts of data points falling into predefined intervals (bins).\nParameters:\n\nvs (sequence of numbers or sequence of sequences): The input data. Can be a single sequence for a simple histogram or a collection of sequences for grouped histograms.\nbins-or-estimate-method (number, keyword, or sequence, optional): Defines the histogram bins.\n\nA number: The desired number of bins. The function calculates equally spaced intervals between the minimum and maximum values.\nA keyword (default: :freedman-diaconis): Uses a specific heuristic to estimate the number of bins (see [estimate-bins]).\nA sequence of numbers: Explicitly defines the bin edges (intervals). Data points are counted if they fall into [edge_i, edge_{i+1}).\nIf omitted, uses the default :freedman-diaconis estimation method.\n\nmn, mx (doubles, optional): Explicit minimum and maximum values to consider for binning. Data outside [mn, mx] are excluded. If omitted, the minimum and maximum of the data are used. When explicit bins-or-estimate-method (sequence of edges) is provided, mn and mx are inferred from the provided edges and vs data is filtered to fit.\n\nReturn Value (Map):\nReturns a map describing the histogram structure and counts. Key elements include:\n\n:size: The number of bins.\n:step: The average width of the bins (only applicable for equally spaced bins).\n:samples: The total number of data points included in the histogram (may be less than input if mn/mx are specified).\n:min, :max: The minimum and maximum data values used for binning.\n:intervals: The sequence of numbers defining the bin edges.\n:bins: A sequence of [lower-edge, count] pairs for each bin.\n:frequencies: A map where keys are the average value of each bin and values are the counts.\n:bins-maps: A sequence of detailed maps for each bin, including :min, :max, :step (bin width), :count, :avg (mean value within the bin), and :probability (count / total samples).\n\nIf the input vs is a sequence of sequences, the function returns a sequence of such maps, one for each inner sequence.\n\n(stats/histogram mpg)\n\n\n{:bins\n   ([10.4 6] [15.100000000000001 12] [19.8 8] [24.5 2] [29.200000000000003 4]),\n :bins-maps ({:avg 13.016666666666666,\n              :count 6,\n              :max 15.100000000000001,\n              :min 10.4,\n              :probability 0.1875,\n              :step 4.700000000000001}\n             {:avg 17.341666666666665,\n              :count 12,\n              :max 19.8,\n              :min 15.100000000000001,\n              :probability 0.375,\n              :step 4.699999999999999}\n             {:avg 22.0375,\n              :count 8,\n              :max 24.5,\n              :min 19.8,\n              :probability 0.25,\n              :step 4.699999999999999}\n             {:avg 26.65,\n              :count 2,\n              :max 29.200000000000003,\n              :min 24.5,\n              :probability 0.0625,\n              :step 4.700000000000003}\n             {:avg 31.775,\n              :count 4,\n              :max 33.9,\n              :min 29.200000000000003,\n              :probability 0.125,\n              :step 4.699999999999996}),\n :frequencies\n   {13.016666666666666 6, 17.341666666666665 12, 22.0375 8, 26.65 2, 31.775 4},\n :intervals (10.4 15.100000000000001 19.8 24.5 29.200000000000003 33.9),\n :max 33.9,\n :min 10.4,\n :samples 32,\n :size 5,\n :step 4.7}\n\n\n\n\nHistogram with 3 bins\n\n(stats/histogram mpg 3)\n\n\n{:bins ([10.4 14] [18.233333333333334 13] [26.066666666666666 5]),\n :bins-maps ({:avg 14.957142857142857,\n              :count 14,\n              :max 18.233333333333334,\n              :min 10.4,\n              :probability 0.4375,\n              :step 7.833333333333334}\n             {:avg 21.469230769230766,\n              :count 13,\n              :max 26.066666666666666,\n              :min 18.233333333333334,\n              :probability 0.40625,\n              :step 7.833333333333332}\n             {:avg 30.879999999999995,\n              :count 5,\n              :max 33.9,\n              :min 26.066666666666666,\n              :probability 0.15625,\n              :step 7.833333333333332}),\n :frequencies\n   {14.957142857142857 14, 21.469230769230766 13, 30.879999999999995 5},\n :intervals (10.4 18.233333333333334 26.066666666666666 33.9),\n :max 33.9,\n :min 10.4,\n :samples 32,\n :size 3,\n :step 7.833333333333333}\n\n\n\n\nHistogram with irregular bins\n\n(stats/histogram mpg [10 20 25 27 35])\n\n\n{:bins ([10.0 18] [20.0 8] [25.0 1] [27.0 5]),\n :bins-maps\n   ({:avg 15.899999999999999,\n     :count 18,\n     :max 20.0,\n     :min 10.0,\n     :probability 0.5625,\n     :step 10.0}\n    {:avg 22.0375, :count 8, :max 25.0, :min 20.0, :probability 0.25, :step 5.0}\n    {:avg 26.0, :count 1, :max 27.0, :min 25.0, :probability 0.03125, :step 2.0}\n    {:avg 30.879999999999995,\n     :count 5,\n     :max 35.0,\n     :min 27.0,\n     :probability 0.15625,\n     :step 8.0}),\n :frequencies {15.899999999999999 18, 22.0375 8, 26.0 1, 30.879999999999995 5},\n :intervals (10 20 25 27 35),\n :max 35.0,\n :min 10.0,\n :samples 32,\n :size 4,\n :step 6.25}\n\n\n\n\nHistogram with given min and max range\n\n\n\nThe estimate-bins function provides recommendations for the number of bins in a histogram based on common rules.\nParameters:\n\nvs (sequence of numbers): The input data.\nbins-or-estimate-method (keyword, optional): The rule to use for estimation.\n\nA keyword (default: :freedman-diaconis): Specifies the rule.\n\n:sqrt: Square root rule (simple). \\(k = \\lceil\\sqrt{n}\\rceil\\)\n:sturges: Sturges’ rule (assumes approximately normal data). \\(k = \\lceil\\log_2(n) + 1\\rceil\\)\n:rice: Rice rule. \\(k = \\lceil 2 n^{1/3} \\rceil\\)\n:doane: Doane’s rule (modification of Sturges’ for non-normal data). \\(k = \\lceil \\log_2(n) + 1 + \\log_2(1 + \\frac{|g_1|}{\\sigma_{g_1}}) \\rceil\\), where \\(g_1\\) is sample skewness and \\(\\sigma_{g_1}\\) is its standard error.\n:scott: Scott’s normal reference rule (bin width \\(h = 3.5 \\hat{\\sigma} n^{-1/3}\\)). \\(k = \\lceil (max - min)/h \\rceil\\)\n:freedman-diaconis (default): Freedman-Diaconis rule (robust to outliers, bin width \\(h = 2 \\cdot IQR \\cdot n^{-1/3}\\)). \\(k = \\lceil (max - min)/h \\rceil\\)\n\nA number: Returns the number itself (useful for passing a fixed value through).\nIf omitted, uses the default :freedman-diaconis rule.\n\n\nReturn Value (Long):\nReturns the estimated number of bins as a long integer. The returned value is constrained to be no greater than the number of samples.\n\nEstimate bins for alcohol data using different methods\n\n\n\n\n\n\nExamples\n\n\n\n\n(stats/estimate-bins alcohol) ;; =&gt; 28\n(stats/estimate-bins alcohol :sqrt) ;; =&gt; 69\n(stats/estimate-bins alcohol :sturges) ;; =&gt; 14\n(stats/estimate-bins alcohol :rice) ;; =&gt; 34\n(stats/estimate-bins alcohol :doane) ;; =&gt; 17\n(stats/estimate-bins alcohol :scott) ;; =&gt; 25\n(stats/estimate-bins alcohol 3) ;; =&gt; 3",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#bootstrap",
    "href": "stats.html#bootstrap",
    "title": "Statistics",
    "section": "Bootstrap",
    "text": "Bootstrap\nBootstrap is a widely used resampling technique in statistics. It involves repeatedly drawing samples with replacement from an original dataset (or simulating data from a model) to create many “bootstrap samples”. By analyzing the distribution of a statistic computed from each of these bootstrap samples, one can estimate the sampling distribution of the statistic, its standard error, bias, and construct confidence intervals without relying on strong parametric assumptions.\n\n\n\n\n\n\nDefined functions\n\n\n\n\nbootstrap\njackknife, jackknife+\nbootstrap-stats\nci-normal, ci-basic, ci-percentile, ci-bc, ci-bca, ci-studentized, ci-t\n\n\n\nThe core of the bootstrap process is generating the resamples. fastmath.stats provides the main bootstrap function for general resampling, and dedicated functions for specific resampling techniques like Jackknife.\n\nbootstrap: Generates bootstrap samples from data or a probabilistic model. It supports various sampling methods (standard resampling with replacement, Jackknife variants, or sampling from a distribution) and options like smoothing, antithetic sampling, and handling multidimensional data.\njackknife: Generates samples using the leave-one-out Jackknife method. For a dataset of size \\(n\\), it produces \\(n\\) samples, each by removing one observation from the original dataset. This is a less computationally intensive resampling method than standard bootstrap, primarily used for bias and variance estimation.\njackknife+: Generates samples using the positive Jackknife method. For a dataset of size \\(n\\), it produces \\(n\\) samples, each by adding one extra copy of an observation to the original dataset, resulting in samples of size \\(n+1\\).\n\nThe primary function for generating bootstrap samples. It’s flexible, supporting both nonparametric (resampling from data) and parametric (sampling from a model) approaches, and various options.\nParameters:\n\ninput (sequence or map): The data source.\n\nIf a sequence of data values (e.g., [1.2 3.4 5.0]), it’s treated as nonparametric input.\nIf a map {:data data-sequence :model model-object (optional)}, it supports parametric bootstrap. If :model is omitted, a distribution is automatically built from :data.\nCan be a sequence of sequences (e.g., [[1 2] [3 4]]) for multidimensional data when :dimensions is :multi.\n\nstatistic (function, optional): A function (fn [sample-sequence]) that calculates a statistic (e.g., fastmath.stats/mean). If provided, bootstrap-stats is automatically called on the results. If nil, the raw bootstrap samples are returned.\nparams (map, optional): Configuration options:\n\n:samples (long, default: 500): Number of bootstrap samples. Ignored for :jackknife/:jackknife+.\n:size (long, optional): Size of each sample. Defaults to original data size. Ignored for :jackknife/:jackknife+.\n:method (keyword, optional): Sampling method.\n\nnil (default): Standard resampling with replacement from data or model.\n:jackknife: Uses [jackknife].\n:jackknife+: Uses [jackknife+].\nOther keywords are passed to fastmath.random/-&gt;seq for sampling from a distribution model.\n\n:rng (random number generator, optional): fastmath.random RNG. Defaults to JVM RNG.\n:smoothing (keyword, optional): Applies smoothing. :kde for Kernel Density Estimation on the model; :gaussian adds noise to resampled values.\n:distribution (keyword, default :real-discrete-distribution): Type of distribution to build if :model is missing.\n:dimensions (keyword, optional): :multi for multidimensional data (sequence of sequences).\n:antithetic? (boolean, default false): Uses antithetic sampling (requires distribution model).\n:include? (boolean, default false): Includes original dataset as one sample.\n\n\nReturns:\n\nIf statistic is provided: A map including original input + analysis results from bootstrap-stats (e.g., :t0, :ts, :bias, :mean, :stddev).\nIf statistic is nil: A map including original input + :samples (a collection of bootstrap sample sequences).\n\nLet’s demonstrate various sampling methods for mpg data and see the outcome.\nFirst we’ll generate two samples using default (resampling with replacement) method without calculating statistics.\n\n(def rng (r/rng :mersenne 12345))\n\n\n(def boot-mpg (boot/bootstrap mpg {:samples 2 :rng rng}))\n\n\nboot-mpg\n\n\n{:data (21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26 30.4 15.8 19.7 15 21.4), :model #object[fastmath.random$eval76580$fn$reify__76594 0x4997319a \"fastmath.random$eval76580$fn$reify__76594@4997319a\"], :samples ((30.4 21.4 18.1 21.4 26.0 19.2 22.8 33.9 30.4 15.0 22.8 33.9 16.4 30.4 21.4 21.0 21.5 21.4 14.7 19.2 21.0 18.1 15.2 18.1 16.4 26.0 14.7 18.1 21.4 26.0 21.4 19.7) (17.3 10.4 10.4 15.8 15.2 32.4 21.0 18.7 21.0 22.8 10.4 22.8 22.8 15.8 13.3 18.1 22.8 21.4 26.0 24.4 15.2 22.8 16.4 17.8 22.8 33.9 14.7 21.5 27.3 15.8 27.3 15.5))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we’ll create set of samples using various bootstrapping methods: jackknife, jackknife+, gaussian and kde (epanechnikov kernel) smoothing, antithetic, systematic, stratified.\njackknife\nRemoves one selected data point from each sample.\n\n(def boot-mpg-jacknife (boot/bootstrap mpg {:method :jackknife}))\n\n\n(count (:samples boot-mpg-jacknife))\n\n\n32\n\n\n\n\n\n\n\n\n\n\n\n\n\njackknife+\nDuplicates one selected data point to each sample.\n\n(def boot-mpg-jacknife+ (boot/bootstrap mpg {:method :jackknife+}))\n\n\n(count (:samples boot-mpg-jacknife))\n\n\n32\n\n\n\n\n\n\n\n\n\n\n\n\n\ngaussian smoothing\nAdds random gaussian noise to bootstrapped samples.\n\n(def boot-mpg-gaussian (boot/bootstrap mpg {:samples 2 :rng rng :smoothing :gaussian}))\n\n\nboot-mpg-gaussian\n\n\n{:data (21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26 30.4 15.8 19.7 15 21.4), :model #object[fastmath.stats.bootstrap$build_model$reify__81181 0x6567bd65 \"fastmath.stats.bootstrap$build_model$reify__81181@6567bd65\"], :samples ((17.67627223078807 14.421273983904124 19.78342723622221 16.07408954450633 14.316546938062789 27.798521999070797 30.77113828285854 18.302867531466383 15.960192150840488 15.864337425252026 14.941708701743925 26.804072729895143 16.574983247993774 29.666351699518902 17.339228247855342 31.40604151247253 22.395701311599353 32.930311855428364 33.78970082261317 30.756294112465746 21.585229731828957 26.74839610035781 23.885829284382098 30.639999179485162 21.687174326300102 27.024298138739095 20.996894755344975 15.037358451484616 27.75666678457644 21.681540315448597 18.258506429956125 21.224613290413426) (15.557680619017212 21.857534902478296 17.173787452087744 21.772458916268835 30.79048018443092 14.401834047062994 29.91805443291403 23.84835433680409 14.97275468619625 15.831977361111361 12.75807146600363 29.478799801629 13.031673716237787 19.899937617191505 10.412081378944666 15.807822683372736 20.77838101998807 34.10293524709373 23.32318729591913 15.691254243065426 15.523902532414375 22.156319169598923 13.174824168708717 21.945995006952142 16.102210424539535 12.657658291816556 22.057155613451208 13.808960669738454 27.20472770459077 15.71216788846756 15.340973186948972 21.86783436275048))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nKDE (epanechnikov) smoothing\nBuilds KDE based continuous distributions and samples from it.\n\n(def boot-mpg-kde (boot/bootstrap mpg {:samples 2 :rng rng :smoothing :kde\n                                     :kernel :epanechnikov}))\n\n\nboot-mpg-kde\n\n\n{:data (21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26 30.4 15.8 19.7 15 21.4), :model #object[fastmath.random$eval76472$fn$reify__76487 0x6d9e541c \"fastmath.random$eval76472$fn$reify__76487@6d9e541c\"], :samples ((22.71166231862099 21.5726212037736 21.491485668355004 26.80401596363164 11.036961153662787 19.671900487652536 19.19056125766834 14.688160551611023 16.200514869948623 24.206628273571038 17.059352260277656 12.680303574909846 22.250524161111514 17.436331799861314 25.681192054352337 22.158874322682465 15.681506740965093 31.02194432899891 19.462270416888536 33.728606429171 20.47057953199578 18.524421058031663 14.958105453393623 14.69506556273741 17.17075042372946 28.295932367320248 13.829212961072399 24.535423656489346 27.104003147221192 18.25942212803992 27.780775341929104 17.920535668692203) (17.28666485268042 12.638080093377924 14.96857016839802 15.521948357649412 8.753906073248814 29.75592094916252 16.347734250920293 34.81462170357484 32.232667295931996 23.212608320127185 10.744533703230774 20.269444624032467 17.09041749905982 17.345419179032728 19.814015784485466 12.438328238728095 15.908886080593 16.16469970328886 20.98631911829065 13.02615677642237 26.615189109218168 12.421811034398033 14.480450305155927 17.91739680794446 16.56234614189687 20.673676874252102 22.65887627864257 28.21681519740815 11.93320510417053 20.01124443774085 18.687367694505223 14.7899861991578))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nantithetic\nSamples in pairs.\n\n(def boot-mpg-antithetic (boot/bootstrap mpg {:samples 2 :rng rng :antithetic? true}))\n\n\nboot-mpg-antithetic\n\n\n{:data (21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26 30.4 15.8 19.7 15 21.4), :model #object[fastmath.random$eval76580$fn$reify__76594 0xbeb638b \"fastmath.random$eval76580$fn$reify__76594@beb638b\"], :samples ((32.4 30.4 21.0 21.5 30.4 21.5 18.7 21.0 21.5 30.4 17.8 14.7 14.3 30.4 15.8 27.3 30.4 15.0 18.7 24.4 15.2 19.2 19.2 14.7 21.4 22.8 21.0 21.0 17.8 24.4 21.5 15.8) (10.4 13.3 18.1 15.8 14.3 15.8 19.7 17.8 15.8 13.3 21.0 27.3 30.4 13.3 21.5 14.7 13.3 26.0 19.7 15.2 24.4 19.2 19.2 27.3 16.4 15.5 18.1 17.8 21.0 15.2 15.8 21.5))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nsystematic\nWhen :systematic method is used, the result will be visible when model is continuous.\n\n(def boot-mpg-systematic (boot/bootstrap mpg {:samples 2 :rng rng :method :systematic\n                                            :smoothing :kde}))\n\n\nboot-mpg-systematic\n\n\n{:data (21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26 30.4 15.8 19.7 15 21.4), :model #object[fastmath.random$eval76472$fn$reify__76487 0x5aabaa1c \"fastmath.random$eval76472$fn$reify__76487@5aabaa1c\"], :samples ((7.606150009949184 10.075237326852191 11.413673809250726 12.393660938519448 13.192701804947857 13.883853741720861 14.504864771667009 15.077964794554122 15.617447530191576 16.133133483769825 16.632135165532496 17.11985976024962 17.600623861695897 18.078056714893737 18.555409167666742 19.035805185460838 19.52248025896614 20.019034758408264 20.529731011895603 21.059877586339276 21.61637474894962 22.208511540390997 22.849235227484154 23.55710632341578 24.359299899230063 25.29497771190241 26.413769034642186 27.750946296365644 29.27566787162567 30.9234351368731 32.76189217919069 35.34339715127126) (8.676493856283274 10.569043222796827 11.758115889203658 12.66764775786899 13.42609913122416 14.091376822965778 14.694919542040203 15.255831778482156 15.78668865312256 16.29628956322358 16.79110934287723 17.27614772688062 17.755459243241486 18.232521242300177 18.710516956755633 19.19258011239735 19.68203595423166 20.182661491418965 20.69901308885707 21.23684228788875 21.803719707918162 22.409951783548575 23.07004582120764 23.804977419951637 24.645471268906874 25.634844499022098 26.822677828928917 28.227959649536935 29.79700481962044 31.487573747147117 33.45546155582857 36.89510371835037))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nstratified\nSimilarly to a :systematic, we should sample from conitinuous distribution.\n\n(def boot-mpg-stratified (boot/bootstrap mpg {:samples 2 :rng rng :method :stratified\n                                            :smoothing :kde}))\n\n\nboot-mpg-stratified\n\n\n{:data (21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26 30.4 15.8 19.7 15 21.4), :model #object[fastmath.random$eval76472$fn$reify__76487 0x4e2f94cb \"fastmath.random$eval76472$fn$reify__76487@4e2f94cb\"], :samples ((7.246091436112749 9.941275520026366 11.32406615601483 12.32370249682059 13.133759200629084 13.831823291372558 14.45745740742365 15.03376742295543 15.575522401157436 16.092810948228664 16.59292399884376 17.081376183935813 17.56255414339351 18.04012960877534 18.51737397306466 18.99741250071758 19.483462705173242 19.97908634934658 20.48847962773443 21.016853572877427 21.570950008680434 22.159839800707662 22.796108151907163 23.497783186703003 24.291216370254393 25.21453611919852 26.316973452729822 27.636791537277393 29.149395484719662 30.78779806206383 32.6021090153638 35.06525265475502) (7.015778582858773 9.864058413269005 11.273191811589978 12.284239339511783 13.100626827648787 13.802643523386974 14.43091808088285 15.00905552836559 15.552100855151096 16.070304383761837 16.57105020709445 17.059919878505468 17.541338233013303 18.01900226855315 18.49619503554834 18.97604305379609 19.461755664336977 19.956872139879 20.465554298720164 20.992957541911824 21.54574327611805 22.13285599843595 22.766691619155246 23.4649857783239 24.253644833601463 25.170221950551866 26.263670285499295 27.573746699267982 29.079371999052828 30.712664195652117 32.51454108242075 34.92000288227524))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nAutomatically created model is in most cases a distribution object and it can be used to generate more data.\n\n(r/-&gt;seq (:model boot-mpg) 10)\n\n\n(27.3 18.7 15.5 22.8 10.4 33.9 14.3 30.4 15.8 30.4)\n\n\n(r/-&gt;seq (:model boot-mpg) 10 :stratified)\n\n\n(10.4 14.3 15.2 15.8 17.8 19.2 21.0 21.5 24.4 30.4)\n\nLet’s extract median\n\n(r/icdf (:model boot-mpg) 0.5)\n\n\n19.2\n\nModel based sampling\nNow let’s simulate our data when model is given. Let’s assume that mpg follows normal distribution. So instead of resampling we will draw samples from fitted normal distribution.\n\n(def boot-mpg-model (boot/bootstrap {:data mpg\n                                   :model (r/distribution :normal {:mu (stats/mean mpg)\n                                                                   :sd (stats/stddev mpg)})}\n                                  stats/mean\n                                  {:samples 10000 :rng rng}))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-numerical data\nBootstrap can be also called on categorical data.\n\n(boot/bootstrap [:cat :dog :cat :dog :dog] {:samples 2, :size 20})\n\n\n{:data [:cat :dog :cat :dog :dog],\n :model\n   #object[fastmath.random$eval76605$fn$reify__76609 0x11e9b525 \"fastmath.random$eval76605$fn$reify__76609@11e9b525\"],\n :samples ((:dog :cat\n                 :cat :cat\n                 :dog :cat\n                 :dog :dog\n                 :cat :dog\n                 :cat :cat\n                 :cat :dog\n                 :cat :dog\n                 :dog :cat\n                 :cat :cat)\n            (:dog :dog\n                  :dog :cat\n                  :dog :dog\n                  :dog :cat\n                  :dog :dog\n                  :cat :dog\n                  :dog :dog\n                  :dog :cat\n                  :cat :dog\n                  :cat :cat))}\n\n\nStatistics\nOnce bootstrap samples are generated, one typically applies the statistic of interest to each sample to get the bootstrap distribution of the statistic.\n\nbootstrap-stats: Computes a specified statistic on the original data (t0) and on each bootstrap sample (ts), and calculates descriptive statistics (mean, median, variance, stddev, SEM, bias) for the distribution of ts. This function is called automatically by bootstrap when a statistic function is provided.\n\nParameters:\n\nboot-data (map): A map containing :data (original data) and :samples (collection of bootstrap samples).\nstatistic (function): The function (fn [sample-sequence]) to apply to the data and samples.\n\nReturns: The input boot-data map augmented with results:\n\n:statistic - statistic calculation function\n:t0 - statistic for original data\n:ts - statistics for all bootstrapped samples\n:bias - difference between t0 and mean of ts\n:mean - mean of ts\n:median - median of ts\n:variance - variance of ts\n:stddev - standard deviation of ts\n:sem - standard error of mean of ts\n\nExample (usually called internally by bootstrap):\n\n(def raw-boot-samples (boot/bootstrap mpg {:samples 1000}))\n\n\n(def analyzed-boot-data (boot/bootstrap-stats raw-boot-samples stats/mean))\n\n\n(dissoc analyzed-boot-data :samples :ts)\n\n\n{:bias -0.005496874999998624,\n :data (21\n        21\n        22.8\n        21.4\n        18.7\n        18.1\n        14.3\n        24.4\n        22.8\n        19.2\n        17.8\n        16.4\n        17.3\n        15.2\n        10.4\n        10.4\n        14.7\n        32.4\n        30.4\n        33.9\n        21.5\n        15.5\n        15.2\n        13.3\n        19.2\n        27.3\n        26\n        30.4\n        15.8\n        19.7\n        15\n        21.4),\n :mean 20.085128125,\n :median 20.075,\n :model\n   #object[fastmath.random$eval76580$fn$reify__76594 0x7d1506ac \"fastmath.random$eval76580$fn$reify__76594@7d1506ac\"],\n :sem 0.18332922671855734,\n :statistic #&lt;Fn@21ad8741 fastmath.stats/mean&gt;,\n :stddev 1.0370667152190232,\n :t0 20.090625,\n :variance 1.0755073718151746}\n\nNow we’ll see bootstrapped ts for different types of sampling methods and sample size = 10000. Vertical line marks t0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent statistic\nWe can also estimate median instead of mean from smoothed samples.\n\n(def boot-mpg-median (boot/bootstrap mpg stats/median {:samples 10000 :smoothing :gaussian :rng rng}))\n\n\n\n\n\n\nCI\nBootstrap confidence intervals are constructed from the distribution of the statistic calculated on the bootstrap samples (:ts). fastmath.stats.bootstrap provides several methods, varying in complexity and robustness. All CI functions take the results map (typically from bootstrap-stats) and an optional significance level alpha (default 0.05 for 95% CI). They return a vector [lower-bound, upper-bound, t0].\n\nci-normal: Normal approximation interval. Assumes the distribution of ts is normal.\nci-basic: Basic (Percentile-t) interval.\nci-percentile: Percentile interval. The simplest method, directly using the quantiles of the bootstrap distribution of the statistic.\nci-bc: Bias-Corrected (BC) interval. Adjusts the Percentile interval by correcting for median bias in the bootstrap distribution.\nci-bca: Bias-Corrected and Accelerated (BCa) interval. Corrects for both bias (\\(z_0\\)) and skewness/non-constant variance (acceleration factor \\(a\\)). This is often considered the most accurate method but is more complex. Requires original :data and :statistic in the input map to calculate \\(a\\) via Jackknife, or it estimates \\(a\\) from ts skewness if data/statistic are missing.\nci-studentized: Studentized (Bootstrap-t) interval.\nci-t: T-distribution based interval.\n\nParameters (common to most CI functions):\n\nboot-data (map): Bootstrap analysis results, typically from [bootstrap-stats]. Must contain :t0 and :ts. Some methods (ci-bca, ci-studentized) require additional keys like :data, :samples, :statistic.\nalpha (double, optional): Significance level. Default is 0.05 (for 95% CI).\nestimation-strategy (keyword, optional, for ci-basic, ci-percentile, ci-bc, ci-bca): Quantile estimation strategy for :s. Defaults to :legacy. See [quantiles].\n\nReturns (common): A vector [lower-bound, upper-bound, t0].\nLet’s calculate various confidence intervals for the mean of mpg using the boot-results map obtained earlier (which includes :t0, :ts, etc.):\n\n(def boot-results (boot/bootstrap mpg stats/mean {:samples 500 :rng rng}))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(:t0 boot-results) ;; =&gt; 20.090625\n(boot/ci-normal boot-results) ;; =&gt; [18.029228142021623 22.207409357978378 20.090625]\n(boot/ci-basic boot-results) ;; =&gt; [17.969375 22.1184375 20.090625]\n(boot/ci-percentile boot-results) ;; =&gt; [18.0628125 22.211875 20.090625]\n(boot/ci-bc boot-results) ;; =&gt; [18.077650533275566 22.225951643655776 20.090625]\n(boot/ci-bca boot-results) ;; =&gt; [18.166803016582605 22.32402694368883 20.090625]\n(boot/ci-studentized boot-results) ;; =&gt; [18.114093259364665 22.75279971068179 20.090625]\n(boot/ci-t boot-results) ;; =&gt; [17.99645503084558 22.184794969154417 20.090625]",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "stats.html#reference",
    "href": "stats.html#reference",
    "title": "Statistics",
    "section": "Reference",
    "text": "Reference\n\nfastmath.stats\nNamespace provides a comprehensive collection of functions for performing statistical analysis in Clojure. It focuses on providing efficient implementations for common statistical tasks, leveraging fastmath’s underlying numerical capabilities.\nThis namespace covers a wide range of statistical methods, including:\n\nDescriptive Statistics: Measures of central tendency (mean, median, mode, expectile), dispersion (variance, standard deviation, MAD, SEM), and shape (skewness, kurtosis, L-moments).\nQuantiles and Percentiles: Functions for calculating percentiles, quantiles, and the median, including weighted versions and various estimation strategies.\nIntervals and Extents: Methods for defining ranges within data, such as span, IQR, standard deviation/MAD/SEM extents, percentile/quantile intervals, prediction intervals (PI, HPDI), and fence boundaries for outlier detection.\nOutlier Detection: Functions for identifying data points outside conventional fence boundaries.\nData Transformation: Utilities for scaling, centering, trimming, winsorizing, and applying power transformations (Box-Cox, Yeo-Johnson) to data.\nCorrelation and Covariance: Measures of the linear and monotonic relationship between two or more variables (Pearson, Spearman, Kendall), and functions for generating covariance and correlation matrices.\nDistance and Similarity Metrics: Functions for quantifying differences or likeness between data sequences or distributions, including error metrics (MAE, MSE, RMSE), L-p norms, and various distribution dissimilarity/similarity measures.\nContingency Tables: Functions for creating, analyzing, and deriving measures of association and agreement (Cramer’s V, Cohen’s Kappa) from contingency tables, including specialized functions for 2x2 tables.\nBinary Classification Metrics: Functions for generating confusion matrices and calculating a wide array of performance metrics (Accuracy, Precision, Recall, F1, MCC, etc.).\nEffect Size: Measures quantifying the magnitude of statistical effects, including difference-based (Cohen’s d, Hedges’ g, Glass’s delta), ratio-based, ordinal/non-parametric (Cliff’s Delta, Vargha-Delaney A), and overlap-based (Cohen’s U, p-overlap), as well as measures related to explained variance (Eta-squared, Omega-squared, Cohen’s f²).\nStatistical Tests: Functions for performing hypothesis tests, including:\n\nNormality and Shape tests (Skewness, Kurtosis, D’Agostino-Pearson K², Jarque-Bera, Bonett-Seier).\nBinomial tests and confidence intervals.\nLocation tests (one-sample and two-sample T/Z tests, paired/unpaired).\nVariance tests (F-test, Levene’s, Brown-Forsythe, Fligner-Killeen).\nGoodness-of-Fit and Independence tests (Power Divergence family including Chi-squared, G-test; AD/KS tests).\nANOVA and Rank Sum tests (One-way ANOVA, Kruskal-Wallis).\nAutocorrelation tests (Durbin-Watson).\n\nTime Series Analysis: Functions for analyzing the dependence structure of time series data, such as Autocorrelation (ACF) and Partial Autocorrelation (PACF).\nHistograms: Functions for computing histograms and estimating optimal binning strategies.\n\nThis namespace aims to provide a robust set of statistical tools for data analysis and modeling within the Clojure ecosystem.\n\n\n-&gt;confusion-matrix DEPRECATED\nDeprecated: Use confusion-matrix\nsource\n\n\n\nL0\nCount equal values in both seqs. Alias for count==\nsource\n\n\n\nL1\n\n(L1 [vs1 vs2-or-val])\n(L1 vs1 vs2-or-val)\n\nCalculates the L1 distance (Manhattan or City Block distance) between two sequences or a sequence and a constant value.\nThe L1 distance is the sum of the absolute differences between corresponding elements.\nParameters:\n\nvs1 (sequence of numbers): The first sequence.\nvs2-or-val (sequence of numbers or single number): The second sequence of numbers, or a single number to compare against each element of vs1.\n\nIf both inputs are sequences, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the calculated L1 distance as a double.\nSee also L2, L2sq, LInf, mae (Mean Absolute Error).\nsource\n\n\n\nL2\n\n(L2 [vs1 vs2-or-val])\n(L2 vs1 vs2-or-val)\n\nCalculates the L2 distance (Euclidean distance) between two sequences or a sequence and a constant value.\nThis is the standard straight-line distance between two points (vectors) in Euclidean space. It is the square root of the L2sq distance.\nParameters:\n\nvs1 (sequence of numbers): The first sequence.\nvs2-or-val (sequence of numbers or single number): The second sequence of numbers, or a single number to compare against each element of vs1.\n\nIf both inputs are sequences, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the calculated L2 distance as a double.\nSee also L1, L2sq, LInf, rmse (Root Mean Squared Error).\nsource\n\n\n\nL2sq\n\n(L2sq [vs1 vs2-or-val])\n(L2sq vs1 vs2-or-val)\n\nCalculates the Squared Euclidean distance between two sequences or a sequence and a constant value.\nThis is the sum of the squared differences between corresponding elements. It is equivalent to the rss (Residual Sum of Squares).\nParameters:\n\nvs1 (sequence of numbers): The first sequence.\nvs2-or-val (sequence of numbers or single number): The second sequence of numbers, or a single number to compare against each element of vs1.\n\nIf both inputs are sequences, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the calculated Squared Euclidean distance as a double.\nSee also L1, L2, LInf, rss (Residual Sum of Squares), mse (Mean Squared Error).\nsource\n\n\n\nLInf\n\n(LInf [vs1 vs2-or-val])\n(LInf vs1 vs2-or-val)\n\nCalculates the L-infinity distance (Chebyshev distance) between two sequences or a sequence and a constant value.\nThe Chebyshev distance is the maximum absolute difference between corresponding elements.\nParameters:\n\nvs1 (sequence of numbers): The first sequence.\nvs2-or-val (sequence of numbers or single number): The second sequence of numbers, or a single number to compare against each element of vs1.\n\nIf both inputs are sequences, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the calculated L-infinity distance as a double.\nSee also L1, L2, L2sq.\nsource\n\n\n\nacf\n\n(acf data)\n(acf data lags)\n\nCalculates the Autocorrelation Function (ACF) for a given time series data.\nThe ACF measures the linear dependence between a time series and its lagged values. It helps identify patterns (like seasonality or trend) and inform the selection of models for time series analysis (e.g., in ARIMA modeling).\nParameters:\n\ndata (seq of numbers): The time series data.\nlags (long or seq of longs, optional):\n\nIf a number, calculates ACF for lags from 0 up to this maximum lag.\nIf a sequence of numbers, calculates ACF for each lag specified in the sequence.\nIf omitted (1-arity call), calculates ACF for lags from 0 up to (dec (count data)).\n\n\nReturns a sequence of doubles: the autocorrelation coefficients for the specified lags. The value at lag 0 is always 1.0.\nSee also acf-ci (Calculates ACF with confidence intervals), pacf, pacf-ci.\nsource\n\n\n\nacf-ci\n\n(acf-ci data)\n(acf-ci data lags)\n(acf-ci data lags alpha)\n\nCalculates the Autocorrelation Function (ACF) for a time series and provides approximate confidence intervals.\nThis function computes the ACF of the input time series data for specified lags (see acf) and includes approximate confidence intervals around the ACF estimates. These intervals help determine whether the autocorrelation at a specific lag is statistically significant (i.e., likely non-zero in the population).\nParameters:\n\ndata (seq of numbers): The time series data.\nlags (long or seq of longs, optional):\n\nIf a number, calculates ACF for lags from 0 up to this maximum lag.\nIf a sequence of numbers, calculates ACF for each lag specified in the sequence.\nIf omitted (1-arity call), calculates ACF for lags from 0 up to (dec (count data)).\n\nalpha (double, optional): The significance level for the confidence intervals. Defaults to 0.05 (for a 95% CI).\n\nReturns a map containing:\n\n:ci (double): The value of the approximate standard confidence interval bound for lags &gt; 0. If the absolute value of an ACF coefficient at lag k &gt; 0 exceeds this value, it is considered statistically significant.\n:acf (seq of doubles): The sequence of autocorrelation coefficients at lags from 0 up to lags (or specified lags if lags is a sequence), calculated using acf.\n:cis (seq of doubles): Cumulative confidence intervals for ACF. These are based on the variance of the sum of squared sample autocorrelations up to each lag.\n\nSee also acf, pacf, pacf-ci.\nsource\n\n\n\nad-test-one-sample\n\n(ad-test-one-sample xs)\n(ad-test-one-sample xs distribution-or-ys)\n(ad-test-one-sample xs distribution-or-ys {:keys [sides kernel bandwidth], :or {sides :right, kernel :gaussian}})\n\nPerforms the Anderson-Darling (AD) test for goodness-of-fit.\nThis test assesses the null hypothesis that a sample xs comes from a specified theoretical distribution or another empirical distribution. It is sensitive to differences in the tails of the distributions.\nParameters:\n\nxs (seq of numbers): The sample data to be tested.\ndistribution-or-ys (optional):\n\nA fastmath.random distribution object to test against. If omitted, defaults to the standard normal distribution (fastmath.random/default-normal).\nA sequence of numbers (ys). In this case, an empirical distribution is estimated from ys using Kernel Density Estimation (KDE) or an enumerated distribution (see :kernel option).\n\nopts (map, optional): Options map:\n\n:sides (keyword, default :right): Specifies the side(s) of the A^2 statistic’s distribution used for p-value calculation.\n\n:right (default): Tests if the observed A^2 statistic is significantly large (standard approach for AD test, indicating poor fit).\n:left: Tests if the observed A^2 statistic is significantly small.\n:two-sided: Tests if the observed A^2 statistic is extreme in either tail.\n\n:kernel (keyword, default :gaussian): Used only when distribution-or-ys is a sequence. Specifies the method to estimate the empirical distribution:\n\n:gaussian (or other KDE kernels): Uses Kernel Density Estimation.\n:enumerated: Creates a discrete empirical distribution from ys.\n\n:bandwidth (double, optional): Bandwidth for KDE (if applicable).\n\n\nReturns a map containing:\n\n:A2: The Anderson-Darling test statistic (A^2).\n:stat: Alias for :A2.\n:p-value: The p-value associated with the test statistic and the specified :sides.\n:n: Sample size of xs.\n:mean: Mean of the sample xs (for context).\n:stddev: Standard deviation of the sample xs (for context).\n:sides: The alternative hypothesis side used for p-value calculation.\n\nsource\n\n\n\nadjacent-values\n\n(adjacent-values vs)\n(adjacent-values vs estimation-strategy)\n(adjacent-values vs q1 q3 m)\n\nLower and upper adjacent values (LAV and UAV).\nLet Q1 is 25-percentile and Q3 is 75-percentile. IQR is (- Q3 Q1).\n\nLAV is smallest value which is greater or equal to the LIF = (- Q1 (* 1.5 IQR)).\nUAV is largest value which is lower or equal to the UIF = (+ Q3 (* 1.5 IQR)).\nthird value is a median of samples\n\nOptional estimation-strategy argument can be set to change quantile calculations estimation type. See estimation-strategies.\nsource\n\n\n\nameasure\n\n(ameasure [group1 group2])\n(ameasure group1 group2)\n\nCalculates the Vargha-Delaney A measure for two independent samples.\nA non-parametric effect size measure quantifying the probability that a randomly chosen value from the first sample (group1) is greater than a randomly chosen value from the second sample (group2).\nParameters:\n\ngroup1: The first independent sample.\ngroup2: The second independent sample.\n\nReturns the calculated A measure (a double) in the range [0, 1]. A value of 0.5 indicates stochastic equality (distributions are overlapping). Values &gt; 0.5 mean group1 tends to be larger; values &lt; 0.5 mean group2 tends to be larger.\nRelated to cliffs-delta and the Wilcoxon-Mann-Whitney U test statistic.\nSee also cliffs-delta, wmw-odds.\nsource\n\n\n\nbinary-measures\n\n(binary-measures tp fn fp tn)\n(binary-measures confusion-matrix)\n(binary-measures actual prediction)\n(binary-measures actual prediction true-value)\n\nCalculates a selected subset of common evaluation metrics for binary classification results.\nThis function is a convenience wrapper around binary-measures-all, providing a map containing the most frequently used metrics derived from a 2x2 confusion matrix.\nThe 2x2 confusion matrix is based on True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN):\n\n\n\n\nPredicted True\nPredicted False\n\n\n\n\nActual True\nTP\nFN\n\n\nActual False\nFP\nTN\n\n\n\nThe function accepts the same input formats as binary-measures-all:\n\n(binary-measures tp fn fp tn): Direct input of the four counts.\n(binary-measures confusion-matrix): Input as a structured representation (map with keys like :tp, :fn, :fp, :tn; sequence of sequences [ [TP FP] [FN TN] ]; or flat sequence [TP FN FP TN]).\n(binary-measures actual prediction): Input as two sequences of outcomes.\n(binary-measures actual prediction true-value): Input as two sequences with a specified encoding for true (success).\n\nParameters:\n\ntp, fn, fp, tn (long): Counts from the confusion matrix.\nconfusion-matrix (map or sequence): Representation of the confusion matrix.\nactual, prediction (sequences): Sequences of true and predicted outcomes.\ntrue-value (optional): Specifies how outcomes are converted to boolean true/false.\n\nReturns a map containing the following selected metrics:\n\n:tp (True Positives)\n:tn (True Negatives)\n:fp (False Positives)\n:fn (False Negatives)\n:accuracy\n:fdr (False Discovery Rate, 1 - Precision)\n:f-measure (F1 Score, harmonic mean of Precision and Recall)\n:fall-out (False Positive Rate)\n:precision (Positive Predictive Value)\n:recall (True Positive Rate / Sensitivity)\n:sensitivity (Alias for Recall/TPR)\n:specificity (True Negative Rate)\n:prevalence (Proportion of positive cases)\n\nSee also confusion-matrix, binary-measures-all, mcc, contingency-2x2-measures-all.\nsource\n\n\n\nbinary-measures-all\n\n(binary-measures-all tp fn fp tn)\n(binary-measures-all confusion-matrix)\n(binary-measures-all actual prediction)\n(binary-measures-all actual prediction true-value)\n\nCalculates a comprehensive set of evaluation metrics for binary classification results.\nThis function computes various statistics derived from a 2x2 confusion matrix, summarizing the performance of a binary classifier.\nThe 2x2 confusion matrix is based on True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN):\n\n\n\n\nPredicted True\nPredicted False\n\n\n\n\nActual True\nTP\nFN\n\n\nActual False\nFP\nTN\n\n\n\nThe function supports several input formats:\n\n(binary-measures-all tp fn fp tn): Direct input of the four counts as arguments.\n\ntp (long): True Positive count.\nfn (long): False Negative count.\nfp (long): False Positive count.\ntn (long): True Negative count.\n\n(binary-measures-all confusion-matrix): Input as a structured representation of the confusion matrix.\n\nconfusion-matrix: Can be:\n\nA map with keys like :tp, :fn, :fp, :tn (e.g., {:tp 10 :fn 2 :fp 5 :tn 80}).\nA sequence of sequences representing rows [TP FP] [FN TN](#LOS-TP FP] [FN TN) (e.g., [10 5] [2 80](#LOS-10 5] [2 80)).\nA flat sequence [TP FN FP TN] (e.g., [10 2 5 80]).\n\n\n(binary-measures-all actual prediction): Input as two sequences of outcomes.\n\nactual (sequence): Sequence of true outcomes.\nprediction (sequence): Sequence of predicted outcomes. Must have the same length as actual. Values in actual and prediction are converted to boolean true/false. By default, any non-nil or non-zero numeric value is treated as true, and nil or 0.0 is treated as false.\n\n(binary-measures-all actual prediction true-value): Input as two sequences with a specified encoding for true.\n\nactual, prediction: Sequences as in the previous arity.\ntrue-value (optional): Specifies how values in actual and prediction are converted to boolean true (success) or false (failure).\n\nnil (default): Non-nil/non-zero (for numbers) is true.\nAny sequence/set: Values found in this collection are true.\nA map: Values are mapped according to the map; if a key is not found or maps to false, the value is false.\nA predicate function: Returns true if the value satisfies the predicate.\n\n\n\nReturns a map containing a wide array of calculated metrics. This includes, but is not limited to:\n\nBasic Counts: :tp, :fn, :fp, :tn\nTotals: :cp (Actual Positives), :cn (Actual Negatives), :pcp (Predicted Positives), :pcn (Predicted Negatives), :total (Grand Total)\nRates (often ratios of counts):\n\n:tpr (True Positive Rate, Recall, Sensitivity, Hit Rate)\n:fnr (False Negative Rate, Miss Rate)\n:fpr (False Positive Rate, Fall-out)\n:tnr (True Negative Rate, Specificity, Selectivity)\n:ppv (Positive Predictive Value, Precision)\n:fdr (False Discovery Rate, 1 - ppv)\n:npv (Negative Predictive Value)\n:for (False Omission Rate, 1 - npv)\n\nRatios/Odds:\n\n:lr+ (Positive Likelihood Ratio)\n:lr- (Negative Likelihood Ratio)\n:dor (Diagnostic Odds Ratio)\n\nCombined Scores:\n\n:accuracy\n:ba (Balanced Accuracy)\n:fm (Fowlkes–Mallows index)\n:pt (Prevalence Threshold)\n:ts (Threat Score, Jaccard index)\n:f-measure / :f1-score (F1 Score, special case of F-beta score)\n:f-beta (Function to calculate F-beta for any beta)\n:mcc / :phi (Matthews Correlation Coefficient, Phi coefficient)\n:bm (Bookmaker Informedness)\n:kappa (Cohen’s Kappa, for 2x2 table)\n:mk (Markedness)\n\n\nMetrics are generally calculated using standard formulas based on the TP, FN, FP, TN counts. For more details on specific metrics, refer to standard classification literature or the Wikipedia page on Precision and recall, which covers many of these concepts.\nSee also confusion-matrix, binary-measures (for a selected subset of metrics), mcc, contingency-2x2-measures-all (for a broader set of 2x2 table measures).\nsource\n\n\n\nbinomial-ci\n\n(binomial-ci number-of-successes number-of-trials)\n(binomial-ci number-of-successes number-of-trials method)\n(binomial-ci number-of-successes number-of-trials method alpha)\n\nCalculates a confidence interval for a binomial proportion.\nGiven the number of observed successes in a fixed number of trials, this function estimates a confidence interval for the true underlying probability of success (p).\nDifferent statistical methods are available for calculating the interval, as the accuracy and behavior of the interval can vary, especially for small sample sizes or probabilities close to 0 or 1.\nParameters:\n\nnumber-of-successes (long): The count of successful outcomes.\nnumber-of-trials (long): The total number of independent trials.\nmethod (keyword, optional): The method used to calculate the confidence interval. Defaults to :asymptotic.\nalpha (double, optional): The significance level (alpha) for the interval. The confidence level is 1 - alpha. Defaults to 0.05 (yielding a 95% CI).\n\nAvailable method values:\n\n:asymptotic: Normal approximation interval (Wald interval), based on the Central Limit Theorem. Simple but can be inaccurate for small samples or probabilities near 0 or 1.\n:agresti-coull: An adjustment to the asymptotic interval, adding ‘pseudo-counts’ to improve performance for small samples.\n:clopper-pearson: An exact method based on inverting binomial tests. Provides guaranteed coverage but can be overly conservative (wider than necessary).\n:wilson: Score interval, derived from the score test. Generally recommended as a good balance of accuracy and coverage for various sample sizes.\n:prop.test: Interval typically used with prop.test in R, applies a continuity correction.\n:cloglog: Confidence interval based on the complementary log-log transformation.\n:logit: Confidence interval based on the logit transformation.\n:probit: Confidence interval based on the probit transformation (inverse of standard normal CDF).\n:arcsine: Confidence interval based on the arcsine transformation.\n:all: Applies all available methods and returns a map where keys are method keywords and values are their respective confidence intervals (as triplets).\n\nReturns:\n\nA vector [lower-bound, upper-bound, estimated-p].\n\nlower-bound (double): The lower limit of the confidence interval.\nupper-bound (double): The upper limit of the confidence interval.\nestimated-p (double): The observed proportion of successes (number-of-successes / number-of-trials).\n\n\nIf method is :all, returns a map of results from each method.\nSee also binomial-test for performing a hypothesis test on a binomial proportion.\nsource\n\n\n\nbinomial-ci-methods\nsource\n\n\n\nbinomial-test\n\n(binomial-test xs)\n(binomial-test xs maybe-params)\n(binomial-test number-of-successes number-of-trials {:keys [alpha p ci-method sides], :or {alpha 0.05, p 0.5, ci-method :asymptotic, sides :two-sided}})\n\nPerforms an exact test of a simple null hypothesis about the probability of success in a Bernoulli experiment, based on the binomial distribution.\nThis test assesses the null hypothesis that the true probability of success (p) in the underlying population is equal to a specified value (default 0.5).\nThe function can be called in two ways:\n\nWith counts: (binomial-test number-of-successes number-of-trials params)\nWith data: (binomial-test xs params), where xs is a sequence of outcomes. In this case, the outcomes in xs are converted to true/false based on the :true-false-conv parameter (if provided, otherwise numeric 1s are true), and the number of successes and total trials are derived from xs.\n\nParameters:\n\nnumber-of-successes (long): Observed number of successful outcomes.\nnumber-of-trials (long): Total number of trials.\nxs (sequence): Sample data (used in the alternative call signature).\nparams (map, optional): Options map:\n\n:p (double, default 0.5): The hypothesized probability of success under the null hypothesis.\n:alpha (double, default 0.05): Significance level for confidence interval calculation.\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis.\n\n:two-sided (default): True probability p is not equal to the hypothesized p.\n:one-sided-greater: True probability p is greater than the hypothesized p.\n:one-sided-less: True probability p is less than the hypothesized p.\n\n:ci-method (keyword, default :asymptotic): Method used to calculate the confidence interval for the probability of success. See binomial-ci and binomial-ci-methods for available options (e.g., :wilson, :clopper-pearson).\n:true-false-conv (optional, used only with xs): A function, set, or map to convert elements of xs into boolean true (success) or false (failure). See binary-measures-all documentation for details. If nil and xs contains numbers, 1.0 is treated as success.\n\n\nReturns a map containing:\n\n:p-value: The probability of observing a result as extreme as, or more extreme than, the observed number of successes, assuming the null hypothesis is true. Calculated using the binomial distribution.\n:p: The hypothesized probability of success used in the test.\n:successes: The observed number of successes.\n:trials: The total number of trials.\n:alpha: Significance level used for the confidence interval.\n:level: Confidence level (1 - alpha).\n:sides / :test-type: Alternative hypothesis side used.\n:stat: The test statistic (the observed number of successes).\n:estimate: The observed proportion of successes (successes / trials).\n:ci-method: Confidence interval method used.\n:confidence-interval: A confidence interval for the true probability of success, calculated using the specified :ci-method and adjusted for the :sides parameter.\n\nsource\n\n\n\nbonett-seier-test\n\n(bonett-seier-test xs)\n(bonett-seier-test xs params)\n(bonett-seier-test xs geary-kurtosis {:keys [sides], :or {sides :two-sided}})\n\nPerforms the Bonett-Seier test for normality based on Geary’s ‘g’ kurtosis measure.\nThis test assesses the null hypothesis that the data comes from a normally distributed population by checking if the sample Geary’s ‘g’ statistic significantly deviates from the value expected under normality (sqrt(2/pi)).\nParameters:\n\nxs (seq of numbers): The sample data. Requires (count xs) &gt; 3 for variance calculation.\ngeary-kurtosis (double, optional): A pre-calculated Geary’s ‘g’ kurtosis value. If omitted, it’s calculated from xs.\nparams (map, optional): Options map:\n\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis regarding the deviation from normal kurtosis.\n\n:two-sided (default): The population kurtosis (measured by ‘g’) is different from normal.\n:one-sided-greater: Population is leptokurtic (‘g’ &lt; sqrt(2/pi)). Note Geary’s ‘g’ decreases with peakedness.\n:one-sided-less: Population is platykurtic (‘g’ &gt; sqrt(2/pi)). Note Geary’s ‘g’ increases with flatness.\n\n\n\nReturns a map containing:\n\n:Z: The final test statistic (approximately standard normal under H0).\n:stat: Alias for :Z.\n:p-value: The p-value associated with Z and the specified :sides.\n:kurtosis: The Geary’s ‘g’ kurtosis value used in the test.\n:n: The sample size.\n:sides: The alternative hypothesis side used.\n\nReferences: - Bonett, D. G., & Seier, E. (2002). A test of normality with high uniform power. Computational Statistics & Data Analysis, 40(3), 435-445. (Provides theoretical basis)\nSee also kurtosis, kurtosis-test, normality-test, jarque-bera-test.\nsource\n\n\n\nbootstrap DEPRECATED\nDeprecated: Please use fastmath.stats.bootstrap/bootstrap instead\n\n(bootstrap vs)\n(bootstrap vs samples)\n(bootstrap vs samples size)\n\nGenerate set of samples of given size from provided data.\nDefault samples is 200, number of size defaults to sample size.\nsource\n\n\n\nbootstrap-ci DEPRECATED\nDeprecated: Please use fastmath.stats.boostrap/ci-basic instead\n\n(bootstrap-ci vs)\n(bootstrap-ci vs alpha)\n(bootstrap-ci vs alpha samples)\n(bootstrap-ci vs alpha samples stat-fn)\n\nBootstrap method to calculate confidence interval.\nAlpha defaults to 0.98, samples to 1000. Last parameter is statistical function used to measure, default: mean.\nReturns ci and statistical function value.\nsource\n\n\n\nbox-cox-infer-lambda\n\n(box-cox-infer-lambda xs)\n(box-cox-infer-lambda xs lambda-range)\n(box-cox-infer-lambda xs lambda-range opts)\n\nFinds the optimal lambda (λ) parameter for the Box-Cox transformation of a dataset using the Maximum Likelihood Estimation (MLE) method.\nThe Box-Cox transformation is a family of power transformations often applied to positive data to make it more closely resemble a normal distribution and stabilize variance. This function estimates the lambda value that maximizes the log-likelihood function of the transformed data, assuming the transformed data is normally distributed.\nParameters:\n\nxs (sequence of numbers): The input numerical data sequence.\nlambda-range (vector of two numbers, optional): A sequence [min-lambda, max-lambda] defining the closed interval within which the optimal lambda is searched. Defaults to [-3.0, 3.0].\nopts (map, optional): Additional options affecting the data used for the likelihood calculation. These options are passed to the internal data preparation step. Key options include:\n\n:alpha (double, default 0.0): A constant value added to xs before estimating lambda. This is often used when xs contains zero or negative values and the standard Box-Cox (which requires positive input) is desired, or to explore transformations around a shifted location.\n:negative? (boolean, default false): If true, indicates that the likelihood is estimated based on the modified Box-Cox transformation (Bickel and Doksum approach) suitable for negative values. The estimation process will work with the absolute values of the data shifted by :alpha.\n\n\nReturns the estimated optimal lambda value as a double.\nThe inferred lambda value can then be used as the lambda parameter for the box-cox-transformation function to apply the actual transformation to the dataset.\nSee also box-cox-transformation, yeo-johnson-infer-lambda, yeo-johnson-transformation.\nsource\n\n\n\nbox-cox-transformation\n\n(box-cox-transformation xs)\n(box-cox-transformation xs lambda)\n(box-cox-transformation xs lambda {:keys [scaled? inverse?], :as opts})\n\nApplies Box-Cox transformation to a data.\nThe Box-Cox transformation is a family of power transformations used to stabilize variance and make data more normally distributed.\nParameters:\n\nxs (seq of numbers): The input data.\nlambda (default 0.0): The power parameter. If nil or [lambda-min, lambda-max], lambda is inferred using maximum log likelihood.\nOptions map:\n\nalpha (optional): A shift parameter applied before transformation.\nscaled? (default false): Scale by geometric mean or any other number\nnegative? (default false): Allow negative values\ninverse? (default: false): Perform inverse operation, lambda can’t be inferred.\n\n\nReturns transformed data.\nRelated: yeo-johnson-transformation\nsource\n\n\n\nbrown-forsythe-test\n\n(brown-forsythe-test xss)\n(brown-forsythe-test xss params)\n\nBrown-Forsythe test for homogeneity of variances.\nThis test is a modification of Levene’s test, using the median instead of the mean for calculating the spread within each group. This makes the test more robust against non-normally distributed data.\nCalls levene-test with :statistic set to median. Accepts the same parameters as levene-test, except for :statistic.\nParameters: - xss (sequence of sequences): A collection of data groups. - params (map, optional): Options map (see levene-test).\nsource\n\n\n\nchisq-test\n\n(chisq-test contingency-table-or-xs)\n(chisq-test contingency-table-or-xs params)\n\nChi square test, a power divergence test for lambda 1.0\nPerforms a power divergence test, which encompasses several common statistical tests like Chi-squared, G-test (likelihood ratio), etc., based on the lambda parameter. This function can perform either a goodness-of-fit test or a test for independence in a contingency table.\nUsage:\n\nGoodness-of-Fit (GOF):\n\nInput: observed-counts (sequence of numbers) and :p (expected probabilities/weights).\nInput: data (sequence of numbers) and :p (a distribution object). In this case, a histogram of data is created (controlled by :bins) and compared against the probability mass/density of the distribution in those bins.\n\nTest for Independence:\n\nInput: contingency-table (2D sequence or map format). The :p option is ignored.\n\n\nOptions map:\n\n:lambda (double, default: 2/3): Determines the specific test statistic. Common values:\n\n1.0: Pearson Chi-squared test (chisq-test).\n0.0: G-test / Multinomial Likelihood Ratio test (multinomial-likelihood-ratio-test).\n-0.5: Freeman-Tukey test (freeman-tukey-test).\n-1.0: Minimum Discrimination Information test (minimum-discrimination-information-test).\n-2.0: Neyman Modified Chi-squared test (neyman-modified-chisq-test).\n2/3: Cressie-Read test (default, cressie-read-test).\n\n:p (seq of numbers or distribution): Expected probabilities/weights (for GOF with counts) or a fastmath.random distribution object (for GOF with data). Ignored for independence tests.\n:alpha (double, default: 0.05): Significance level for confidence intervals.\n:ci-sides (keyword, default: :two-sided): Sides for bootstrap confidence intervals (:two-sided, :one-sided-greater, :one-sided-less).\n:sides (keyword, default: :one-sided-greater): Alternative hypothesis side for the p-value calculation against the Chi-squared distribution (:one-sided-greater, :one-sided-less, :two-sided).\n:bootstrap-samples (long, default: 1000): Number of bootstrap samples for confidence interval estimation.\n:ddof (long, default: 0): Delta degrees of freedom. Adjustment subtracted from the calculated degrees of freedom.\n:bins (number, keyword, or seq): Used only for GOF test against a distribution. Specifies the number of bins, an estimation method (see histogram), or explicit bin edges for histogram creation.\n\nReturns a map containing:\n\n:stat: The calculated power divergence test statistic.\n:chi2: Alias for :stat.\n:df: Degrees of freedom for the test.\n:p-value: The p-value associated with the test statistic.\n:n: Total number of observations.\n:estimate: Observed proportions.\n:expected: Expected counts or proportions under the null hypothesis.\n:confidence-interval: Bootstrap confidence intervals for the observed proportions.\n:lambda, :alpha, :sides, :ci-sides: Input options used.\n\nsource\n\n\n\nci\n\n(ci vs)\n(ci vs alpha)\n\nT-student based confidence interval for given data. Alpha value defaults to 0.05.\nLast value is mean.\nsource\n\n\n\ncliffs-delta\n\n(cliffs-delta [group1 group2])\n(cliffs-delta group1 group2)\n\nCalculates Cliff’s Delta (δ), a non-parametric effect size measure for assessing the difference between two groups of ordinal or continuous data.\nCliff’s Delta quantifies the degree of overlap between two distributions. It represents the probability that a randomly chosen value from the first group is greater than a randomly chosen value from the second group, minus the reverse probability.\nParameters:\n\ngroup1 (seq of numbers): The first sample.\ngroup2 (seq of numbers): The second sample.\n\nReturns the calculated Cliff’s Delta value as a double.\nInterpretation:\n\nA value of +1 indicates complete separation where every value in group1 is greater than every value in group2.\nA value of -1 indicates complete separation where every value in group2 is greater than every value in group1.\nA value of 0 indicates complete overlap between the distributions.\nValues between -1 and 1 indicate varying degrees of overlap. Cohen (1988) suggested guidelines for effect size: |δ| &lt; 0.147 (negligible), 0.147 ≤ |δ| &lt; 0.33 (small), 0.33 ≤ |δ| &lt; 0.474 (medium), |δ| ≥ 0.474 (large).\n\nCliff’s Delta is a robust measure, suitable for ordinal data or when assumptions of parametric tests (like normality or equal variances) are violated. It is closely related to the wmw-odds (Wilcoxon-Mann-Whitney odds) and the ameasure (Vargha-Delaney A).\nSee also wmw-odds, ameasure, cohens-d, glass-delta.\nsource\n\n\n\ncoefficient-matrix\n\n(coefficient-matrix vss)\n(coefficient-matrix vss measure-fn)\n(coefficient-matrix vss measure-fn symmetric?)\n\nGenerates a matrix of pairwise coefficients from a sequence of sequences.\nThis function calculates a matrix where the element at row i and column j is the result of applying the provided measure-fn to the i-th sequence and the j-th sequence from the input vss.\nParameters:\n\nvss (sequence of sequences of numbers): The collection of data sequences. Each inner sequence is treated as a variable or set of observations. All inner sequences should ideally have the same length if the measure-fn expects it.\nmeasure-fn (function, optional): A function of two arguments (sequences) that returns a double representing the coefficient or measure between them. Defaults to pearson-correlation.\nsymmetric? (boolean, optional): If true, the function assumes that measure-fn(a, b) is equal to measure-fn(b, a). It calculates the upper (or lower) triangle of the matrix and mirrors the values to the other side. This is an optimization for symmetric measures like correlation and covariance. If false (default), all pairwise combinations (i, j) are calculated independently.\n\nReturns a sequence of sequences (a matrix) of doubles.\nNote: While this function’s symmetric? parameter defaults to false, convenience functions like correlation-matrix and covariance-matrix wrap this function and explicitly set symmetric? to true as their respective measures are symmetric.\nSee also correlation-matrix, covariance-matrix.\nsource\n\n\n\ncohens-d\n\n(cohens-d [group1 group2])\n(cohens-d group1 group2)\n(cohens-d group1 group2 method)\n\nCalculate Cohen’s d effect size between two groups.\nCohen’s d is a standardized measure used to quantify the magnitude of the difference between the means of two independent groups. It expresses the mean difference in terms of standard deviation units.\nThe most common formula for Cohen’s d is:\n  d = (mean(group1) - mean(group2)) / pooled_stddev\nwhere pooled_stddev is the pooled standard deviation of the two groups, calculated under the assumption of equal variances.\nParameters:\n\ngroup1 (seq of numbers): The first independent sample.\ngroup2 (seq of numbers): The second independent sample.\nmethod (optional keyword): Specifies the method for calculating the pooled standard deviation, affecting the denominator of the formula. Possible values are :unbiased (default), :biased, or :avg. See pooled-stddev for details on these methods.\n\nReturns the calculated Cohen’s d effect size as a double.\nInterpretation guidelines (approximate for normal distributions): - |d| = 0.2: small effect - |d| = 0.5: medium effect - |d| = 0.8: large effect\nAssumptions: - The two samples are independent. - Data within each group are approximately normally distributed. - The choice of :method implies assumptions about equal variances (default :unbiased and :biased assume equal variances, while :avg does not but might be less standard).\nSee also hedges-g (a version bias-corrected for small sample sizes), glass-delta (an alternative effect size measure using the control group standard deviation), pooled-stddev.\nsource\n\n\n\ncohens-d-corrected\n\n(cohens-d-corrected [group1 group2])\n(cohens-d-corrected group1 group2)\n(cohens-d-corrected group1 group2 method)\n\nCalculates Cohen’s d effect size corrected for bias in small sample sizes.\nThis function applies a correction factor (derived from the gamma function) to Cohen’s d (cohens-d) to provide a less biased estimate of the population effect size when sample sizes are small. This corrected measure is sometimes referred to as Hedges’ g, though this function specifically implements the correction applied to Cohen’s d.\nThe correction factor is (1 - 3 / (4 * df - 1)) where df is the degrees of freedom used in the standard Cohen’s d calculation.\nParameters:\n\ngroup1 (seq of numbers): The first independent sample.\ngroup2 (seq of numbers): The second independent sample.\nmethod (optional keyword): Specifies the method for calculating the pooled standard deviation, affecting the denominator of the formula (passed to cohens-d). Possible values are :unbiased (default), :biased, or :avg. See pooled-stddev for details on these methods.\n\nReturns the calculated bias-corrected Cohen’s d effect size as a double.\nNote: While this function is named cohens-d-corrected, Hedges’ g (calculated by hedges-g-corrected) also applies a similar small-sample bias correction. Differences might exist based on the specific correction formula or degree of freedom definition used. This function uses (count group1) + (count group2) - 2 as the degrees of freedom for the correction by default (when :unbiased method is used for cohens-d).\nSee also cohens-d, hedges-g, hedges-g-corrected.\nsource\n\n\n\ncohens-f\n\n(cohens-f [group1 group2])\n(cohens-f group1 group2)\n(cohens-f group1 group2 type)\n\nCalculates Cohen’s f, a measure of effect size derived as the square root of Cohen’s f² (cohens-f2).\nCohen’s f is a standardized measure quantifying the magnitude of an effect, often used in the context of ANOVA or regression. It is the square root of the ratio of the variance explained by the effect to the unexplained variance.\nParameters:\n\ngroup1 (seq of numbers): The dependent variable.\ngroup2 (seq of numbers): The independent variable (or predictor). Must have the same length as group1.\ntype (keyword, optional): Specifies the measure of ‘Proportion of Variance Explained’ used in the underlying cohens-f2 calculation. Defaults to :eta.\n\n:eta (default): Uses Eta-squared (sample R²), a measure of variance explained in the sample.\n:omega: Uses Omega-squared, a less biased estimate of variance explained in the population.\n:epsilon: Uses Epsilon-squared, another less biased estimate of variance explained in the population.\nAny function: A function accepting group1 and group2 and returning a double representing the proportion of variance explained.\n\n\nReturns the calculated Cohen’s f effect size as a double. Values range from 0 upwards.\nInterpretation:\n\nValues are positive. Larger values indicate a stronger effect (more variance in group1 explained by group2).\nCohen’s guidelines for interpreting the magnitude of f² (and by extension, f) are:\n\n\\(f = 0.10\\) (approx. \\(f^2 = 0.01\\)): small effect\n\\(f = 0.25\\) (approx. \\(f^2 = 0.0625\\)): medium effect\n\\(f = 0.40\\) (approx. \\(f^2 = 0.16\\)): large effect (Note: Guidelines are often quoted for f², interpret f as \\(\\sqrt{f^2}\\))\n\n\nSee also cohens-f2, eta-sq, omega-sq, epsilon-sq.\nsource\n\n\n\ncohens-f2\n\n(cohens-f2 [group1 group2])\n(cohens-f2 group1 group2)\n(cohens-f2 group1 group2 type)\n\nCalculates Cohen’s f², a measure of effect size often used in ANOVA or regression.\nCohen’s f² quantifies the magnitude of the effect of an independent variable or set of predictors on a dependent variable, expressed as the ratio of the variance explained by the effect to the unexplained variance.\nThis function allows calculating f² using different measures for the ‘Proportion of Variance Explained’, specified by the type parameter:\n\n:eta (default): Uses eta-sq (Eta-squared), which in this implementation is equivalent to the sample \\(R^2\\) from a linear regression of group1 on group2. This is a measure of the proportion of variance explained in the sample.\n:omega: Uses omega-sq (Omega-squared), a less biased estimate of the proportion of variance explained in the population.\n:epsilon: Uses epsilon-sq (Epsilon-squared), another less biased estimate of the proportion of variance explained in the population, similar to adjusted \\(R^2\\).\nAny function: A function accepting group1 and group2 and returning a double representing the proportion of variance explained.\n\nParameters:\n\ngroup1 (seq of numbers): The dependent variable.\ngroup2 (seq of numbers): The independent variable (or predictor). Must have the same length as group1.\ntype (keyword, optional): Specifies the measure of ‘Proportion of Variance Explained’ to use (:eta, :omega, :epsilon or any function). Defaults to :eta.\n\nReturns the calculated Cohen’s f² effect size as a double. Values range from 0 upwards.\nInterpretation Guidelines (approximate, often used for F-tests in ANOVA/regression): - \\(f^2 = 0.02\\): small effect - \\(f^2 = 0.15\\): medium effect - \\(f^2 = 0.35\\): large effect\nSee also cohens-f, eta-sq, omega-sq, epsilon-sq.\nsource\n\n\n\ncohens-kappa\n\n(cohens-kappa group1 group2)\n(cohens-kappa contingency-table)\n\nCalculates Cohen’s Kappa coefficient (κ), a statistic that measures inter-rater agreement for categorical items, while correcting for chance agreement.\nIt is often used to assess the consistency of agreement between two raters or methods. Its value typically ranges from -1 to +1:\n\nκ = 1: Perfect agreement.\nκ = 0: Agreement is no better than chance.\nκ &lt; 0: Agreement is worse than chance.\n\nThe function can be called in two ways:\n\nWith two sequences group1 and group2: The function will automatically construct a 2x2 contingency table from the unique values in the sequences (assuming they represent two binary variables). The mapping of values to table cells (e.g., what corresponds to TP, TN, FP, FN) depends on how contingency-table orders the unique values. For direct control over which cell is which, use the contingency table input.\nWith a contingency table: The contingency table can be provided as:\n\nA map where keys are [row-index, column-index] tuples and values are counts (e.g., {[0 0] TP, [0 1] FP, [1 0] FN, [1 1] TN}). This is the output format of contingency-table with two inputs. The mapping of indices to TP/TN/FP/FN depends on the order of unique values in the original data if generated by contingency-table, or the explicit structure if created manually or via rows-&gt;contingency-table. Standard convention maps [0 0] to TP, [0 1] to FP, [1 0] to FN, and [1 1] to TN for binary outcomes.\nA sequence of sequences representing the rows of the table (e.g., [TP FP] [FN TN](#LOS-TP FP] [FN TN)). This is equivalent to rows-&gt;contingency-table.\n\n\nParameters:\n\ngroup1 (sequence): The first sequence of binary outcomes/categories.\ngroup2 (sequence): The second sequence of binary outcomes/categories. Must have the same length as group1.\ncontingency-table (map or sequence of sequences): A pre-computed 2x2 contingency table. The cell values should represent counts (e.g., TP, FN, FP, TN).\n\nReturns the calculated Cohen’s Kappa coefficient as a double.\nSee also weighted-kappa (for ordinal data with partial agreement), contingency-table, contingency-2x2-measures, binary-measures-all.\nsource\n\n\n\ncohens-q\n\n(cohens-q r1 r2)\n(cohens-q group1 group2a group2b)\n(cohens-q group1a group2a group1b group2b)\n\nCompares two correlation coefficients by calculating the difference between their Fisher z-transformations.\nThe Fisher z-transformation (atanh) of a correlation coefficient r helps normalize the sampling distribution of correlation coefficients. The difference between two z’-transformed correlations is often used as a test statistic.\nThe function supports comparing correlations in different scenarios via its arities:\n\n(cohens-q r1 r2): Calculates the difference between the Fisher z-transformations of two correlation values r1 and r2 provided directly. This is typically used when comparing two independent correlation coefficients (e.g., correlations from two separate studies). Returns atanh(r1) - atanh(r2).\n\nr1, r2 (double): Correlation coefficient values (-1.0 to 1.0).\n\n(cohens-q group1 group2a group2b): Calculates the difference between the correlation of group1 with group2a and the correlation of group1 with group2b. This is commonly used for comparing dependent correlations (where group1 is a common variable). Calculates atanh(pearson-correlation(group1, group2a)) - atanh(pearson-correlation(group1, group2b)).\n\ngroup1, group2a, group2b (sequences): Data sequences from which Pearson correlations are computed.\n\n(cohens-q group1a group2a group1b group2b): Calculates the difference between the correlation of group1a with group2a and the correlation of group1b with group2b. This is typically used for comparing two independent correlations obtained from two distinct pairs of variables (all four sequences are independent). Calculates atanh(pearson-correlation(group1a, group2a)) - atanh(pearson-correlation(group1b, group2b)).\n\ngroup1a, group2a, group1b, group2b (sequences): Data sequences from which Pearson correlations are computed.\n\n\nReturns the difference between the Fisher z-transformed correlation values as a double.\nNote: For comparing dependent correlations (3-arity case), standard statistical tests (e.g., Steiger’s test) are more complex than a simple difference of z-transforms and involve the correlation between group2a and group2b. This function provides the basic difference value.\nsource\n\n\n\ncohens-u1\n\n(cohens-u1 [group1 group2])\n(cohens-u1 group1 group2)\n\nCalculates a non-parametric measure of difference or separation between two samples.\nThis function computes a value derived from cohens-u2, which internally quantifies a minimal difference between corresponding quantiles of the two empirical distributions.\nParameters:\n\ngroup1 (seq of numbers): The first sample.\ngroup2 (seq of numbers): The second sample.\n\nReturns the calculated measure as a double.\nInterpretation:\n\nValues close to -1 indicate high similarity or maximum overlap between the distributions (as the minimal difference between quantiles approaches zero).\nIncreasing values indicate greater difference or separation between the distributions (as the minimal difference between quantiles is larger).\n\nThis measure is symmetric, meaning the order of group1 and group2 does not affect the result. It is a non-parametric measure applicable to any data samples.\nSee also cohens-u2 (the measure this calculation is based on), cohens-u3 (related non-parametric measure), cohens-u1-normal (the version applicable to normal data).\nsource\n\n\n\ncohens-u1-normal\n\n(cohens-u1-normal group1 group2)\n(cohens-u1-normal group1 group2 method)\n(cohens-u1-normal d)\n\nCalculates Cohen’s U1, a measure of non-overlap between two distributions assumed to be normal with equal variances.\nCohen’s U1 quantifies the proportion of scores in the lower-scoring group that overlap with the scores in the higher-scoring group. A U1 of 0 means no overlap, while a U1 of 1 means complete overlap (distributions are identical).\nThis measure is calculated directly from Cohen’s d statistic (cohens-d) assuming normal distributions and equal variances.\nParameters:\n\ngroup1 (seq of numbers): The first sample.\ngroup2 (seq of numbers): The second sample.\nmethod (optional keyword): Specifies the method for calculating the pooled standard deviation used in the underlying cohens-d calculation. Possible values are :unbiased (default), :biased, or :avg. See pooled-stddev for details.\nd (double): A pre-calculated Cohen’s d value. If provided, group1, group2, and method are ignored.\n\nReturns the calculated Cohen’s U1 as a double [0, 1].\nAssumptions: - Both samples are drawn from normally distributed populations. - The populations have equal variances (homoscedasticity).\nSee also cohens-d, cohens-u2-normal, cohens-u3-normal, p-overlap (a non-parametric overlap measure).\nsource\n\n\n\ncohens-u2\n\n(cohens-u2 [group1 group2])\n(cohens-u2 group1 group2)\n\nCalculates a measure of overlap between two samples, referred to as Cohen’s U2.\nThis function quantifies the degree to which the distributions of group1 and group2 overlap. It is related to comparing values at corresponding percentile levels across the two groups or the proportion of values in one group that are below the median of the other. A value of 0 indicates no overlap, while a value of 1 indicates complete overlap (distributions are identical).\nThe measure is symmetric, meaning (cohens-u2 group1 group2) is equal to (cohens-u2 group2 group1).\nThis is a non-parametric measure, suitable for any data samples, and does not assume normality, unlike cohens-u2-normal.\nParameters:\n\ngroup1, group2 (sequences): The two samples directly as arguments.\n\nReturns the calculated Cohen’s U2 value as a double. The value typically ranges from 0 to 1. A value closer to 0.5 indicates substantial overlap between the distributions (e.g., the median of one group is near the median of the other); values closer to 0 or 1 indicate less overlap (greater separation between the distributions).\nsource\n\n\n\ncohens-u2-normal\n\n(cohens-u2-normal group1 group2)\n(cohens-u2-normal group1 group2 method)\n(cohens-u2-normal d)\n\nCalculates Cohen’s U2, a measure of overlap between two distributions assumed to be normal with equal variances.\nCohen’s U2 quantifies the proportion of scores in the lower-scoring group that are below the point located halfway between the means of the two groups (or equivalently, the proportion of scores in the higher-scoring group that are above this halfway point). This measure is calculated from Cohen’s d statistic (cohens-d) using the standard normal cumulative distribution function (\\(\\Phi\\)): \\(\\Phi(0.5 |d|)\\).\nParameters:\n\ngroup1 (seq of numbers): The first sample.\ngroup2 (seq of numbers): The second sample.\nmethod (optional keyword): Specifies the method for calculating the pooled standard deviation used in the underlying cohens-d calculation. Possible values are :unbiased (default), :biased, or :avg. See pooled-stddev for details.\nd (double): A pre-calculated Cohen’s d value. If provided, group1, group2, and method are ignored.\n\nReturns the calculated Cohen’s U2 as a double [0.0, 1.0]. A value closer to 0.5 indicates greater overlap between the distributions; values closer to 0 or 1 indicate less overlap.\nAssumptions: - Both samples are drawn from normally distributed populations. - The populations have equal variances (homoscedasticity).\nSee also cohens-d, cohens-u1-normal, cohens-u3-normal, p-overlap (a non-parametric overlap measure).\nsource\n\n\n\ncohens-u3\n\n(cohens-u3 [group1 group2])\n(cohens-u3 group1 group2)\n(cohens-u3 group1 group2 estimation-strategy)\n\nCalculates Cohen’s U3 for two samples.\nIn this implementation, Cohen’s U3 is defined as the proportion of values in the second sample (group2) that are less than the median of the first sample (group1).\nParameters:\n\ngroup1 (seq of numbers): The first sample. The median of this sample is used as the threshold.\ngroup2 (seq of numbers): The second sample. Values from this sample are counted if they fall below the median of group1.\nestimation-strategy (optional keyword): The strategy used to estimate the median of group1. Defaults to :legacy. See median or quantile for available strategies (e.g., :r1 through :r9).\n\nReturns the calculated proportion as a double between 0.0 and 1.0.\nInterpretation:\n\nA value close to 0 means most values in group2 are greater than or equal to the median of group1.\nA value close to 0.5 means approximately half the values in group2 are below the median of group1.\nA value close to 1 means most values in group2 are less than the median of group1.\n\nNote: This measure is not symmetric. (cohens-u3 group1 group2) is generally not equal to (cohens-u3 group2 group1).\nThis is a non-parametric measure, suitable for any data samples, and does not assume normality, unlike cohens-u3-normal.\nSee also cohens-u3-normal (the version applicable to normal data), cohens-u2 (a related symmetric non-parametric measure), median, quantile.\nsource\n\n\n\ncohens-u3-normal\n\n(cohens-u3-normal group1 group2)\n(cohens-u3-normal group1 group2 method)\n(cohens-u3-normal d)\n\nCalculates Cohen’s U3, a measure of overlap between two distributions assumed to be normal with equal variances.\nCohen’s U3 quantifies the proportion of scores in the lower-scoring group that fall below the mean of the higher-scoring group. It is calculated from Cohen’s d statistic (cohens-d) using the standard normal cumulative distribution function (\\(\\Phi\\)): U3 = Φ(d).\nThe measure is asymmetric: U3(group1, group2) is not necessarily equal to U3(group2, group1). The interpretation depends on which group is considered the ‘higher-scoring’ one based on the sign of d. By convention, the result often represents the proportion of the first group (group1) that is below the mean of the second group (group2) if d is negative, or the proportion of the second group (group2) that is below the mean of the first group (group1) if d is positive.\nParameters:\n\ngroup1 (seq of numbers): The first sample.\ngroup2 (seq of numbers): The second sample.\nmethod (optional keyword): Specifies the method for calculating the pooled standard deviation used in the underlying cohens-d calculation. Possible values are :unbiased (default), :biased, or :avg. See pooled-stddev for details.\nd (double): A pre-calculated Cohen’s d value. If provided, group1, group2, and method are ignored.\n\nReturns the calculated Cohen’s U3 as a double [0.0, 1.0]. A value close to 0.5 suggests significant overlap. Values closer to 0 or 1 suggest less overlap (greater separation between the means).\nAssumptions: - Both samples are drawn from normally distributed populations. - The populations have equal variances (homoscedasticity).\nSee also cohens-d, cohens-u1-normal, cohens-u2-normal, p-overlap (a non-parametric overlap measure).\nsource\n\n\n\ncohens-w\n\n(cohens-w group1 group2)\n(cohens-w contingency-table)\n\nCalculates Cohen’s W effect size for the association between two nominal variables represented in a contingency table.\nCohen’s W is a measure of association derived from the Pearson’s Chi-squared statistic. It quantifies the magnitude of the difference between the observed frequencies and the frequencies expected under the assumption of independence between the variables.\nIts value ranges from 0 upwards: - A value of 0 indicates no association between the variables. - Larger values indicate a stronger association.\nThe function can be called in two ways:\n\nWith two sequences group1 and group2: The function will automatically construct a contingency table from the unique values in the sequences.\nWith a contingency table: The contingency table can be provided as:\n\nA map where keys are [row-index, column-index] tuples and values are counts (e.g., {[0 0] 10, [0 1] 5, [1 0] 3, [1 1] 12}). This is the output format of contingency-table with two inputs.\nA sequence of sequences representing the rows of the table (e.g., [10 5] [3 12](#LOS-10 5] [3 12)). This is equivalent to rows-&gt;contingency-table.\n\n\nParameters:\n\ngroup1 (sequence): The first sequence of categorical data.\ngroup2 (sequence): The second sequence of categorical data. Must have the same length as group1.\ncontingency-table (map or sequence of sequences): A pre-computed contingency table.\n\nReturns the calculated Cohen’s W coefficient as a double.\nSee also chisq-test, cramers-v, cramers-c, tschuprows-t, contingency-table.\nsource\n\n\n\nconfusion-matrix\n\n(confusion-matrix tp fn fp tn)\n(confusion-matrix confusion-mat)\n(confusion-matrix actual prediction)\n(confusion-matrix actual prediction encode-true)\n\nCreates a 2x2 confusion matrix for binary classification.\nA confusion matrix summarizes the results of a binary classification problem, showing the counts of True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN).\nTP: Actual is True, Predicted is True FP: Actual is False, Predicted is True (Type I error) FN: Actual is True, Predicted is False (Type II error) TN: Actual is False, Predicted is False\nThe function supports several input formats:\n\n(confusion-matrix tp fn fp tn): Direct input of the four counts.\n\ntp (long): True Positive count.\nfn (long): False Negative count.\nfp (long): False Positive count.\ntn (long): True Negative count.\n\n(confusion-matrix confusion-matrix-representation): Input as a structured representation.\n\nconfusion-matrix-representation: Can be:\n\nA map with keys like :tp, :fn, :fp, :tn (e.g., {:tp 10 :fn 2 :fp 5 :tn 80}).\nA sequence of sequences representing rows [TP FP] [FN TN](#LOS-TP FP] [FN TN) (e.g., [10 5] [2 80](#LOS-10 5] [2 80)).\nA flat sequence [TP FN FP TN] (e.g., [10 2 5 80]).\n\n\n(confusion-matrix actual prediction): Input as two sequences of outcomes.\n\nactual (sequence): Sequence of true outcomes.\nprediction (sequence): Sequence of predicted outcomes. Must have the same length as actual. Values in actual and prediction are compared element-wise. By default, any non-nil or non-zero value is treated as true, and nil or 0.0 is treated as false.\n\n(confusion-matrix actual prediction encode-true): Input as two sequences with a specified encoding for true.\n\nactual, prediction: Sequences as in the previous arity.\nencode-true: Specifies how values in actual and prediction are converted to boolean true or false.\n\nnil (default): Non-nil/non-zero is true.\nAny sequence/set: Values found in this collection are true.\nA map: Values are mapped according to the map; if a key is not found or maps to false, the value is false.\nA predicate function: Returns true if the value satisfies the predicate.\n\n\n\nReturns a map with keys :tp, :fn, :fp, and :tn representing the counts.\nThis function is commonly used to prepare input for binary classification metrics like those provided by binary-measures-all and binary-measures.\nsource\n\n\n\ncontingency-2x2-measures\n\n(contingency-2x2-measures & args)\n\nCalculates a subset of common statistics and measures for a 2x2 contingency table.\nThis function provides a selection of the most frequently used measures from the more comprehensive contingency-2x2-measures-all.\nThe function accepts the same input formats as contingency-2x2-measures-all:\n\n(contingency-2x2-measures a b c d): Takes the four counts as arguments.\n(contingency-2x2-measures [a b c d]): Takes a sequence of the four counts.\n(contingency-2x2-measures [a b] [c d](#LOS-a b] [c d)): Takes a sequence of sequences representing the rows.\n(contingency-2x2-measures {:a a :b b :c c :d d}): Takes a map of counts (accepts :a/:b/:c/:d keys).\n\nParameters:\n\na, b, c, d (long): Counts in the 2x2 table cells.\nmap-or-seq (map or sequence): A representation of the 2x2 table.\n\nReturns a map containing a selection of measures:\n\n:OR: Odds Ratio (Odds Ratio)\n:chi2: Pearson’s Chi-squared statistic\n:yates: Yates’ continuity corrected Chi-squared statistic\n:cochran-mantel-haenszel: Cochran-Mantel-Haenszel statistic\n:cohens-kappa: Cohen’s Kappa coefficient\n:yules-q: Yule’s Q measure of association\n:holley-guilfords-g: Holley-Guilford’s G measure\n:huberts-gamma: Hubert’s Gamma measure\n:yules-y: Yule’s Y measure of association\n:cramers-v: Cramer’s V measure of association\n:phi: Phi coefficient (Matthews Correlation Coefficient)\n:scotts-pi: Scott’s Pi measure of agreement\n:cohens-h: Cohen’s H measure\n:PCC: Pearson’s Contingency Coefficient\n:PCC-adjusted: Adjusted Pearson’s Contingency Coefficient\n:TCC: Tschuprow’s Contingency Coefficient\n:F1: F1 Score\n:bangdiwalas-b: Bangdiwala’s B statistic\n:mcnemars-chi2: McNemar’s Chi-squared test statistic\n:gwets-ac1: Gwet’s AC1 measure\n\nFor a more comprehensive set of 2x2 measures and their detailed descriptions, see contingency-2x2-measures-all.\nsource\n\n\n\ncontingency-2x2-measures-all\n\n(contingency-2x2-measures-all a b c d)\n(contingency-2x2-measures-all map-or-seq)\n(contingency-2x2-measures-all [a b] [c d])\n\nCalculates a comprehensive set of statistics and measures for a 2x2 contingency table.\nA 2x2 contingency table cross-tabulates two categorical variables, each with two levels. The table counts are typically represented as:\n+—+—+ | a | b | +—+—+ | c | d | +—+—+\nWhere a, b, c, d are the counts in the respective cells.\nThis function calculates numerous measures, including:\n\nChi-squared statistics (Pearson, Yates’ corrected, CMH) and their p-values.\nMeasures of association (Phi, Yule’s Q, Holley-Guilford’s G, Hubert’s Gamma, Yule’s Y, Cramer’s V, Scott’s Pi, Cohen’s H, Pearson/Tschuprow’s CC).\nMeasures of agreement (Cohen’s Kappa).\nRisk and effect size measures (Odds Ratio (OR), Relative Risk (RR), Risk Difference (RD), NNT, etc.).\nTable marginals and proportions.\n\nThe function can be called with the four counts directly or with a representation of the contingency table:\n\n(contingency-2x2-measures-all a b c d): Takes the four counts as arguments.\n(contingency-2x2-measures-all [a b c d]): Takes a sequence of the four counts.\n(contingency-2x2-measures-all [a b] [c d](#LOS-a b] [c d)): Takes a sequence of sequences representing the rows.\n(contingency-2x2-measures-all {:a a :b b :c c :d d}): Takes a map of counts (accepts :a/:b/:c/:d keys).\n\nParameters:\n\na (long): Count in the top-left cell.\nb (long): Count in the top-right cell.\nc (long): Count in the bottom-left cell.\nd (long): Count in the bottom-right cell.\nmap-or-seq (map or sequence): A representation of the 2x2 table as described above.\n\nReturns a map containing a wide range of calculated statistics. Keys include: :n, :table, :expected, :marginals, :proportions, :p-values (map), :OR, :lOR, :RR, :risk (map), :SE, :measures (map).\nSee also contingency-2x2-measures for a selected subset of these measures, mcc for the Matthews Correlation Coefficient (Phi), and binary-measures-all for metrics derived from a confusion matrix (often a 2x2 table in binary classification).\nsource\n\n\n\ncontingency-table\n\n(contingency-table & seqs)\n\nCreates a frequency map (contingency table) from one or more sequences.\nIf one sequence xs is provided, it returns a simple frequency map of the values in xs.\nIf multiple sequences s1, s2, ..., sn are provided, it creates a contingency table of the tuples formed by the corresponding elements [s1_i, s2_i, ..., sn_i] at each index i. The returned map keys are these tuples, and values are their frequencies.\nParameters:\n\nseqs (one or more sequences): The input sequences. All sequences should ideally have the same length, as elements are paired by index.\n\nReturns a map where keys represent unique combinations of values (or single values if only one sequence is input) and values are the counts of these combinations.\nSee also rows-&gt;contingency-table, contingency-table-&gt;marginals.\nsource\n\n\n\ncontingency-table-&gt;marginals\n\n(contingency-table-&gt;marginals ct)\n\nCalculates marginal sums (row and column totals) and the grand total from a contingency table.\nA contingency table represents the frequency distribution of observations for two or more categorical variables. This function summarizes these frequencies along the rows and columns.\nThe function accepts two main input formats for the contingency table:\n\nA map where keys are [row-index, column-index] tuples and values are counts (e.g., {[0 0] 10, [0 1] 5, [1 0] 3, [1 1] 12}). This format is produced by contingency-table when given multiple sequences or by rows-&gt;contingency-table.\nA sequence of sequences representing the rows of the table, where each inner sequence contains counts for the columns in that row (e.g., [10 5] [3 12](#LOS-10 5] [3 12)). The function internally converts this format to the map format.\n\nParameters:\n\nct (map or sequence of sequences): The contingency table input.\n\nReturns a map containing:\n\n:rows: A sequence of [row-index, row-total] pairs.\n:cols: A sequence of [column-index, column-total] pairs.\n:n: The grand total of all counts in the table.\n:diag: A sequence of [[index, index], count] pairs for cells on the diagonal (where row index equals column index). This is useful for square tables like confusion matrices.\n\nSee also contingency-table, rows-&gt;contingency-table.\nsource\n\n\n\ncorrelation\n\n(correlation [vs1 vs2])\n(correlation vs1 vs2)\n\nCalculates the correlation coefficient between two sequences.\nBy default, this function calculates the Pearson product-moment correlation coefficient, which measures the linear relationship between two datasets.\nThis function handles the standard deviation normalization based on whether the inputs vs1 and vs2 are treated as samples or populations (it uses sample standard deviation derived from variance).\nParameters:\n\n[vs1 vs2] (sequence of two sequences): A sequence containing the two sequences of numbers.\nvs1, vs2 (sequences): The two sequences of numbers directly as arguments.\n\nBoth sequences must have the same length.\nReturns the calculated correlation coefficient (a value between -1.0 and 1.0) as a double. Returns NaN if one or both sequences have zero variance (are constant).\nSee also covariance, pearson-correlation, spearman-correlation, kendall-correlation, correlation-matrix.\nsource\n\n\n\ncorrelation-matrix\n\n(correlation-matrix vss)\n(correlation-matrix vss measure)\n\nGenerates a matrix of pairwise correlation coefficients from a sequence of sequences.\nGiven a collection of data sequences vss, where each inner sequence represents a variable, this function calculates a square matrix where the element at row i and column j is the correlation coefficient between the i-th and j-th sequences in vss.\nParameters:\n\nvss (sequence of sequences of numbers): The collection of data sequences. Each inner sequence is treated as a variable. All inner sequences must have the same length.\nmeasure (keyword, optional): Specifies the type of correlation coefficient to calculate. Defaults to :pearson.\n\n:pearson (default): Calculates the Pearson product-moment correlation coefficient.\n:kendall: Calculates Kendall’s Tau rank correlation coefficient.\n:spearman: Calculates Spearman’s rank correlation coefficient.\n\n\nReturns a sequence of sequences (a matrix) of doubles representing the correlation matrix. The matrix is symmetric, as correlation is a symmetric measure.\nSee also pearson-correlation, spearman-correlation, kendall-correlation, covariance-matrix, coefficient-matrix.\nsource\n\n\n\ncount=\n\n(count= [vs1 vs2-or-val])\n(count= vs1 vs2-or-val)\n\nCount equal values in both seqs. Same as L0\nCalculates the number of pairs of corresponding elements that are equal between two sequences, or between a sequence and a single scalar value.\nParameters:\n\nvs1 (sequence of numbers): The first sequence.\nvs2-or-val (sequence of numbers or single number): The second sequence of numbers, or a single number to compare against each element of vs1.\n\nIf both inputs are sequences, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the count of equal elements as a long integer.\nsource\n\n\n\ncovariance\n\n(covariance [vs1 vs2])\n(covariance vs1 vs2)\n\nCovariance of two sequences.\nThis function calculates the sample covariance.\nParameters:\n\n[vs1 vs2] (sequence of two sequences): A sequence containing the two sequences of numbers.\nvs1, vs2 (sequences): The two sequences of numbers directly as arguments.\n\nBoth sequences must have the same length.\nReturns the calculated sample covariance as a double.\nSee also correlation, covariance-matrix.\nsource\n\n\n\ncovariance-matrix\n\n(covariance-matrix vss)\n\nGenerates a matrix of pairwise covariance coefficients from a sequence of sequences.\nGiven a collection of data sequences vss, where each inner sequence represents a variable, this function calculates a square matrix where the element at row i and column j is the sample covariance between the i-th and j-th sequences in vss.\nParameters:\n\nvss (sequence of sequences of numbers): The collection of data sequences. Each inner sequence is treated as a variable. All inner sequences must have the same length.\n\nReturns a sequence of sequences (a matrix) of doubles representing the covariance matrix. The matrix is symmetric, as covariance is a symmetric measure (\\(Cov(X,Y) = Cov(Y,X)\\)).\nInternally uses coefficient-matrix with the covariance function and symmetric? set to true.\nSee also covariance, correlation-matrix, coefficient-matrix.\nsource\n\n\n\ncramers-c\n\n(cramers-c group1 group2)\n(cramers-c contingency-table)\n\nCalculates Cramer’s C, a measure of association (effect size) between two nominal variables represented in a contingency table.\nIts value ranges from 0 to 1, where 0 indicates no association and 1 indicates a perfect association. It is particularly useful for tables larger than 2x2.\nThe function can be called in two ways:\n\nWith two sequences group1 and group2: The function will automatically construct a contingency table from the unique values in the sequences.\nWith a contingency table: The contingency table can be provided as:\n\nA map where keys are [row-index, column-index] tuples and values are counts (e.g., {[0 0] 10, [0 1] 5, [1 0] 3, [1 1] 12}). This is the output format of contingency-table with two inputs.\nA sequence of sequences representing the rows of the table (e.g., [10 5] [3 12](#LOS-10 5] [3 12)). This is equivalent to rows-&gt;contingency-table.\n\n\nParameters:\n\ngroup1 (sequence): The first sequence of categorical data.\ngroup2 (sequence): The second sequence of categorical data. Must have the same length as group1.\ncontingency-table (map or sequence of sequences): A pre-computed contingency table.\n\nReturns the calculated Cramer’s C coefficient as a double.\nSee also chisq-test, cramers-v, cohens-w, tschuprows-t, contingency-table.\nsource\n\n\n\ncramers-v\n\n(cramers-v group1 group2)\n(cramers-v contingency-table)\n\nCalculates Cramer’s V, a measure of association (effect size) between two nominal variables represented in a contingency table.\nIts value ranges from 0 to 1, where 0 indicates no association and 1 indicates a perfect association. It is related to the Pearson’s Chi-squared statistic and is useful for tables of any size.\nThe function can be called in two ways:\n\nWith two sequences group1 and group2: The function will automatically construct a contingency table from the unique values in the sequences.\nWith a contingency table: The contingency table can be provided as:\n\nA map where keys are [row-index, column-index] tuples and values are counts (e.g., {[0 0] 10, [0 1] 5, [1 0] 3, [1 1] 12}). This is the output format of contingency-table with two inputs.\nA sequence of sequences representing the rows of the table (e.g., [10 5] [3 12](#LOS-10 5] [3 12)). This is equivalent to rows-&gt;contingency-table.\n\n\nParameters:\n\ngroup1 (sequence): The first sequence of categorical data.\ngroup2 (sequence): The second sequence of categorical data. Must have the same length as group1.\ncontingency-table (map or sequence of sequences): A pre-computed contingency table.\n\nReturns the calculated Cramer’s V coefficient as a double.\nSee also chisq-test, cramers-c, cohens-w, tschuprows-t, contingency-table.\nsource\n\n\n\ncramers-v-corrected\n\n(cramers-v-corrected group1 group2)\n(cramers-v-corrected contingency-table)\n\nCalculates the corrected Cramer’s V, a measure of association (effect size) between two nominal variables represented in a contingency table, with a correction to reduce bias, particularly for small sample sizes or tables with many cells having small expected counts.\nLike the uncorrected Cramer’s V (cramers-v), its value ranges from 0 to 1, where 0 indicates no association and 1 indicates a perfect association. The correction tends to yield a value closer to the true population value in biased situations.\nThe function can be called in two ways:\n\nWith two sequences group1 and group2: The function will automatically construct a contingency table from the unique values in the sequences.\nWith a contingency table: The contingency table can be provided as:\n\nA map where keys are [row-index, column-index] tuples and values are counts (e.g., {[0 0] 10, [0 1] 5, [1 0] 3, [1 1] 12}). This is the output format of contingency-table with two inputs.\nA sequence of sequences representing the rows of the table (e.g., [10 5] [3 12](#LOS-10 5] [3 12)). This is equivalent to rows-&gt;contingency-table.\n\n\nParameters:\n\ngroup1 (sequence): The first sequence of categorical data.\ngroup2 (sequence): The second sequence of categorical data. Must have the same length as group1.\ncontingency-table (map or sequence of sequences): A pre-computed contingency table.\n\nReturns the calculated corrected Cramer’s V coefficient as a double.\nSee also chisq-test, cramers-v (uncorrected), cramers-c, cohens-w, tschuprows-t, contingency-table.\nsource\n\n\n\ncressie-read-test\n\n(cressie-read-test contingency-table-or-xs)\n(cressie-read-test contingency-table-or-xs params)\n\nCressie-Read test, a power divergence test for lambda 2/3\nPerforms a power divergence test, which encompasses several common statistical tests like Chi-squared, G-test (likelihood ratio), etc., based on the lambda parameter. This function can perform either a goodness-of-fit test or a test for independence in a contingency table.\nUsage:\n\nGoodness-of-Fit (GOF):\n\nInput: observed-counts (sequence of numbers) and :p (expected probabilities/weights).\nInput: data (sequence of numbers) and :p (a distribution object). In this case, a histogram of data is created (controlled by :bins) and compared against the probability mass/density of the distribution in those bins.\n\nTest for Independence:\n\nInput: contingency-table (2D sequence or map format). The :p option is ignored.\n\n\nOptions map:\n\n:lambda (double, default: 2/3): Determines the specific test statistic. Common values:\n\n1.0: Pearson Chi-squared test (chisq-test).\n0.0: G-test / Multinomial Likelihood Ratio test (multinomial-likelihood-ratio-test).\n-0.5: Freeman-Tukey test (freeman-tukey-test).\n-1.0: Minimum Discrimination Information test (minimum-discrimination-information-test).\n-2.0: Neyman Modified Chi-squared test (neyman-modified-chisq-test).\n2/3: Cressie-Read test (default, cressie-read-test).\n\n:p (seq of numbers or distribution): Expected probabilities/weights (for GOF with counts) or a fastmath.random distribution object (for GOF with data). Ignored for independence tests.\n:alpha (double, default: 0.05): Significance level for confidence intervals.\n:ci-sides (keyword, default: :two-sided): Sides for bootstrap confidence intervals (:two-sided, :one-sided-greater, :one-sided-less).\n:sides (keyword, default: :one-sided-greater): Alternative hypothesis side for the p-value calculation against the Chi-squared distribution (:one-sided-greater, :one-sided-less, :two-sided).\n:bootstrap-samples (long, default: 1000): Number of bootstrap samples for confidence interval estimation.\n:ddof (long, default: 0): Delta degrees of freedom. Adjustment subtracted from the calculated degrees of freedom.\n:bins (number, keyword, or seq): Used only for GOF test against a distribution. Specifies the number of bins, an estimation method (see histogram), or explicit bin edges for histogram creation.\n\nReturns a map containing:\n\n:stat: The calculated power divergence test statistic.\n:chi2: Alias for :stat.\n:df: Degrees of freedom for the test.\n:p-value: The p-value associated with the test statistic.\n:n: Total number of observations.\n:estimate: Observed proportions.\n:expected: Expected counts or proportions under the null hypothesis.\n:confidence-interval: Bootstrap confidence intervals for the observed proportions.\n:lambda, :alpha, :sides, :ci-sides: Input options used.\n\nsource\n\n\n\ndemean\n\n(demean vs)\n\nSubtract mean from a sequence\nsource\n\n\n\ndissimilarity\n\n(dissimilarity method P-observed Q-expected)\n(dissimilarity method P-observed Q-expected {:keys [bins probabilities? epsilon log-base power remove-zeros?], :or {probabilities? true, epsilon 1.0E-6, log-base m/E, power 2.0}})\n\nVarious PDF distance between two histograms (frequencies) or probabilities.\nQ can be a distribution object. Then, histogram will be created out of P.\nArguments:\n\nmethod - distance method\nP-observed - frequencies, probabilities or actual data (when Q is a distribution of :bins is set)\nQ-expected - frequencies, probabilities or distribution object (when P is a data or :bins is set)\n\nOptions:\n\n:probabilities? - should P/Q be converted to a probabilities, default: true.\n:epsilon - small number which replaces 0.0 when division or logarithm is used`\n:log-base - base for logarithms, default: e\n:power - exponent for :minkowski distance, default: 2.0\n:bins - number of bins or bins estimation method, see histogram.\n\nThe list of methods: :euclidean, :city-block, :manhattan, :chebyshev, :minkowski, :sorensen, :gower, :soergel, :kulczynski, :canberra, :lorentzian, :non-intersection, :wave-hedges, :czekanowski, :motyka, :tanimoto, :jaccard, :dice, :bhattacharyya, :hellinger, :matusita, :squared-chord, :euclidean-sq, :squared-euclidean, :pearson-chisq, :chisq, :neyman-chisq, :squared-chisq, :symmetric-chisq, :divergence, :clark, :additive-symmetric-chisq, :kullback-leibler, :jeffreys, :k-divergence, :topsoe, :jensen-shannon, :jensen-difference, :taneja, :kumar-johnson, :avg\nSee more: Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions by Sung-Hyuk Cha\nsource\n\n\n\ndurbin-watson\n\n(durbin-watson rs)\n\nCalculates the Durbin-Watson statistic (d) for a sequence of residuals.\nThis statistic is used to test for the presence of serial correlation, especially first-order (lag-1) autocorrelation, in the residuals from a regression analysis. Autocorrelation violates the assumption of independent errors.\nParameters:\n\nrs (sequence of numbers): The sequence of residuals from a regression model. The sequence should represent observations ordered by time or sequence index.\n\nReturns the calculated Durbin-Watson statistic as a double. The value ranges from 0 to 4.\nInterpretation:\n\nValues near 2 suggest no first-order autocorrelation.\nValues less than 2 suggest positive autocorrelation (residuals tend to be followed by residuals of the same sign).\nValues greater than 2 suggest negative autocorrelation (residuals tend to be followed by residuals of the opposite sign).\n\nsource\n\n\n\nepsilon-sq\n\n(epsilon-sq [group1 group2])\n(epsilon-sq group1 group2)\n\nCalculates Epsilon squared (ε²), an effect size measure for the simple linear regression of group1 on group2.\nEpsilon squared estimates the proportion of variance in the dependent variable (group1) that is accounted for by the independent variable (group2) in the population. It is considered a less biased alternative to the sample R-squared (r2-determination).\nThe calculation is based on the sums of squares from the simple linear regression of group1 on group2.\nParameters:\n\ngroup1 (seq of numbers): The dependent variable.\ngroup2 (seq of numbers): The independent variable. Must have the same length as group1.\n\nReturns the calculated Epsilon squared value as a double. The value typically ranges from 0.0 to 1.0.\nInterpretation:\n\n0.0 indicates that group2 explains none of the variance in group1 in the population.\n1.0 indicates that group2 perfectly explains the variance in group1 in the population.\n\nNote: While often presented in the context of ANOVA, this implementation applies the formula to the sums of squares obtained from a simple linear regression between the two sequences.\nSee also eta-sq (Eta-squared, often based on \\(R^2\\)), omega-sq (another adjusted R²-like measure), r2-determination (R-squared).\nsource\n\n\n\nestimate-bins\n\n(estimate-bins vs)\n(estimate-bins vs bins-or-estimate-method)\n\nEstimate number of bins for histogram.\nPossible methods are: :sqrt :sturges :rice :doane :scott :freedman-diaconis (default).\nThe number returned is not higher than number of samples.\nsource\n\n\n\nestimation-strategies-list\nList of estimation strategies for percentile/quantile functions.\nsource\n\n\n\neta-sq\n\n(eta-sq [group1 group2])\n(eta-sq group1 group2)\n\nCalculates a measure of association between two sequences, named eta-sq (Eta-squared).\nNote: The current implementation calculates the R-squared coefficient of determination from a simple linear regression where the first input sequence (group1) is treated as the dependent variable and the second (group2) as the independent variable. In this context, it quantifies the proportion of the variance in group1 that is linearly predictable from group2.\nParameters:\n\ngroup1 (seq of numbers): The first sequence (treated as dependent variable).\ngroup2 (seq of numbers): The second sequence (treated as independent variable).\n\nReturns the calculated R-squared value as a double [0.0, 1.0].\nInterpretation:\n\n0.0 indicates that group2 explains none of the variance in group1 linearly.\n1.0 indicates that group2 linearly explains all the variance in group1.\n\nWhile Eta-squared (\\(\\eta^2\\)) is commonly used in ANOVA to quantify the proportion of variance in a dependent variable explained by group membership, this function’s calculation method differs from the standard ANOVA \\(\\eta^2\\) unless group2 explicitly represents numeric codes for two groups.\nSee also r2-determination (which is equivalent to this function), pearson-correlation, omega-sq, epsilon-sq, one-way-anova-test.\nsource\n\n\n\nexpectile\n\n(expectile vs tau)\n(expectile vs weights tau)\n\nCalculate the tau-th expectile of a sequence vs.\nExpectiles are related to quantiles but are determined by minimizing an asymmetrically weighted sum of squared differences, rather than absolute differences. The tau parameter controls the asymmetry.\nA key property is that the expectile for tau = 0.5 is equal to the mean.\nThe calculation involves finding the value t such that the weighted sum of w_i * (v_i - t) is zero, where the effective weights depend on tau and whether v_i is above or below t.\nParameters:\n\nvs: Sequence of data values.\nweights (optional): Sequence of corresponding non-negative weights. Must have the same count as vs. If omitted, calculates the unweighted expectile.\ntau: The expectile level, a value between 0.0 and 1.0 (inclusive).\n\nReturns the calculated expectile as a double.\nSee also quantile, mean, median.\nsource\n\n\n\nextent\n\n(extent vs)\n(extent vs mean?)\n\nReturn extent (min, max, mean) values from sequence. Mean is optional (default: true)\nsource\n\n\n\nf-test\n\n(f-test xs ys)\n(f-test xs ys {:keys [sides alpha], :or {sides :two-sided, alpha 0.05}})\n\nPerforms an F-test to compare the variances of two independent samples.\nThe test assesses the null hypothesis that the variances of the populations from which xs and ys are drawn are equal.\nAssumes independence of samples. The test is sensitive to departures from the assumption that both populations are normally distributed.\nParameters:\n\nxs (seq of numbers): The first sample.\nys (seq of numbers): The second sample.\nparams (map, optional): Options map:\n\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis regarding the ratio of variances (Var(xs) / Var(ys)).\n\n:two-sided (default): Variances are not equal (ratio != 1).\n:one-sided-greater: Variance of xs is greater than variance of ys (ratio &gt; 1).\n:one-sided-less: Variance of xs is less than variance of ys (ratio &lt; 1).\n\n:alpha (double, default 0.05): Significance level for the confidence interval.\n\n\nReturns a map containing:\n\n:F: The calculated F-statistic (ratio of sample variances: Var(xs) / Var(ys)).\n:stat: Alias for :F.\n:estimate: Alias for :F, representing the estimated ratio of variances.\n:df: Degrees of freedom as [numerator-df, denominator-df], corresponding to [(count xs)-1, (count ys)-1].\n:n: Sample sizes as [count xs, count ys].\n:nx: Sample size of xs.\n:ny: Sample size of ys.\n:sides: The alternative hypothesis side used (:two-sided, :one-sided-greater, or :one-sided-less).\n:test-type: Alias for :sides.\n:p-value: The p-value associated with the F-statistic and the specified :sides.\n:confidence-interval: A confidence interval for the true ratio of the population variances (Var(xs) / Var(ys)).\n\nsource\n\n\n\nfligner-killeen-test\n\n(fligner-killeen-test xss)\n(fligner-killeen-test xss {:keys [sides], :or {sides :one-sided-greater}})\n\nPerforms the Fligner-Killeen test for homogeneity of variances across two or more groups.\nThe Fligner-Killeen test is a non-parametric test that assesses the null hypothesis that the variances of the groups are equal. It is robust against departures from normality. The test is based on ranks of the absolute deviations from the group medians.\nParameters:\n\nxss (sequence of sequences): A collection where each element is a sequence representing a group of observations.\nparams (map, optional): Options map with the following key:\n\n:sides (keyword, default :one-sided-greater): Alternative hypothesis side for the Chi-squared test. Possible values: :one-sided-greater, :one-sided-less, :two-sided.\n\n\nReturns a map containing:\n\n:chi2: The Fligner-Killeen test statistic (Chi-squared value).\n:stat: Alias for :chi2.\n:p-value: The p-value for the test.\n:df: Degrees of freedom for the test (number of groups - 1).\n:n: Sequence of sample sizes for each group.\n:SSt: Sum of squares between groups (treatment) based on transformed ranks.\n:SSe: Sum of squares within groups (error) based on transformed ranks.\n:DFt: Degrees of freedom between groups.\n:DFe: Degrees of freedom within groups.\n:MSt: Mean square between groups.\n:MSe: Mean square within groups.\n:sides: Test side used.\n\nsource\n\n\n\nfreeman-tukey-test\n\n(freeman-tukey-test contingency-table-or-xs)\n(freeman-tukey-test contingency-table-or-xs params)\n\nFreeman-Tukey test, a power divergence test for lambda -0.5\nPerforms a power divergence test, which encompasses several common statistical tests like Chi-squared, G-test (likelihood ratio), etc., based on the lambda parameter. This function can perform either a goodness-of-fit test or a test for independence in a contingency table.\nUsage:\n\nGoodness-of-Fit (GOF):\n\nInput: observed-counts (sequence of numbers) and :p (expected probabilities/weights).\nInput: data (sequence of numbers) and :p (a distribution object). In this case, a histogram of data is created (controlled by :bins) and compared against the probability mass/density of the distribution in those bins.\n\nTest for Independence:\n\nInput: contingency-table (2D sequence or map format). The :p option is ignored.\n\n\nOptions map:\n\n:lambda (double, default: 2/3): Determines the specific test statistic. Common values:\n\n1.0: Pearson Chi-squared test (chisq-test).\n0.0: G-test / Multinomial Likelihood Ratio test (multinomial-likelihood-ratio-test).\n-0.5: Freeman-Tukey test (freeman-tukey-test).\n-1.0: Minimum Discrimination Information test (minimum-discrimination-information-test).\n-2.0: Neyman Modified Chi-squared test (neyman-modified-chisq-test).\n2/3: Cressie-Read test (default, cressie-read-test).\n\n:p (seq of numbers or distribution): Expected probabilities/weights (for GOF with counts) or a fastmath.random distribution object (for GOF with data). Ignored for independence tests.\n:alpha (double, default: 0.05): Significance level for confidence intervals.\n:ci-sides (keyword, default: :two-sided): Sides for bootstrap confidence intervals (:two-sided, :one-sided-greater, :one-sided-less).\n:sides (keyword, default: :one-sided-greater): Alternative hypothesis side for the p-value calculation against the Chi-squared distribution (:one-sided-greater, :one-sided-less, :two-sided).\n:bootstrap-samples (long, default: 1000): Number of bootstrap samples for confidence interval estimation.\n:ddof (long, default: 0): Delta degrees of freedom. Adjustment subtracted from the calculated degrees of freedom.\n:bins (number, keyword, or seq): Used only for GOF test against a distribution. Specifies the number of bins, an estimation method (see histogram), or explicit bin edges for histogram creation.\n\nReturns a map containing:\n\n:stat: The calculated power divergence test statistic.\n:chi2: Alias for :stat.\n:df: Degrees of freedom for the test.\n:p-value: The p-value associated with the test statistic.\n:n: Total number of observations.\n:estimate: Observed proportions.\n:expected: Expected counts or proportions under the null hypothesis.\n:confidence-interval: Bootstrap confidence intervals for the observed proportions.\n:lambda, :alpha, :sides, :ci-sides: Input options used.\n\nsource\n\n\n\ngeomean\n\n(geomean vs)\n(geomean vs weights)\n\nCalculates the geometric mean of a sequence vs.\nThe geometric mean is suitable for averaging ratios or rates of change and requires all values in the sequence to be positive. It is calculated as the n-th root of the product of n numbers.\nParameters:\n\nvs: Sequence of numbers. Non-positive values will result in NaN or 0.0 due to the internal use of log.\nweights (optional): Sequence of non-negative weights corresponding to vs. Must have the same count as vs.\n\nReturns the calculated geometric mean as a double.\nSee also mean, harmean, powmean.\nsource\n\n\n\nglass-delta\n\n(glass-delta [group1 group2])\n(glass-delta group1 group2)\n\nCalculates Glass’s delta (Δ), an effect size measure for the difference between two group means, using the standard deviation of the control group.\nGlass’s delta is used to quantify the magnitude of the difference between an experimental group and a control group, specifically when the control group’s standard deviation is considered a better estimate of the population standard deviation than a pooled variance.\nParameters:\n\ngroup1 (seq of numbers): The experimental group.\ngroup2 (seq of numbers): The control group.\n\nReturns the calculated Glass’s delta as a double.\nThis measure is less common than cohens-d or hedges-g but is preferred when the intervention is expected to affect the variance or when group2 (the control) is clearly the baseline against which variability should be assessed.\nSee also cohens-d, hedges-g.\nsource\n\n\n\nharmean\n\n(harmean vs)\n(harmean vs weights)\n\nCalculates the harmonic mean of a sequence vs.\nThe harmonic mean is the reciprocal of the arithmetic mean of the reciprocals of the observations.\nParameters:\n\nvs: Sequence of numbers. Values must be non-zero.\nweights (optional): Sequence of non-negative weights corresponding to vs. Must have the same count as vs.\n\nReturns the calculated harmonic mean as a double.\nSee also mean, geomean, powmean.\nsource\n\n\n\nhedges-g\n\n(hedges-g [group1 group2])\n(hedges-g group1 group2)\n\nCalculates Hedges’s g effect size for comparing the means of two independent groups.\nHedges’s g is a standardized measure quantifying the magnitude of the difference between the means of two independent groups. It is similar to Cohen’s d but uses the unbiased pooled standard deviation in the denominator.\nThis implementation calculates g using the unbiased pooled standard deviation as the denominator.\nParameters:\n\ngroup1, group2 (sequences): The two independent samples directly as arguments.\n\nReturns the calculated Hedges’s g effect size as a double.\nNote: This specific function uses the unbiased pooled standard deviation but does not apply the small-sample bias correction factor (often denoted as J) sometimes associated with Hedges’s g. For a bias-corrected version, see hedges-g-corrected. This function is equivalent to calling (cohens-d group1 group2 :unbiased).\nSee also cohens-d, hedges-g-corrected, glass-delta, pooled-stddev.\nsource\n\n\n\nhedges-g*\n\n(hedges-g* [group1 group2])\n(hedges-g* group1 group2)\n\nCalculates a less biased estimate of Hedges’s g effect size for comparing the means of two independent groups, using the exact J bias correction.\nHedges’s g is a standardized measure of the difference between two means. For small sample sizes, the standard Hedges’s g (and Cohen’s d) can overestimate the true population effect size. This function applies a specific correction factor, often denoted as J, to mitigate this bias.\nThe calculation involves: 1. Calculating the standard Hedges’s g (equivalent to hedges-g, which uses the unbiased pooled standard deviation). 2. Calculating the J correction factor based on the degrees of freedom (n1 + n2 - 2) using the gamma function. 3. Multiplying the standard Hedges’s g by the J factor.\nThe J factor is calculated as (Gamma(df/2) / (sqrt(df/2) * Gamma((df-1)/2))).\nParameters:\n\ngroup1 (seq of numbers): The first independent sample.\ngroup2 (seq of numbers): The second independent sample.\n\nReturns the calculated bias-corrected Hedges’s g effect size as a double.\nThis version of Hedges’s g is generally preferred over the standard version or Cohen’s d when working with small sample sizes, as it provides a more accurate estimate of the population effect size.\nAssumptions: - The two samples are independent. - Data within each group are approximately normally distributed. - Equal variances are assumed for calculating the pooled standard deviation.\nSee also cohens-d, hedges-g (uncorrected), hedges-g-corrected (another correction method).\nsource\n\n\n\nhedges-g-corrected\n\n(hedges-g-corrected [group1 group2])\n(hedges-g-corrected group1 group2)\n\nCalculates a small-sample bias-corrected effect size for comparing the means of two independent groups, often referred to as a form of Hedges’s g.\nThis function calculates Cohen’s d (cohens-d) using the unbiased pooled standard deviation (equivalent to hedges-g), and then applies a specific correction factor designed to reduce the bias in the effect size estimate for small sample sizes.\nThe correction factor applied is (1 - 3 / (4 * df - 1)), where df is the degrees of freedom for the unbiased pooled variance calculation (n1 + n2 - 2). This corresponds to calling cohens-d-corrected with the :unbiased method for pooled standard deviation.\nParameters:\n\ngroup1 (seq of numbers): The first independent sample.\ngroup2 (seq of numbers): The second independent sample.\n\nReturns the calculated bias-corrected effect size as a double.\nNote: This function applies a correction factor. For the more standard Hedges’s g bias correction using the exact gamma function based correction factor, see hedges-g*.\nSee also cohens-d, cohens-d-corrected, hedges-g, hedges-g*, pooled-stddev.\nsource\n\n\n\nhistogram\n\n(histogram vs)\n(histogram vs bins-or-estimate-method)\n(histogram vs bins-or-estimate-method [mn mx])\n(histogram vs bins-or-estimate-method mn mx)\n\nCalculate histogram.\nEstimation method can be a number, named method: :sqrt :sturges :rice :doane :scott :freedman-diaconis (default) or a sequence of points used as intervals. In the latter case or when mn and mx values are provided - data will be filtered to fit in desired interval(s).\nReturns map with keys:\n\n:size - number of bins\n:step - average distance between bins\n:bins - seq of pairs of range lower value and number of elements\n:min - min value\n:max - max value\n:samples - number of used samples\n:frequencies - a map containing counts for bin’s average\n:intervals - intervals used to create bins\n:bins-maps - seq of maps containing:\n\n:min - lower bound\n:max - upper bound\n:step - actual distance between bins\n:count - number of elements\n:avg - average value\n:probability - probability for bin\n\n\nIf difference between min and max values is 0, number of bins is set to 1.\nsource\n\n\n\nhpdi-extent\n\n(hpdi-extent vs)\n(hpdi-extent vs size)\n\nHigher Posterior Density interval + median.\nsize parameter is the target probability content of the interval.\nsource\n\n\n\ninner-fence-extent\n\n(inner-fence-extent vs)\n(inner-fence-extent vs estimation-strategy)\n\nReturns LIF, UIF and median\nsource\n\n\n\niqr\n\n(iqr vs)\n(iqr vs estimation-strategy)\n\nInterquartile range.\nsource\n\n\n\njarque-bera-test\n\n(jarque-bera-test xs)\n(jarque-bera-test xs params)\n(jarque-bera-test xs skew kurt {:keys [sides], :or {sides :one-sided-greater}})\n\nPerforms the Jarque-Bera goodness-of-fit test to determine if sample data exhibits skewness and kurtosis consistent with a normal distribution.\nThe test assesses the null hypothesis that the data comes from a normally distributed population (i.e., population skewness is 0 and population excess kurtosis is 0).\nThe test statistic is calculated as: JB = (n/6) * (S^2 + (1/4)*K^2) where n is the sample size, S is the sample skewness (using :g1 type), and K is the excess kurtosis :g2. Under the null hypothesis, the JB statistic asymptotically follows a Chi-squared distribution with 2 degrees of freedom.\nParameters:\n\nxs (seq of numbers): The sample data.\nskew (double, optional): A pre-calculated sample skewness value (type :g1). If omitted, it’s calculated from xs.\nkurt (double, optional): A pre-calculated sample excess kurtosis value (type :g2). If omitted, it’s calculated from xs.\nparams (map, optional): Options map:\n\n:sides (keyword, default :one-sided-greater): Specifies the side(s) of the Chi-squared(2) distribution used for p-value calculation.\n\n:one-sided-greater (default and standard for JB): Tests if the JB statistic is significantly large, indicating departure from normality.\n:one-sided-less: Tests if the statistic is significantly small.\n:two-sided: Tests if the statistic is extreme in either tail.\n\n\n\nReturns a map containing:\n\n:Z: The calculated Jarque-Bera test statistic (labeled :Z for consistency, though it follows Chi-squared(2)).\n:stat: Alias for :Z.\n:p-value: The p-value associated with the test statistic and :sides, derived from the Chi-squared(2) distribution.\n:skewness: The sample skewness (type :g1) used in the calculation.\n:kurtosis: The sample kurtosis (type :g2) used in the calculation.\n\nSee also skewness-test, kurtosis-test, normality-test, bonett-seier-test.\nsource\n\n\n\njensen-shannon-divergence DEPRECATED\nDeprecated: Use dissimilarity.\n\n(jensen-shannon-divergence [vs1 vs2])\n(jensen-shannon-divergence vs1 vs2)\n\nJensen-Shannon divergence of two sequences.\nsource\n\n\n\nkendall-correlation\n\n(kendall-correlation [vs1 vs2])\n(kendall-correlation vs1 vs2)\n\nCalculates Kendall’s rank correlation coefficient (Kendall’s Tau) between two sequences.\nKendall’s Tau is a non-parametric statistic used to measure the ordinal association between two measured quantities. It assesses the degree of similarity between the orderings of data when ranked by each of the quantities.\nThe coefficient value ranges from -1.0 (perfect disagreement in ranking) to 1.0 (perfect agreement in ranking), with 0.0 indicating no monotonic relationship. Unlike Pearson correlation, it does not require the relationship to be linear.\nParameters:\n\n[vs1 vs2] (sequence of two sequences): A sequence containing the two sequences of numbers.\nvs1, vs2 (sequences): The two sequences of numbers directly as arguments.\n\nBoth input sequences must contain only numbers and must have the same length.\nReturns the calculated Kendall’s Tau coefficient as a double.\nSee also pearson-correlation, spearman-correlation, correlation.\nsource\n\n\n\nkruskal-test\n\n(kruskal-test xss)\n(kruskal-test xss {:keys [sides], :or {sides :right}})\n\nPerforms the Kruskal-Wallis H-test (rank sum test) for independent samples.\nThe Kruskal-Wallis test is a non-parametric alternative to one-way ANOVA. It determines whether there is a statistically significant difference between the distributions of two or more independent groups. It does not assume normality but requires that distributions have a similar shape for the test to be valid.\nParameters:\n\ndata-groups (vector of sequences): A collection where each element is a sequence representing a group of observations.\na map containing :sides key with values of: :right (default), :left or :both\n\nReturns a map containing:\n\n:stat: The Kruskal-Wallis H statistic.\n:n: Total number of observations across all groups.\n:df: Degrees of freedom (number of groups - 1).\n:k: Number of groups.\n:sides: Test side\n:p-value: The p-value for the test (null hypothesis: all groups have the same distribution).\n\nsource\n\n\n\nks-test-one-sample\n\n(ks-test-one-sample xs)\n(ks-test-one-sample xs distribution-or-ys)\n(ks-test-one-sample xs distribution-or-ys {:keys [sides kernel bandwidth distinct?], :or {sides :two-sided, kernel :gaussian, distinct? true}})\n\nPerforms the one-sample Kolmogorov-Smirnov (KS) test.\nThis test compares the empirical cumulative distribution function (ECDF) of a sample xs against a specified theoretical distribution or the ECDF of another empirical sample. It assesses the null hypothesis that xs is drawn from the reference distribution.\nParameters:\n\nxs (seq of numbers): The sample data to be tested.\ndistribution-or-ys (optional):\n\nA fastmath.random distribution object to test against. If omitted, defaults to the standard normal distribution (fastmath.random/default-normal).\nA sequence of numbers (ys). In this case, an empirical distribution is estimated from ys using Kernel Density Estimation (KDE) or an enumerated distribution (see :kernel option).\n\nopts (map, optional): Options map:\n\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis regarding the difference between the ECDF of xs and the reference CDF.\n\n:two-sided (default): Tests if the ECDF of xs is different from the reference CDF.\n:right: Tests if the ECDF of xs is significantly below the reference CDF (i.e., xs tends to have larger values, stochastically greater).\n:left: Tests if the ECDF of xs is significantly above the reference CDF (i.e., xs tends to have smaller values, stochastically smaller).\n\n:kernel (keyword, default :gaussian): Used only when distribution-or-ys is a sequence. Specifies the method to estimate the empirical distribution:\n\n:gaussian (or other KDE kernels): Uses Kernel Density Estimation.\n:enumerated: Creates a discrete empirical distribution from ys.\n\n:bandwidth (double, optional): Bandwidth for KDE (if applicable).\n:distinct? (boolean or keyword, default true): How to handle duplicate values in xs.\n\ntrue (default): Removes duplicate values from xs before computation.\nfalse: Uses all values in xs, including duplicates.\n:jitter: Adds a small amount of random noise to each value in xs to break ties.\n\n\n\nReturns a map containing:\n\n:n: Sample size of xs (after applying :distinct?).\n:dp: Maximum positive difference (ECDF(xs) - CDF(ref)).\n:dn: Maximum positive difference (CDF(ref) - ECDF(xs)).\n:d: The KS test statistic (max absolute difference: max(dp, dn)).\n:stat: The specific statistic used for p-value calculation, depending on :sides (d, dp, or dn).\n:p-value: The p-value associated with the test statistic and the specified :sides.\n:sides: The alternative hypothesis side used.\n\nsource\n\n\n\nks-test-two-samples\n\n(ks-test-two-samples xs ys)\n(ks-test-two-samples xs ys {:keys [method sides distinct? correct?], :or {sides :two-sided, distinct? :ties, correct? true}})\n\nPerforms the two-sample Kolmogorov-Smirnov (KS) test.\nThis test compares the empirical cumulative distribution functions (ECDFs) of two independent samples, xs and ys, to assess the null hypothesis that they are drawn from the same continuous distribution.\nParameters:\n\nxs (seq of numbers): The first sample.\nys (seq of numbers): The second sample.\nopts (map, optional): Options map:\n\n:method (keyword, optional): Specifies the calculation method for the p-value.\n\n:exact: Attempts an exact calculation (suitable for small samples, sensitive to ties). Default if nx * ny &lt; 10000.\n:approximate: Uses the asymptotic Kolmogorov distribution (suitable for larger samples). Default otherwise.\n\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis.\n\n:two-sided (default): Tests if the distributions differ (ECDFs are different).\n:right: Tests if xs is stochastically greater than ys (ECDF(xs) is below ECDF(ys)).\n:left: Tests if xs is stochastically smaller than ys (ECDF(xs) is above ECDF(ys)).\n\n:distinct? (keyword or boolean, default :ties): How to handle duplicate values (ties).\n\n:ties (default): Includes all points. Passes information about ties to the :exact calculation method. Accuracy depends on the exact method’s tie handling.\n:jitter: Adds a small amount of random noise to break ties before comparison. A practical approach if exact tie handling is complex or not required.\ntrue: Applies distinct to xs and ys separately before combining. May not resolve all ties between the combined samples.\nfalse: Uses the data as-is, without attempting to handle ties explicitly (may lead to less accurate p-values, especially with the exact method).\n\n:correct? (boolean, default true): Apply continuity correction when using the :exact calculation method for a more accurate p-value especially for smaller sample sizes.\n\n\nReturns a map containing:\n\n:nx: Number of observations in xs (after :distinct? processing if applicable).\n:ny: Number of observations in ys (after :distinct? processing if applicable).\n:n: Effective sample size used for asymptotic calculation (nx*ny / (nx+ny)).\n:dp: Maximum positive difference (ECDF(xs) - ECDF(ys)).\n:dn: Maximum positive difference (ECDF(ys) - ECDF(xs)).\n:d: The KS test statistic (max absolute difference: max(dp, dn)).\n:stat: The specific statistic used for p-value calculation (d, dp, or dn for exact; scaled version for approximate).\n:KS: Alias for :stat.\n:p-value: The p-value associated with the test statistic and :sides.\n:sides: The alternative hypothesis side used.\n:method: The calculation method used (:exact or :approximate).\n\nNote on Ties: The KS test is strictly defined for continuous distributions where ties have zero probability. The presence of ties in sample data affects the p-value calculation. The :distinct? option provides ways to manage this, with :jitter being a common pragmatic choice.\nsource\n\n\n\nkullback-leibler-divergence DEPRECATED\nDeprecated: Use dissimilarity.\n\n(kullback-leibler-divergence [vs1 vs2])\n(kullback-leibler-divergence vs1 vs2)\n\nKullback-Leibler divergence of two sequences.\nsource\n\n\n\nkurtosis\n\n(kurtosis vs)\n(kurtosis vs typ)\n\nCalculates the kurtosis of a sequence, a measure of the ‘tailedness’ or ‘peakedness’ of the distribution compared to a normal distribution.\nParameters:\n\nvs (seq of numbers): The input sequence.\ntyp (keyword or sequence, optional): Specifies the type of kurtosis measure to calculate. Different types use different algorithms and may have different expected values under normality (e.g., 0 or 3). Defaults to :G2.\n\nAvailable typ values:\n\n:G2 (Default): Sample kurtosis based on the fourth standardized moment, as implemented by Apache Commons Math Kurtosis. Its value approaches 3 for a large normal sample, but the exact expected value depends on sample size.\n:g2 or :excess: Sample excess kurtosis. This is calculated from :G2 and adjusted for sample bias, such that the expected value for a normal distribution is approximately 0.\n:kurt: Kurtosis definition where normal = 3. Calculated as :g2 + 3.\n:b2: Kurtosis defined as fourth moment divided by standard deviation to the power of 4\n:geary: Geary’s ‘g’, a robust measure calculated as mean_abs_deviation / population_stddev. Expected value for normal is sqrt(2/pi) ≈ 0.798. Lower values indicate leptokurtosis.\n:moors: Moors’ robust kurtosis measure based on octiles. The implementation returns a centered version where the expected value for normal is 0.\n:crow: Crow-Siddiqui robust kurtosis measure based on quantiles. The implementation returns a centered version where the expected value for normal is 0. Can accept parameters alpha and beta via sequential type [:crow alpha beta].\n:hogg: Hogg’s robust kurtosis measure based on trimmed means. The implementation returns a centered version where the expected value for normal is 0. Can accept parameters alpha and beta via sequential type [:hogg alpha beta].\n:l-kurtosis: L-kurtosis (τ₄), the ratio of the 4th L-moment (λ₄) to the 2nd L-moment (λ₂, L-scale). Calculated directly using l-moment with the :ratio? option set to true. It’s a robust measure. Expected value for normal distribution is ≈ 0.1226.\n\nInterpretation (for excess kurtosis :g2):\n\nPositive values indicate a leptokurtic distribution (heavier tails, more peaked than normal).\nNegative values indicate a platykurtic distribution (lighter tails, flatter than normal).\nValues near 0 suggest kurtosis similar to a normal distribution.\n\nReturns the calculated kurtosis value as a double.\nSee also kurtosis-test, bonett-seier-test, normality-test, jarque-bera-test, l-moment.\nsource\n\n\n\nkurtosis-test\n\n(kurtosis-test xs)\n(kurtosis-test xs params)\n(kurtosis-test xs kurt {:keys [sides type], :or {sides :two-sided, type :kurt}})\n\nPerforms a test for normality based on sample kurtosis.\nThis test assesses the null hypothesis that the data comes from a normally distributed population by checking if the sample kurtosis significantly deviates from the kurtosis expected under normality (approximately 3).\nThe test works by:\n\nCalculating the sample kurtosis (type configurable via :type, default :kurt).\nStandardizing the difference between the sample kurtosis and the expected kurtosis under normality using the theoretical standard error.\nApplying a further transformation (e.g., Anscombe-Glynn/D’Agostino) to this standardized score to yield a final test statistic Z that more closely follows a standard normal distribution under the null hypothesis, especially for smaller sample sizes.\n\nParameters:\n\nxs (seq of numbers): The sample data.\nkurt (double, optional): A pre-calculated kurtosis value. If omitted, it’s calculated from xs.\nparams (map, optional): Options map:\n\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis.\n\n:two-sided (default): The population kurtosis is different from normal.\n:one-sided-greater: The population kurtosis is greater than normal (leptokurtic).\n:one-sided-less: The population kurtosis is less than normal (platykurtic).\n\n:type (keyword, default :kurt): The type of kurtosis to calculate if kurt is not provided. See kurtosis for options (e.g., :kurt, :G2, :g2).\n\n\nReturns a map containing:\n\n:Z: The final test statistic, approximately standard normal under H0.\n:stat: Alias for :Z.\n:p-value: The p-value associated with Z and the specified :sides.\n:kurtosis: The sample kurtosis value used in the test (either provided or calculated).\n\nSee also skewness-test, normality-test, jarque-bera-test, bonett-seier-test.\nsource\n\n\n\nl-moment\n\n(l-moment vs order)\n(l-moment vs order {:keys [s t sorted? ratio?], :or {s 0, t 0}, :as opts})\n\nCalculates L-moment, TL-moment (trimmed) or (T)L-moment ratios.\nOptions:\n\n:s (default: 0) - number of left trimmed values\n:t (default: 0) - number of right tirmmed values\n:sorted? (default: false) - if input is already sorted\n:ratio? (default: false) - normalized l-moment, l-moment ratio\n\nsource\n\n\n\nl-variation\n\n(l-variation vs)\n\nCoefficient of L-variation, L-CV\nsource\n\n\n\nlevene-test\n\n(levene-test xss)\n(levene-test xss {:keys [sides statistic scorediff], :or {sides :one-sided-greater, statistic mean, scorediff abs}})\n\nPerforms Levene’s test for homogeneity of variances across two or more groups.\nLevene’s test assesses the null hypothesis that the variances of the groups are equal. It calculates an ANOVA on the absolute deviations of the data points from their group center (mean by default).\nParameters:\n\nxss (sequence of sequences): A collection where each element is a sequence representing a group of observations.\nparams (map, optional): Options map with the following keys:\n\n:sides (keyword, default :one-sided-greater): Alternative hypothesis side for the F-test. Possible values: :one-sided-greater, :one-sided-less, :two-sided.\n:statistic (fn, default mean): Function to calculate the center of each group (e.g., mean, median). Using median results in the Brown-Forsythe test.\n:scorediff (fn, default abs): Function applied to the difference between each data point and its group center (e.g., abs, sq).\n\n\nReturns a map containing:\n\n:W: The Levene test statistic (which is an F-statistic).\n:stat: Alias for :W.\n:p-value: The p-value for the test.\n:df: Degrees of freedom for the F-statistic ([DFt, DFe]).\n:n: Sequence of sample sizes for each group.\n:SSt: Sum of squares between groups (treatment).\n:SSe: Sum of squares within groups (error).\n:DFt: Degrees of freedom between groups.\n:DFe: Degrees of freedom within groups.\n:MSt: Mean square between groups.\n:MSe: Mean square within groups.\n:sides: Test side used.\n\nSee also brown-forsythe-test.\nsource\n\n\n\nmad\nAlias for median-absolute-deviation\nsource\n\n\n\nmad-extent\n\n(mad-extent vs)\n\n-/+ median-absolute-deviation and median\nsource\n\n\n\nmae\n\n(mae [vs1 vs2-or-val])\n(mae vs1 vs2-or-val)\n\nCalculates the Mean Absolute Error (MAE) between two sequences or a sequence and constant value.\nMAE is a measure of the difference between two sequences of values. It quantifies the average magnitude of the errors, without considering their direction.\nParameters:\n\nvs1 (sequence of numbers): The first sequence (often the observed or true values).\nvs2-or-val (sequence of numbers or single number): The second sequence (often the predicted or reference values), or a single number to compare against each element of vs1.\n\nIf both inputs are sequences, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the calculated Mean Absolute Error as a double.\nNote: MAE is less sensitive to large outliers than metrics like Mean Squared Error (MSE) because it uses the absolute value of differences rather than the squared difference.\nSee also me (Mean Error), mse (Mean Squared Error), rmse (Root Mean Squared Error).\nsource\n\n\n\nmape\n\n(mape [vs1 vs2-or-val])\n(mape vs1 vs2-or-val)\n\nCalculates the Mean Absolute Percentage Error (MAPE) between two sequences or a sequence and a constant value.\nMAPE is a measure of prediction accuracy of a forecasting method, for example in time series analysis. It is calculated as the average of the absolute percentage errors.\nParameters:\n\nvs1 (sequence of numbers): The first sequence (conventionally, the actual or true values).\nvs2-or-val (sequence of numbers or single number): The second sequence (conventionally, the predicted or reference values), or a single number to compare against each element of vs1.\n\nIf both inputs are sequences, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the calculated Mean Absolute Percentage Error as a double.\nNote: MAPE is scale-independent and useful for comparing performance across different datasets. However, it is undefined if any of the actual values (x_i) are zero, and can be skewed by small actual values.\nSee also me (Mean Error), mae (Mean Absolute Error), mse (Mean Squared Error), rmse (Root Mean Squared Error).\nsource\n\n\n\nmaximum\n\n(maximum vs)\n\nFinds the maximum value in a sequence of numbers.\nsource\n\n\n\nmcc\n\n(mcc group1 group2)\n(mcc ct)\n\nCalculates the Matthews Correlation Coefficient (MCC), also known as the Phi coefficient, for a 2x2 contingency table or binary classification outcomes.\nMCC is a measure of the quality of binary classifications. It is a balanced measure which can be used even if the classes are of very different sizes. Its value ranges from -1 to +1.\n\nA coefficient of +1 represents a perfect prediction.\n0 represents a prediction no better than random.\n-1 represents a perfect inverse prediction.\n\nThe function can be called in two ways:\n\nWith two sequences group1 and group2: The function will automatically construct a 2x2 contingency table from the unique values in the sequences (assuming they represent two binary variables). The mapping of values to table cells (e.g., what corresponds to TP, TN, FP, FN) depends on how contingency-table orders the unique values. For direct control over which cell is which, use the contingency table input.\nWith a contingency table: The contingency table can be provided as:\n\nA map where keys are [row-index, column-index] tuples and values are counts (e.g., {[0 0] TP, [0 1] FP, [1 0] FN, [1 1] TN}). This is the output format of contingency-table with two inputs.\nA sequence of sequences representing the rows of the table (e.g., [TP FP] [FN TN](#LOS-TP FP] [FN TN)). This is equivalent to rows-&gt;contingency-table.\n\n\nParameters:\n\ngroup1 (sequence): The first sequence of binary outcomes/categories.\ngroup2 (sequence): The second sequence of binary outcomes/categories. Must have the same length as group1.\ncontingency-table (map or sequence of sequences): A pre-computed 2x2 contingency table.\n\nReturns the calculated Matthews Correlation Coefficient as a double.\nNote: The implementation uses marginal sums from the contingency table, which is mathematically equivalent to the standard formula but avoids potential division by zero in the denominator product if any marginal sum is zero.\nSee also contingency-table, contingency-2x2-measures, binary-measures-all.\nsource\n\n\n\nme\n\n(me [vs1 vs2-or-val])\n(me vs1 vs2-or-val)\n\nCalculates the Mean Error (ME) between two sequences or a sequence and constant value.\nParameters:\n\nvs1 (sequence of numbers): The first sequence.\nvs2-or-val (sequence of numbers or single number): The second sequence of numbers, or a single number to compare against each element of vs1.\n\nBoth sequences (vs1 and vs2) must have the same length if both are sequences. If vs2-or-val is a single number, it is compared element-wise to vs1.\nReturns the calculated Mean Error as a double.\nNote: Positive ME indicates that vs1 values tend to be greater than vs2 values on average, while negative ME indicates vs1 values tend to be smaller. ME can be influenced by the magnitude of errors and their signs. It does not directly measure the magnitude of the typical error due to potential cancellation of positive and negative differences.\nSee also mae (Mean Absolute Error), mse (Mean Squared Error), rmse (Root Mean Squared Error).\nsource\n\n\n\nmean\n\n(mean vs)\n(mean vs weights)\n\nCalculates the arithmetic mean (average) of a sequence vs.\nIf weights are provided, calculates the weighted arithmetic mean.\nParameters:\n\nvs: Sequence of numbers.\nweights (optional): Sequence of non-negative weights corresponding to vs. Must have the same count as vs.\n\nReturns the calculated mean as a double.\nSee also geomean, harmean, powmean, median.\nsource\n\n\n\nmean-absolute-deviation\n\n(mean-absolute-deviation vs)\n(mean-absolute-deviation vs center)\n\nCalculates the Mean Absolute Deviation of a sequence vs.\nMeanAD is a measure of the variability of a univariate sample of quantitative data. It is defined as the mean of the absolute deviations from a central point, typically the data’s mean.\nMeanAD = mean(|X_i - center|)\nParameters:\n\nvs: Sequence of numbers.\ncenter (optional, double): The central point from which to calculate deviations. If nil or not provided, the arithmetic mean of vs is used as the center.\n\nReturns the calculated Mean Absolute Deviation as a double.\nUnlike median-absolute-deviation, which uses the median of absolute deviations from the median, the Mean Absolute Deviation uses the mean of absolute deviations from the mean (or specified center). This makes it more sensitive to outliers than median-absolute-deviation but less sensitive than the standard deviation.\nSee also median-absolute-deviation, stddev, mean.\nsource\n\n\n\nmeans-ratio\n\n(means-ratio [group1 group2])\n(means-ratio group1 group2)\n(means-ratio group1 group2 adjusted?)\n\nCalculates the ratio of the mean of group1 to the mean of group2.\nThis is a measure of effect size in the ‘Ratio Family’, comparing the central tendency of two groups multiplicatively.\nParameters:\n\ngroup1 (seq of numbers): The first independent sample. The mean of this group is the numerator.\ngroup2 (seq of numbers): The second independent sample. The mean of this group is the denominator.\nadjusted? (boolean, optional): If true, applies a small-sample bias correction to the ratio. Defaults to false.\n\nReturns the calculated ratio of means as a double.\nA value greater than 1 indicates that group1 has a larger mean than group2. A value less than 1 indicates group1 has a smaller mean. A value close to 1 indicates similar means.\nThe adjusted? version attempts to provide a less biased estimate of the population mean ratio, particularly for small sample sizes, by incorporating variances into the calculation (based on Bickel and Doksum, see also means-ratio-corrected).\nSee also means-ratio-corrected (which is equivalent to calling this with adjusted? set to true).\nsource\n\n\n\nmeans-ratio-corrected\n\n(means-ratio-corrected [group1 group2])\n(means-ratio-corrected group1 group2)\n\nCalculates a bias-corrected ratio of the mean of group1 to the mean of group2.\nThis function applies a correction (based on Bickel and Doksum) to the simple ratio mean(group1) / mean(group2) to reduce bias, particularly for small sample sizes.\nIt is equivalent to calling (means-ratio group1 group2 true).\nParameters:\n\ngroup1 (seq of numbers): The first independent sample. The mean of this group is the numerator.\ngroup2 (seq of numbers): The second independent sample. The mean of this group is the denominator.\n\nReturns the calculated bias-corrected ratio of means as a double.\nSee also means-ratio (for the simple, uncorrected ratio).\nsource\n\n\n\nmedian\n\n(median vs estimation-strategy)\n(median vs)\n\nCalculates median of a sequence vs.\nAn optional estimation-strategy keyword can be provided to specify the method used for estimating the quantile, particularly how interpolation is handled when the desired quantile falls between data points in the sorted sequence.\nAvailable estimation-strategy values:\n\n:legacy (Default): The original method used in Apache Commons Math.\n:r1 through :r9: Correspond to the nine quantile estimation algorithms recommended by Hyndman and Fan (1996). Each strategy differs slightly in how it calculates the index (e.g., using np or (n+1)p) and how it interpolates between points.\n\nFor detailed mathematical descriptions of each estimation strategy, refer to the Apache Commons Math Percentile documentation.\nSee also quantile, median-3\nsource\n\n\n\nmedian-3\n\n(median-3 a b c)\n\nMedian of three values. See median.\nsource\n\n\n\nmedian-absolute-deviation\n\n(median-absolute-deviation vs)\n(median-absolute-deviation vs center-or-estimation-strategy)\n(median-absolute-deviation vs center estimation-strategy)\n\nCalculates the Median Absolute Deviation (MAD) of a sequence vs.\nMAD is a robust measure of the variability of a univariate sample of quantitative data. It is defined as the median of the absolute deviations from the data’s median (or a specified center).\nMAD = median(|X_i - median(X)|)\nParameters:\n\nvs: Sequence of numbers.\ncenter-or-estimation-strategy (optional): The central point from which to calculate deviations or estimation strategy. If nil or not provided, the median of vs is used as the center. If keyword, it’s treated as estimation strategy for median.\nestimation-strategy (optional, keyword): The estimation strategy to use for calculating the median(s). This applies to the calculation of the central value (if center is not provided) and to the final median of the absolute deviations. See median or quantile for available strategies (e.g., :legacy, :r1 through :r9).\n\nReturns the calculated MAD as a double.\nMAD is less sensitive to outliers than the standard deviation.\nSee also mean-absolute-deviation, stddev, median, quantile.\nsource\n\n\n\nminimum\n\n(minimum vs)\n\nFinds the minimum value in a sequence of numbers.\nsource\n\n\n\nminimum-discrimination-information-test\n\n(minimum-discrimination-information-test contingency-table-or-xs)\n(minimum-discrimination-information-test contingency-table-or-xs params)\n\nMinimum discrimination information test, a power divergence test for lambda -1.0\nPerforms a power divergence test, which encompasses several common statistical tests like Chi-squared, G-test (likelihood ratio), etc., based on the lambda parameter. This function can perform either a goodness-of-fit test or a test for independence in a contingency table.\nUsage:\n\nGoodness-of-Fit (GOF):\n\nInput: observed-counts (sequence of numbers) and :p (expected probabilities/weights).\nInput: data (sequence of numbers) and :p (a distribution object). In this case, a histogram of data is created (controlled by :bins) and compared against the probability mass/density of the distribution in those bins.\n\nTest for Independence:\n\nInput: contingency-table (2D sequence or map format). The :p option is ignored.\n\n\nOptions map:\n\n:lambda (double, default: 2/3): Determines the specific test statistic. Common values:\n\n1.0: Pearson Chi-squared test (chisq-test).\n0.0: G-test / Multinomial Likelihood Ratio test (multinomial-likelihood-ratio-test).\n-0.5: Freeman-Tukey test (freeman-tukey-test).\n-1.0: Minimum Discrimination Information test (minimum-discrimination-information-test).\n-2.0: Neyman Modified Chi-squared test (neyman-modified-chisq-test).\n2/3: Cressie-Read test (default, cressie-read-test).\n\n:p (seq of numbers or distribution): Expected probabilities/weights (for GOF with counts) or a fastmath.random distribution object (for GOF with data). Ignored for independence tests.\n:alpha (double, default: 0.05): Significance level for confidence intervals.\n:ci-sides (keyword, default: :two-sided): Sides for bootstrap confidence intervals (:two-sided, :one-sided-greater, :one-sided-less).\n:sides (keyword, default: :one-sided-greater): Alternative hypothesis side for the p-value calculation against the Chi-squared distribution (:one-sided-greater, :one-sided-less, :two-sided).\n:bootstrap-samples (long, default: 1000): Number of bootstrap samples for confidence interval estimation.\n:ddof (long, default: 0): Delta degrees of freedom. Adjustment subtracted from the calculated degrees of freedom.\n:bins (number, keyword, or seq): Used only for GOF test against a distribution. Specifies the number of bins, an estimation method (see histogram), or explicit bin edges for histogram creation.\n\nReturns a map containing:\n\n:stat: The calculated power divergence test statistic.\n:chi2: Alias for :stat.\n:df: Degrees of freedom for the test.\n:p-value: The p-value associated with the test statistic.\n:n: Total number of observations.\n:estimate: Observed proportions.\n:expected: Expected counts or proportions under the null hypothesis.\n:confidence-interval: Bootstrap confidence intervals for the observed proportions.\n:lambda, :alpha, :sides, :ci-sides: Input options used.\n\nsource\n\n\n\nmode\n\n(mode vs method)\n(mode vs method opts)\n(mode vs)\n\nFind the value that appears most often in a dataset vs.\nIf multiple values share the same highest frequency (or estimated density/histogram peak), this function returns only the first one encountered during processing. The specific mode returned in case of a tie is not guaranteed to be stable. Use modes if you need all tied modes.\nFor samples potentially drawn from a continuous distribution, several estimation methods are provided via the method argument:\n\n:histogram: Calculates the mode based on the peak of a histogram constructed from vs. Uses interpolation within the bin with the highest frequency. Accepts options via opts, primarily :bins to control histogram construction (see histogram).\n:pearson: Estimates the mode using Pearson’s second skewness coefficient formula: mode ≈ 3 * median - 2 * mean. Accepts :estimation-strategy in opts for median calculation (see median).\n:kde: Estimates the mode by finding the original data point in vs with the highest estimated probability density, based on Kernel Density Estimation (KDE). Accepts KDE options in opts like :kernel, :bandwidth, etc. (passed to fastmath.kernel.density/kernel-density).\n:default (or when method is omitted): Finds the exact value that occurs most frequently in vs. Suitable for discrete data.\n\nThe optional opts map provides method-specific configuration.\nSee also modes (returns all modes) and wmode (for weighted data).\nsource\n\n\n\nmodes\n\n(modes vs method)\n(modes vs method opts)\n(modes vs)\n\nFind the values that appear most often in a dataset vs.\nReturns sequence with all most appearing values. For the default method (discrete data), modes are sorted in increasing order.\nFor samples potentially drawn from a continuous distribution, simply finding the most frequent exact value might not be meaningful. Several estimation methods are provided via the method argument:\n\n:histogram: Calculates the mode(s) based on the peak(s) of a histogram constructed from vs. Uses interpolation within the bin(s) with the highest frequency. Accepts options via opts, primarily :bins to control histogram construction (see histogram).\n:pearson: Estimates the mode using Pearson’s second skewness coefficient formula: mode ≈ 3 * median - 2 * mean. Accepts :estimation-strategy in opts for median calculation (see median). Returns a single estimated mode.\n:kde: Estimates the mode(s) by finding the original data points in vs with the highest estimated probability density, based on Kernel Density Estimation (KDE). Accepts KDE options in opts like :kernel, :bandwidth, etc. (passed to fastmath.kernel.density/kernel-density).\n:default (or when method is omitted): Finds the exact value(s) that occur most frequently in vs. Suitable for discrete data.\n\nThe optional opts map provides method-specific configuration.\nSee also mode (returns only the first mode) and wmodes (for weighted data).\nsource\n\n\n\nmodified-power-transformation DEPRECATED\nDeprecated: Use `(box-cox-transformation xs lambda {:negative? true})\n\n(modified-power-transformation xs)\n(modified-power-transformation xs lambda)\n(modified-power-transformation xs lambda alpha)\n\nApplies a modified power transformation (Bickel and Doksum) to a data.\nsource\n\n\n\nmoment\n\n(moment vs)\n(moment vs order)\n(moment vs order {:keys [absolute? center mean? normalize?], :or {mean? true}})\n\nCalculate moment (central or/and absolute) of given order (default: 2).\nAdditional parameters as a map:\n\n:absolute? - calculate sum as absolute values (default: false)\n:mean? - returns mean (proper moment) or just sum of differences (default: true)\n:center - value of center (default: nil = mean)\n:normalize? - apply normalization by standard deviation to the order power\n\nsource\n\n\n\nmse\n\n(mse [vs1 vs2-or-val])\n(mse vs1 vs2-or-val)\n\nCalculates the Mean Squared Error (MSE) between two sequences or a sequence and a constant value.\nMSE is a measure of the quality of an estimator or predictor. It quantifies the average of the squared differences between corresponding elements of the input sequences.\nParameters:\n\nvs1 (sequence of numbers): The first sequence (often the observed or true values).\nvs2-or-val (sequence of numbers or single number): The second sequence (often the predicted or reference values), or a single number to compare against each element of vs1.\n\nIf both inputs are sequences, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the calculated Mean Squared Error as a double.\nNote: MSE penalizes larger errors more heavily than smaller errors because the errors are squared. This makes it sensitive to outliers. It is the average of the rss (Residual Sum of Squares). Its square root is the rmse.\nSee also rss (Residual Sum of Squares), rmse (Root Mean Squared Error), me (Mean Error), mae (Mean Absolute Error), r2 (Coefficient of Determination).\nsource\n\n\n\nmultinomial-likelihood-ratio-test\n\n(multinomial-likelihood-ratio-test contingency-table-or-xs)\n(multinomial-likelihood-ratio-test contingency-table-or-xs params)\n\nMultinomial likelihood ratio test, a power divergence test for lambda 0.0\nPerforms a power divergence test, which encompasses several common statistical tests like Chi-squared, G-test (likelihood ratio), etc., based on the lambda parameter. This function can perform either a goodness-of-fit test or a test for independence in a contingency table.\nUsage:\n\nGoodness-of-Fit (GOF):\n\nInput: observed-counts (sequence of numbers) and :p (expected probabilities/weights).\nInput: data (sequence of numbers) and :p (a distribution object). In this case, a histogram of data is created (controlled by :bins) and compared against the probability mass/density of the distribution in those bins.\n\nTest for Independence:\n\nInput: contingency-table (2D sequence or map format). The :p option is ignored.\n\n\nOptions map:\n\n:lambda (double, default: 2/3): Determines the specific test statistic. Common values:\n\n1.0: Pearson Chi-squared test (chisq-test).\n0.0: G-test / Multinomial Likelihood Ratio test (multinomial-likelihood-ratio-test).\n-0.5: Freeman-Tukey test (freeman-tukey-test).\n-1.0: Minimum Discrimination Information test (minimum-discrimination-information-test).\n-2.0: Neyman Modified Chi-squared test (neyman-modified-chisq-test).\n2/3: Cressie-Read test (default, cressie-read-test).\n\n:p (seq of numbers or distribution): Expected probabilities/weights (for GOF with counts) or a fastmath.random distribution object (for GOF with data). Ignored for independence tests.\n:alpha (double, default: 0.05): Significance level for confidence intervals.\n:ci-sides (keyword, default: :two-sided): Sides for bootstrap confidence intervals (:two-sided, :one-sided-greater, :one-sided-less).\n:sides (keyword, default: :one-sided-greater): Alternative hypothesis side for the p-value calculation against the Chi-squared distribution (:one-sided-greater, :one-sided-less, :two-sided).\n:bootstrap-samples (long, default: 1000): Number of bootstrap samples for confidence interval estimation.\n:ddof (long, default: 0): Delta degrees of freedom. Adjustment subtracted from the calculated degrees of freedom.\n:bins (number, keyword, or seq): Used only for GOF test against a distribution. Specifies the number of bins, an estimation method (see histogram), or explicit bin edges for histogram creation.\n\nReturns a map containing:\n\n:stat: The calculated power divergence test statistic.\n:chi2: Alias for :stat.\n:df: Degrees of freedom for the test.\n:p-value: The p-value associated with the test statistic.\n:n: Total number of observations.\n:estimate: Observed proportions.\n:expected: Expected counts or proportions under the null hypothesis.\n:confidence-interval: Bootstrap confidence intervals for the observed proportions.\n:lambda, :alpha, :sides, :ci-sides: Input options used.\n\nsource\n\n\n\nneyman-modified-chisq-test\n\n(neyman-modified-chisq-test contingency-table-or-xs)\n(neyman-modified-chisq-test contingency-table-or-xs params)\n\nNeyman modifield chi square test, a power divergence test for lambda -2.0\nPerforms a power divergence test, which encompasses several common statistical tests like Chi-squared, G-test (likelihood ratio), etc., based on the lambda parameter. This function can perform either a goodness-of-fit test or a test for independence in a contingency table.\nUsage:\n\nGoodness-of-Fit (GOF):\n\nInput: observed-counts (sequence of numbers) and :p (expected probabilities/weights).\nInput: data (sequence of numbers) and :p (a distribution object). In this case, a histogram of data is created (controlled by :bins) and compared against the probability mass/density of the distribution in those bins.\n\nTest for Independence:\n\nInput: contingency-table (2D sequence or map format). The :p option is ignored.\n\n\nOptions map:\n\n:lambda (double, default: 2/3): Determines the specific test statistic. Common values:\n\n1.0: Pearson Chi-squared test (chisq-test).\n0.0: G-test / Multinomial Likelihood Ratio test (multinomial-likelihood-ratio-test).\n-0.5: Freeman-Tukey test (freeman-tukey-test).\n-1.0: Minimum Discrimination Information test (minimum-discrimination-information-test).\n-2.0: Neyman Modified Chi-squared test (neyman-modified-chisq-test).\n2/3: Cressie-Read test (default, cressie-read-test).\n\n:p (seq of numbers or distribution): Expected probabilities/weights (for GOF with counts) or a fastmath.random distribution object (for GOF with data). Ignored for independence tests.\n:alpha (double, default: 0.05): Significance level for confidence intervals.\n:ci-sides (keyword, default: :two-sided): Sides for bootstrap confidence intervals (:two-sided, :one-sided-greater, :one-sided-less).\n:sides (keyword, default: :one-sided-greater): Alternative hypothesis side for the p-value calculation against the Chi-squared distribution (:one-sided-greater, :one-sided-less, :two-sided).\n:bootstrap-samples (long, default: 1000): Number of bootstrap samples for confidence interval estimation.\n:ddof (long, default: 0): Delta degrees of freedom. Adjustment subtracted from the calculated degrees of freedom.\n:bins (number, keyword, or seq): Used only for GOF test against a distribution. Specifies the number of bins, an estimation method (see histogram), or explicit bin edges for histogram creation.\n\nReturns a map containing:\n\n:stat: The calculated power divergence test statistic.\n:chi2: Alias for :stat.\n:df: Degrees of freedom for the test.\n:p-value: The p-value associated with the test statistic.\n:n: Total number of observations.\n:estimate: Observed proportions.\n:expected: Expected counts or proportions under the null hypothesis.\n:confidence-interval: Bootstrap confidence intervals for the observed proportions.\n:lambda, :alpha, :sides, :ci-sides: Input options used.\n\nsource\n\n\n\nnormality-test\n\n(normality-test xs)\n(normality-test xs params)\n(normality-test xs skew kurt {:keys [sides], :or {sides :one-sided-greater}})\n\nPerforms the D’Agostino-Pearson K² omnibus test for normality.\nThis test combines the results of the skewness and kurtosis tests to provide an overall assessment of whether the sample data deviates from a normal distribution in terms of either asymmetry or peakedness/tailedness.\nThe test works by: 1. Calculating a normalized test statistic (Z₁) for skewness using skewness-test. 2. Calculating a normalized test statistic (Z₂) for kurtosis using kurtosis-test. 3. Combining these into an omnibus statistic: K² = Z₁² + Z₂². 4. Under the null hypothesis that the data comes from a normal distribution, K² approximately follows a Chi-squared distribution with 2 degrees of freedom.\nParameters:\n\nxs (seq of numbers): The sample data.\nskew (double, optional): A pre-calculated skewness value (type :g1 used by default in underlying test).\nkurt (double, optional): A pre-calculated kurtosis value (type :kurt used by default in underlying test).\nparams (map, optional): Options map:\n\n:sides (keyword, default :one-sided-greater): Specifies the side(s) of the Chi-squared(2) distribution used for p-value calculation.\n\n:one-sided-greater (default and standard): Tests if K² is significantly large, indicating departure from normality in skewness, kurtosis, or both.\n:one-sided-less: Tests if the K² statistic is significantly small.\n:two-sided: Tests if the K² statistic is extreme in either tail.\n\n\n\nReturns a map containing:\n\n:Z: The calculated K² omnibus test statistic (labeled :Z for consistency, though it follows Chi-squared(2)).\n:stat: Alias for :Z.\n:p-value: The p-value associated with the K² statistic and :sides.\n:skewness: The sample skewness value used (either provided or calculated).\n:kurtosis: The sample kurtosis value used (either provided or calculated).\n\nSee also skewness-test, kurtosis-test, jarque-bera-test.\nsource\n\n\n\nomega-sq\n\n(omega-sq [group1 group2])\n(omega-sq group1 group2)\n(omega-sq group1 group2 degrees-of-freedom)\n\nCalculates Omega squared (ω²), an effect size measure for the simple linear regression of group1 on group2.\nOmega squared estimates the proportion of variance in the dependent variable (group1) that is accounted for by the independent variable (group2) in the population. It is considered a less biased alternative to r2-determination.\nParameters:\n\ngroup1 (seq of numbers): The dependent variable.\ngroup2 (seq of numbers): The independent variable. Must have the same length as group1.\ndegrees-of-freedom (double, optional): The degrees of freedom for the regression model. Defaults to 1.0, which is standard for simple linear regression and used in the 2-arity version. Providing a different value allows calculating ω² for cases with multiple predictors if the sums of squares are computed for the overall model.\n\nReturns the calculated Omega squared value as a double. The value typically ranges from 0.0 to 1.0.\nInterpretation:\n\n0.0 indicates that group2 explains none of the variance in group1 in the population.\n1.0 indicates that group2 perfectly explains the variance in group1 in the population.\n\nNote: While often presented in the context of ANOVA, this implementation applies the formula to the sums of squares obtained from a simple linear regression between the two sequences. The 3-arity version allows specifying a custom degrees of freedom for regression, which might be relevant for calculating overall \\(\\omega^2\\) in multiple regression contexts (where degrees-of-freedom would be the number of predictors).\nSee also eta-sq (Eta-squared, often based on \\(R^2\\)), epsilon-sq (another adjusted R²-like measure), r2-determination (R-squared).\nsource\n\n\n\none-way-anova-test\n\n(one-way-anova-test xss)\n(one-way-anova-test xss {:keys [sides], :or {sides :one-sided-greater}})\n\nPerforms a one-way analysis of variance (ANOVA) test.\nANOVA tests the null hypothesis that the means of two or more independent groups are equal. It assumes that the data within each group are normally distributed and have equal variances.\nParameters:\n\nxss (sequence of sequences): A collection where each element is a sequence representing a group of observations.\nparams (map, optional): Options map with the following key:\n\n:sides (keyword, default :one-sided-greater): Alternative hypothesis side for the F-test. Possible values: :one-sided-greater, :one-sided-less, :two-sided.\n\n\nReturns a map containing:\n\n:F: The F-statistic for the test.\n:stat: Alias for :F.\n:p-value: The p-value for the test.\n:df: Degrees of freedom for the F-statistic ([DFt, DFe]).\n:n: Sequence of sample sizes for each group.\n:SSt: Sum of squares between groups (treatment).\n:SSe: Sum of squares within groups (error).\n:DFt: Degrees of freedom between groups.\n:DFe: Degrees of freedom within groups.\n:MSt: Mean square between groups.\n:MSe: Mean square within groups.\n:sides: Test side used.\n\nsource\n\n\n\nouter-fence-extent\n\n(outer-fence-extent vs)\n(outer-fence-extent vs estimation-strategy)\n\nReturns LOF, UOF and median\nsource\n\n\n\noutliers\n\n(outliers vs)\n(outliers vs estimation-strategy)\n(outliers vs q1 q3)\n\nFind outliers defined as values outside inner fences.\nLet Q1 is 25-percentile and Q3 is 75-percentile. IQR is (- Q3 Q1).\n\nLIF (Lower Inner Fence) equals (- Q1 (* 1.5 IQR)).\nUIF (Upper Inner Fence) equals (+ Q3 (* 1.5 IQR)).\n\nReturns a sequence of outliers.\nOptional estimation-strategy argument can be set to change quantile calculations estimation type. See estimation-strategies.\nsource\n\n\n\np-overlap\n\n(p-overlap [group1 group2])\n(p-overlap group1 group2)\n(p-overlap group1 group2 {:keys [kde bandwidth min-iterations steps], :or {kde :gaussian, min-iterations 3, steps 500}})\n\nCalculates the overlapping index between the estimated distributions of two samples using Kernel Density Estimation (KDE).\nThis function estimates the probability density function (PDF) for group1 and group2 using KDE and then calculates the area of overlap between the two estimated PDFs. The area of overlap is the integral of the minimum of the two density functions.\nParameters:\n\ngroup1 (seq of numbers): The first sample.\ngroup2 (seq of numbers): The second sample.\nopts (map, optional): Options map for KDE and integration:\n\n:kde (keyword, default :gaussian): The kernel function to use for KDE. See fastmath.kernel.density/kernel-density+ for options.\n:bandwidth (double, optional): The bandwidth for KDE. If omitted, it is automatically estimated.\n:min-iterations (long, default 3): Minimum number of iterations for Romberg integration.\n:steps (long, default 500): Number of steps (subintervals) for numerical integration over the relevant range.\n\n\nReturns the calculated overlapping index as a double, representing the area of overlap between the two estimated distributions. A value closer to 1 indicates greater overlap, while a value closer to 0 indicates less overlap.\nThis measure quantifies the degree to which two distributions share common values and can be seen as a measure of similarity.\nsource\n\n\n\np-value\n\n(p-value stat)\n(p-value distribution stat)\n(p-value distribution stat sides)\n\nCalculates the p-value for a given test statistic based on a reference probability distribution.\nThe p-value represents the probability of observing a test statistic as extreme as, or more extreme than, the provided stat, assuming the null hypothesis is true (where the null hypothesis implies stat follows the given distribution).\nParameters:\n\ndistribution (distribution object, optional): The probability distribution object (from fastmath.random) that the test statistic follows under the null hypothesis. Defaults to the standard normal distribution (fastmath.random/default-normal) if omitted.\nstat (double): The observed value of the test statistic.\nsides (keyword, optional): Specifies the type of alternative hypothesis and how ‘extremeness’ is defined. Defaults to :two-sided.\n\n:two-sided or :both: Alternative hypothesis is that the true parameter is different from the null value (tests for extremeness in either tail). Calculates 2 * min(CDF(stat), CCDF(stat)) (adjusted for discrete).\n:one-sided-greater or :right: Alternative hypothesis is that the true parameter is greater than the null value (tests for extremeness in the right tail). Calculates CCDF(stat) (adjusted for discrete).\n:one-sided-less, :left, or :one-sided: Alternative hypothesis is that the true parameter is less than the null value (tests for extremeness in the left tail). Calculates CDF(stat).\n\n\nNote: For discrete distributions, a continuity correction (stat - 1 for CCDF calculations) is applied when calculating right-tail or two-tail probabilities involving the upper tail. This ensures the probability mass at the statistic value is correctly accounted for.\nReturns the calculated p-value (a double between 0.0 and 1.0).\nsource\n\n\n\npacf\n\n(pacf data)\n(pacf data lags)\n\nCalculates the Partial Autocorrelation Function (PACF) for a given time series data.\nThe PACF measures the linear dependence between a time series and its lagged values after removing the effects of the intermediate lags. It helps identify the direct relationship at each lag and is used to determine the order of autoregressive (AR) components in time series models (e.g., ARIMA).\nParameters:\n\ndata (seq of numbers): The time series data.\nlags (long, optional): The maximum lag for which to calculate the PACF. If omitted, calculates PACF for lags from 0 up to (dec (count data)).\n\nReturns a sequence of doubles representing the partial autocorrelation coefficients for the specified lags. The value at lag 0 is always 0.0.\nSee also acf, acf-ci, pacf-ci.\nsource\n\n\n\npacf-ci\n\n(pacf-ci data)\n(pacf-ci data lags)\n(pacf-ci data lags alpha)\n\nCalculates the Partial Autocorrelation Function (PACF) for a time series and provides approximate confidence intervals.\nThis function computes the PACF of the input time series data for specified lags (see pacf) and includes approximate confidence intervals around the PACF estimates. These intervals help determine whether the partial autocorrelation at a specific lag is statistically significant (i.e., likely non-zero in the population).\nParameters:\n\ndata (seq of numbers): The time series data.\nlags (long, optional): The maximum lag for which to calculate the PACF and CI. If omitted, calculates for lags up to (dec (count data)).\nalpha (double, optional): The significance level for the confidence intervals. Defaults to 0.05 (for a 95% CI).\n\nReturns a map containing:\n\n:ci (double): The value of the approximate standard confidence interval bound for lags &gt; 0. If the absolute value of a PACF coefficient at lag k &gt; 0 exceeds this value, it is considered statistically significant.\n:pacf (seq of doubles): The sequence of partial autocorrelation coefficients at lags from 0 up to lags (calculated using pacf).\n\nSee also pacf, acf, acf-ci.\nsource\n\n\n\npearson-correlation\n\n(pearson-correlation [vs1 vs2])\n(pearson-correlation vs1 vs2)\n\nCalculates the Pearson product-moment correlation coefficient between two sequences.\nThis function measures the linear relationship between two datasets. The coefficient value ranges from -1.0 (perfect negative linear correlation) to 1.0 (perfect positive linear correlation), with 0.0 indicating no linear correlation.\nParameters:\n\n[vs1 vs2] (sequence of two sequences): A sequence containing the two sequences of numbers.\nvs1, vs2 (sequences): The two sequences of numbers directly as arguments.\n\nBoth input sequences must contain only numbers and must have the same length.\nReturns the calculated Pearson correlation coefficient as a double. Returns NaN if either sequence has zero variance (i.e., all elements are the same).\nSee also correlation (general correlation, defaults to Pearson), spearman-correlation, kendall-correlation, correlation-matrix.\nsource\n\n\n\npearson-r\n\n(pearson-r [group1 group2])\n(pearson-r group1 group2)\n\nCalculates the Pearson r correlation coefficient between two sequences.\nThis function is an alias for pearson-correlation.\nSee pearson-correlation for detailed documentation, parameters, and usage examples.\nsource\n\n\n\npercentile\n\n(percentile vs p)\n(percentile vs p estimation-strategy)\n\nCalculates the p-th percentile of a sequence vs.\nThe percentile p is a value between 0 and 100, inclusive.\nAn optional estimation-strategy keyword can be provided to specify the method used for estimating the percentile, particularly how interpolation is handled when the desired percentile falls between data points in the sorted sequence.\nAvailable estimation-strategy values:\n\n:legacy (Default): The original method used in Apache Commons Math.\n:r1 through :r9: Correspond to the nine quantile estimation algorithms recommended by Hyndman and Fan (1996). Each strategy differs slightly in how it calculates the index (e.g., using np or (n+1)p) and how it interpolates between points.\n\nFor detailed mathematical descriptions of each estimation strategy, refer to the Apache Commons Math Percentile documentation.\nSee also quantile (which uses a 0.0-1.0 range) and percentiles.\nsource\n\n\n\npercentile-bc-extent\n\n(percentile-bc-extent vs)\n(percentile-bc-extent vs p)\n(percentile-bc-extent vs p1 p2)\n(percentile-bc-extent vs p1 p2 estimation-strategy)\n\nReturn bias corrected percentile range and mean for bootstrap samples. See https://projecteuclid.org/euclid.ss/1032280214\np - calculates extent of bias corrected p and 100-p (default: p=2.5)\nSet estimation-strategy to :r7 to get the same result as in R coxed::bca.\nsource\n\n\n\npercentile-bca-extent\n\n(percentile-bca-extent vs)\n(percentile-bca-extent vs p)\n(percentile-bca-extent vs p1 p2)\n(percentile-bca-extent vs p1 p2 estimation-strategy)\n(percentile-bca-extent vs p1 p2 accel estimation-strategy)\n\nReturn bias corrected percentile range and mean for bootstrap samples. Also accounts for variance variations throught the accelaration parameter. See https://projecteuclid.org/euclid.ss/1032280214\np - calculates extent of bias corrected p and 100-p (default: p=2.5)\nSet estimation-strategy to :r7 to get the same result as in R coxed::bca.\nsource\n\n\n\npercentile-extent\n\n(percentile-extent vs)\n(percentile-extent vs p)\n(percentile-extent vs p1 p2)\n(percentile-extent vs p1 p2 estimation-strategy)\n\nReturn percentile range and median.\np - calculates extent of p and 100-p (default: p=25)\nsource\n\n\n\npercentiles\n\n(percentiles vs)\n(percentiles vs ps)\n(percentiles vs ps estimation-strategy)\n\nCalculates the sequence of p-th percentiles of a sequence vs.\nPercentiles ps is sequence of values between 0 and 100, inclusive.\nAn optional estimation-strategy keyword can be provided to specify the method used for estimating the percentile, particularly how interpolation is handled when the desired percentile falls between data points in the sorted sequence.\nAvailable estimation-strategy values:\n\n:legacy (Default): The original method used in Apache Commons Math.\n:r1 through :r9: Correspond to the nine quantile estimation algorithms recommended by Hyndman and Fan (1996). Each strategy differs slightly in how it calculates the index (e.g., using np or (n+1)p) and how it interpolates between points.\n\nFor detailed mathematical descriptions of each estimation strategy, refer to the Apache Commons Math Percentile documentation.\nSee also quantiles (which uses a 0.0-1.0 range) and percentile.\nsource\n\n\n\npi\n\n(pi vs)\n(pi vs size)\n(pi vs size estimation-strategy)\n\nReturns PI as a map, quantile intervals based on interval size.\nQuantiles are (1-size)/2 and 1-(1-size)/2\nsource\n\n\n\npi-extent\n\n(pi-extent vs)\n(pi-extent vs size)\n(pi-extent vs size estimation-strategy)\n\nReturns PI extent, quantile intervals based on interval size + median.\nQuantiles are (1-size)/2 and 1-(1-size)/2\nsource\n\n\n\npooled-mad\n\n(pooled-mad groups)\n(pooled-mad groups const)\n\nCalculate pooled median absolute deviation for samples.\nk is a scaling constant which equals around 1.4826 by default.\nsource\n\n\n\npooled-stddev\n\n(pooled-stddev groups)\n(pooled-stddev groups method)\n\nCalculate pooled standard deviation for samples and method\nMethods:\n\n:unbiased - sqrt of weighted average of variances (default)\n:biased - biased version of :unbiased, no count correction.\n:avg - sqrt of average of variances\n\nsource\n\n\n\npooled-variance\n\n(pooled-variance groups)\n(pooled-variance groups method)\n\nCalculate pooled variance for samples and method.\nMethods: * :unbiased - weighted average of variances (default) * :biased - biased version of :unbiased, no count correction. * :avg - average of variances\nsource\n\n\n\npopulation-stddev\n\n(population-stddev vs)\n(population-stddev vs mu)\n\nCalculate population standard deviation of vs.\nSee stddev.\nsource\n\n\n\npopulation-variance\n\n(population-variance vs)\n(population-variance vs mu)\n\nCalculate population variance of vs.\nSee variance.\nsource\n\n\n\npopulation-wstddev\n\n(population-wstddev vs weights)\n\nCalculate population weighted standard deviation of vs\nsource\n\n\n\npopulation-wvariance\n\n(population-wvariance vs freqs)\n\nCalculate weighted population variance of vs.\nsource\n\n\n\npower-divergence-test\n\n(power-divergence-test contingency-table-or-xs)\n(power-divergence-test contingency-table-or-xs {:keys [lambda ci-sides sides p alpha bootstrap-samples ddof bins], :or {lambda m/TWO_THIRD, sides :one-sided-greater, ci-sides :two-sided, alpha 0.05, bootstrap-samples 1000, ddof 0}})\n\nPerforms a power divergence test, which encompasses several common statistical tests like Chi-squared, G-test (likelihood ratio), etc., based on the lambda parameter. This function can perform either a goodness-of-fit test or a test for independence in a contingency table.\nUsage:\n\nGoodness-of-Fit (GOF):\n\nInput: observed-counts (sequence of numbers) and :p (expected probabilities/weights).\nInput: data (sequence of numbers) and :p (a distribution object). In this case, a histogram of data is created (controlled by :bins) and compared against the probability mass/density of the distribution in those bins.\n\nTest for Independence:\n\nInput: contingency-table (2D sequence or map format). The :p option is ignored.\n\n\nOptions map:\n\n:lambda (double, default: 2/3): Determines the specific test statistic. Common values:\n\n1.0: Pearson Chi-squared test (chisq-test).\n0.0: G-test / Multinomial Likelihood Ratio test (multinomial-likelihood-ratio-test).\n-0.5: Freeman-Tukey test (freeman-tukey-test).\n-1.0: Minimum Discrimination Information test (minimum-discrimination-information-test).\n-2.0: Neyman Modified Chi-squared test (neyman-modified-chisq-test).\n2/3: Cressie-Read test (default, cressie-read-test).\n\n:p (seq of numbers or distribution): Expected probabilities/weights (for GOF with counts) or a fastmath.random distribution object (for GOF with data). Ignored for independence tests.\n:alpha (double, default: 0.05): Significance level for confidence intervals.\n:ci-sides (keyword, default: :two-sided): Sides for bootstrap confidence intervals (:two-sided, :one-sided-greater, :one-sided-less).\n:sides (keyword, default: :one-sided-greater): Alternative hypothesis side for the p-value calculation against the Chi-squared distribution (:one-sided-greater, :one-sided-less, :two-sided).\n:bootstrap-samples (long, default: 1000): Number of bootstrap samples for confidence interval estimation.\n:ddof (long, default: 0): Delta degrees of freedom. Adjustment subtracted from the calculated degrees of freedom.\n:bins (number, keyword, or seq): Used only for GOF test against a distribution. Specifies the number of bins, an estimation method (see histogram), or explicit bin edges for histogram creation.\n\nReturns a map containing:\n\n:stat: The calculated power divergence test statistic.\n:chi2: Alias for :stat.\n:df: Degrees of freedom for the test.\n:p-value: The p-value associated with the test statistic.\n:n: Total number of observations.\n:estimate: Observed proportions.\n:expected: Expected counts or proportions under the null hypothesis.\n:confidence-interval: Bootstrap confidence intervals for the observed proportions.\n:lambda, :alpha, :sides, :ci-sides: Input options used.\n\nsource\n\n\n\npower-transformation DEPRECATED\nDeprecated: Use `(box-cox-transformation xs lambda {:scaled true})\n\n(power-transformation xs)\n(power-transformation xs lambda)\n(power-transformation xs lambda alpha)\n\nApplies a power transformation to a data.\nsource\n\n\n\npowmean\n\n(powmean vs power)\n(powmean vs weights power)\n\nCalculates the generalized power mean (also known as the Hölder mean) of a sequence vs.\nThe power mean is a generalization of the Pythagorean means (arithmetic, geometric, harmonic) and other means like the quadratic mean (RMS). It is defined for a non-zero real number power.\nParameters:\n\nvs: Sequence of numbers. Constraints depend on the power:\n\nFor power &gt; 0, values should be non-negative.\nFor power = 0, values must be positive (reduces to geometric mean).\nFor power &lt; 0, values must be positive and non-zero.\n\nweights (optional): Sequence of non-negative weights corresponding to vs. Must have the same count as vs.\npower (double): The exponent defining the mean.\n\nSpecial Cases:\n\npower = 0: Returns the geomean.\npower = 1: Returns the arithmetic mean.\npower = -1: Equivalent to the harmean. (Handled by the general formula)\npower = 2: Returns the Root Mean Square (RMS) or quadratic mean.\npower = inf: Returns maximum.\npower = -inf: Returms minimum.\nThe implementation includes optimized paths for power values 1/3, 0.5, 2, and 3.\n\nReturns the calculated power mean as a double.\nSee also mean, geomean, harmean.\nsource\n\n\n\npsnr\n\n(psnr [vs1 vs2-or-val])\n(psnr vs1 vs2-or-val)\n(psnr vs1 vs2-or-val max-value)\n\nPeak Signal-to-Noise Ratio (PSNR).\nPSNR is a measure used to quantify the quality of reconstruction of lossy compression codecs (e.g., for images or video). It is calculated using the Mean Squared Error (MSE) between the original and compressed images/signals. A higher PSNR generally indicates a higher quality signal reconstruction (i.e., less distortion).\nParameters:\n\nvs1 (sequence of numbers): The first sequence (conventionally, the original or reference signal/data).\nvs2-or-val (sequence of numbers or single number): The second sequence (conventionally, the reconstructed or noisy signal/data), or a single number to compare against each element of vs1.\nmax-value (optional, double): The maximum possible value of a sample in the data. If not provided, the function automatically determines the maximum value present across both input sequences (vs1 and vs2 if a sequence, or vs1 and the scalar value if vs2-or-val is a number). Providing an explicit max-value is often more appropriate based on the data type’s theoretical maximum range (e.g., 255 for 8-bit).\n\nIf vs2-or-val is a sequence, both vs1 and vs2 must have the same length.\nReturns the calculated Peak Signal-to-Noise Ratio as a double. Returns -Double/Infinity if the MSE is zero (perfect match). Returns NaN if MSE is non-positive.\nSee also mse, rmse.\nsource\n\n\n\nquantile\n\n(quantile vs q)\n(quantile vs q estimation-strategy)\n\nCalculates the q-th quantile of a sequence vs.\nThe quantile q is a value between 0.0 and 1.0, inclusive.\nAn optional estimation-strategy keyword can be provided to specify the method used for estimating the quantile, particularly how interpolation is handled when the desired quantile falls between data points in the sorted sequence.\nAvailable estimation-strategy values:\n\n:legacy (Default): The original method used in Apache Commons Math.\n:r1 through :r9: Correspond to the nine quantile estimation algorithms recommended by Hyndman and Fan (1996). Each strategy differs slightly in how it calculates the index (e.g., using np or (n+1)p) and how it interpolates between points.\n\nFor detailed mathematical descriptions of each estimation strategy, refer to the Apache Commons Math Percentile documentation.\nSee also percentile (which uses a 0-100 range) and quantiles.\nsource\n\n\n\nquantile-extent\n\n(quantile-extent vs)\n(quantile-extent vs q)\n(quantile-extent vs q1 q2)\n(quantile-extent vs q1 q2 estimation-strategy)\n\nReturn quantile range and median.\nq - calculates extent of q and 1.0-q (default: q=0.25)\nsource\n\n\n\nquantiles\n\n(quantiles vs)\n(quantiles vs qs)\n(quantiles vs qs estimation-strategy)\n\nCalculates the sequence of q-th quantiles of a sequence vs.\nQuantiles q is a sequence of values between 0.0 and 1.0, inclusive.\nAn optional estimation-strategy keyword can be provided to specify the method used for estimating the quantile, particularly how interpolation is handled when the desired quantile falls between data points in the sorted sequence.\nAvailable estimation-strategy values:\n\n:legacy (Default): The original method used in Apache Commons Math.\n:r1 through :r9: Correspond to the nine quantile estimation algorithms recommended by Hyndman and Fan (1996). Each strategy differs slightly in how it calculates the index (e.g., using np or (n+1)p) and how it interpolates between points.\n\nFor detailed mathematical descriptions of each estimation strategy, refer to the Apache Commons Math Percentile documentation.\nSee also percentiles (which uses a 0-100 range) and quantile.\nsource\n\n\n\nr2\n\n(r2 [vs1 vs2-or-val])\n(r2 vs1 vs2-or-val)\n(r2 vs1 vs2-or-val no-of-variables)\n\nCalculates the Coefficient of Determination (\\(R^2\\)) or adjusted version between two sequences or a sequence and a constant value.\n\\(R^2\\) is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s) in a statistical model. It indicates how well the model fits the observed data.\nThe standard \\(R^2\\) is calculated as \\(1 - (RSS / TSS)\\), where: - \\(RSS\\) (Residual Sum of Squares) is the sum of the squared differences between the observed values (vs1) and the predicted/reference values (vs2 or vs2-or-val). See rss. - \\(TSS\\) (Total Sum of Squares) is the sum of the squared differences between the observed values (vs1) and their mean. This is calculated using moment of order 2 with :mean? set to false.\nThis function has two arities:\n\n(r2 vs1 vs2-or-val): Calculates the standard \\(R^2\\).\n\nvs1 (seq of numbers): The sequence of observed or actual values.\nvs2-or-val (seq of numbers or single number): The sequence of predicted or reference values, or a single constant value to compare against.\n\nReturns the calculated standard \\(R^2\\) as a double. For simple linear regression, this is equal to the square of the Pearson correlation coefficient (r2-determination). \\(R^2\\) typically ranges from 0 to 1 in this context, but can be negative if the chosen model fits the data worse than a horizontal line through the mean of the observed data.\n(r2 vs1 vs2-or-val no-of-variables): Calculates the Adjusted \\(R^2\\). The adjusted \\(R^2\\) is a modified version of \\(R^2\\) that has been adjusted for the number of predictors in the model. It increases only if the new term improves the model more than would be expected by chance. The formula for adjusted \\(R^2\\) is: \\[ R^2_{adj} = 1 - (1 - R^2) \\frac{n-1}{n-p-1} \\] where \\(n\\) is the number of observations (length of vs1) and \\(p\\) is the number of independent variables (no-of-variables).\n\nvs1 (seq of numbers): The sequence of observed or actual values.\nvs2-or-val (seq of numbers or single number): The sequence of predicted or reference values, or a single constant value to compare against.\nno-of-variables (double): The number of independent variables (\\(p\\)) used in the model that produced the vs2-or-val predictions.\n\nReturns the calculated adjusted \\(R^2\\) as a double.\n\nBoth vs1 and vs2 (if vs2-or-val is a sequence) must have the same length.\nSee also rss, mse, rmse, pearson-correlation, r2-determination.\nsource\n\n\n\nr2-determination\n\n(r2-determination [group1 group2])\n(r2-determination group1 group2)\n\nCalculates the Coefficient of Determination (\\(R^2\\)) between two sequences.\nThis function computes the square of the Pearson product-moment correlation coefficient (pearson-correlation) between group1 and group2.\n\\(R^2\\) measures the proportion of the variance in one variable that is predictable from the other variable in a linear relationship. For a simple linear regression with one independent variable, this value is equivalent to the \\(R^2\\) calculated from the Residual Sum of Squares (RSS) and Total Sum of Squares (TSS).\nParameters:\n\ngroup1 (seq of numbers): The first sequence.\ngroup2 (seq of numbers): The second sequence.\n\nBoth sequences must have the same length.\nReturns the calculated \\(R^2\\) value (a double between 0.0 and 1.0) as a double. Returns NaN if the Pearson correlation cannot be calculated (e.g., one sequence is constant).\nSee also r2 (for general \\(R^2\\) and adjusted \\(R^2\\)), pearson-correlation.\nsource\n\n\n\nrank-epsilon-sq\n\n(rank-epsilon-sq xss)\n\nCalculates Rank Epsilon-squared (ε²), a measure of effect size for the Kruskal-Wallis H-test.\nRank Epsilon-squared is a non-parametric measure quantifying the proportion of the total variability (based on ranks) in the dependent variable that is associated with group membership (the independent variable). It is analogous to Eta-squared or Epsilon-squared in one-way ANOVA but used for the rank-based Kruskal-Wallis test.\nThis function calculates Epsilon-squared based on the Kruskal-Wallis H statistic (H) and the total number of observations (n) across all groups.\nParameters:\n\nxss (sequence of sequences): A collection where each element is a sequence representing a group of observations, as used in kruskal-test.\n\nReturns the calculated Rank Epsilon-squared value as a double, ranging from 0 to 1.\nInterpretation:\n\nA value of 0 indicates no difference in the distributions across groups.\nA value closer to 1 indicates that a large proportion of the variability is due to differences between group ranks.\n\nRank Epsilon-squared is a useful supplement to the Kruskal-Wallis test, providing a measure of the magnitude of the group effect that is not sensitive to assumptions about the data distribution shape (beyond having similar shapes for valid interpretation of the Kruskal-Wallis test itself).\nSee also kruskal-test, rank-eta-sq (another rank-based effect size).\nsource\n\n\n\nrank-eta-sq\n\n(rank-eta-sq xss)\n\nCalculates the Rank Eta-squared (η²), an effect size measure for the Kruskal-Wallis H-test.\nRank Eta-squared is a non-parametric measure quantifying the proportion of the total variability (based on ranks) in the dependent variable that is associated with group membership (the independent variable). It is analogous to Eta-squared in one-way ANOVA but used for the rank-based Kruskal-Wallis test.\nThe statistic is calculated based on the Kruskal-Wallis H statistic, the number of groups (k), and the total number of observations (n).\nParameters:\n\nxss (sequence of sequences): A collection where each element is a sequence representing a group of observations, as used in kruskal-test.\n\nReturns the calculated Rank Eta-squared value as a double, ranging from 0 to 1.\nInterpretation: - A value of 0 indicates no difference in the distributions across groups (all variability is within groups). - A value closer to 1 indicates that a large proportion of the variability is due to differences between group ranks.\nRank Eta-squared is a useful supplement to the Kruskal-Wallis test, providing a measure of the magnitude of the group effect that is not sensitive to assumptions about the data distribution shape (beyond having similar shapes for valid interpretation of the Kruskal-Wallis test itself).\nSee also kruskal-test, rank-epsilon-sq (another rank-based effect size).\nsource\n\n\n\nremove-outliers\n\n(remove-outliers vs)\n(remove-outliers vs estimation-strategy)\n(remove-outliers vs q1 q3)\n\nRemove outliers defined as values outside inner fences.\nLet Q1 is 25-percentile and Q3 is 75-percentile. IQR is (- Q3 Q1).\n\nLIF (Lower Inner Fence) equals (- Q1 (* 1.5 IQR)).\nUIF (Upper Inner Fence) equals (+ Q3 (* 1.5 IQR)).\n\nReturns a sequence without outliers.\nOptional estimation-strategy argument can be set to change quantile calculations estimation type. See estimation-strategies.\nsource\n\n\n\nrescale\n\n(rescale vs)\n(rescale vs low high)\n\nLineary rascale data to desired range, [0,1] by default\nsource\n\n\n\nrmse\n\n(rmse [vs1 vs2-or-val])\n(rmse vs1 vs2-or-val)\n\nCalculates the Root Mean Squared Error (RMSE) between two sequences or a sequence and a constant value.\nRMSE is the square root of the mse (Mean Squared Error). It represents the standard deviation of the residuals (prediction errors) and has the same units as the original data, making it more interpretable than MSE. It measures the average magnitude of the errors, penalizing larger errors more than smaller ones due to the squaring involved.\nParameters:\n\nvs1 (sequence of numbers): The first sequence (often the observed or true values).\nvs2-or-val (sequence of numbers or single number): The second sequence (often the predicted or reference values), or a single number to compare against each element of vs1.\n\nIf both inputs are sequences, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the calculated Root Mean Squared Error as a double.\nSee also mse (Mean Squared Error), rss (Residual Sum of Squares), me (Mean Error), mae (Mean Absolute Error), r2 (Coefficient of Determination).\nsource\n\n\n\nrobust-standardize\n\n(robust-standardize vs)\n(robust-standardize vs q)\n\nNormalize samples to have median = 0 and MAD = 1.\nIf q argument is used, scaling is done by quantile difference (Q_q, Q_(1-q)). Set 0.25 for IQR.\nsource\n\n\n\nrows-&gt;contingency-table\n\n(rows-&gt;contingency-table xss)\n\nConverts a sequence of sequences (representing rows of counts) into a map-based contingency table.\nThis function takes a collection where each inner sequence is treated as a row of counts in a grid or matrix. It transforms this matrix representation into a map where keys are [row-index, column-index] tuples and values are the non-zero counts at that intersection.\nThis is particularly useful for converting structured count data, like the output of some grouping or tabulation processes, into a format suitable for functions expecting a contingency table map (like contingency-table-&gt;marginals or chi-squared tests).\nParameters:\n\nxss (sequence of sequences of numbers): A collection where each inner sequence xs_i contains counts for row i. Values within xs_i are interpreted as counts for columns 0, 1, ....\n\nReturns a map where keys are [row-index, column-index] vectors and values are the corresponding non-zero counts from the input matrix. Zero counts are omitted from the output map.\nSee also contingency-table (for building tables from raw data), contingency-table-&gt;marginals.\nsource\n\n\n\nrss\n\n(rss [vs1 vs2-or-val])\n(rss vs1 vs2-or-val)\n\nCalculates the Residual Sum of Squares (RSS) between two sequences or a sequence and a constant value.\nRSS is a measure of the discrepancy between data and a model, often used in regression analysis to quantify the total squared difference between observed values and predicted (or reference) values.\nParameters:\n\nvs1 (sequence of numbers): The first sequence (often observed values).\nvs2-or-val (sequence of numbers or single number): The second sequence (often predicted or reference values), or a single number to compare against each element of vs1.\n\nIf both sequences (vs1 and vs2) are provided, they must have the same length. If vs2-or-val is a single number, it is effectively treated as a sequence of that number repeated count(vs1) times.\nReturns the calculated Residual Sum of Squares as a double.\nSee also mse (Mean Squared Error), rmse (Root Mean Squared Error), r2 (Coefficient of Determination).\nsource\n\n\n\nsecond-moment DEPRECATED\nDeprecated: Use moment function\nsource\n\n\n\nsem\n\n(sem vs)\n\nCalculates the Standard Error of the Mean (SEM) for a sequence vs.\nThe SEM estimates the standard deviation of the sample mean, providing an indication of how accurately the sample mean represents the population mean. It is calculated as:\nSEM = stddev(vs) / sqrt(count(vs))\nwhere stddev(vs) is the sample standard deviation and count(vs) is the sample size.\nParameters:\n\nvs: Sequence of numbers.\n\nReturns the calculated SEM as a double.\nA smaller SEM indicates that the sample mean is likely to be a more precise estimate of the population mean.\nSee also stddev, mean.\nsource\n\n\n\nsem-extent\n\n(sem-extent vs)\n\n-/+ sem and mean\nsource\n\n\n\nsimilarity\n\n(similarity method P-observed Q-expected)\n(similarity method P-observed Q-expected {:keys [bins probabilities? epsilon], :or {probabilities? true, epsilon 1.0E-6}})\n\nVarious PDF similarities between two histograms (frequencies) or probabilities.\nQ can be a distribution object. Then, histogram will be created out of P.\nArguments:\n\nmethod - distance method\nP-observed - frequencies, probabilities or actual data (when Q is a distribution)\nQ-expected - frequencies, probabilities or distribution object (when P is a data)\n\nOptions:\n\n:probabilities? - should P/Q be converted to a probabilities, default: true.\n:epsilon - small number which replaces 0.0 when division or logarithm is used`\n:bins - number of bins or bins estimation method, see histogram.\n\nThe list of methods: :intersection, :czekanowski, :motyka, :kulczynski, :ruzicka, :inner-product, :harmonic-mean, :cosine, :jaccard, :dice, :fidelity, :squared-chord\nSee more: Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions by Sung-Hyuk Cha\nsource\n\n\n\nskewness\n\n(skewness vs)\n(skewness vs typ)\n\nCalculate skewness from sequence, a measure of the asymmetry of the probability distribution about its mean.\nParameters:\n\nvs (seq of numbers): The input sequence.\ntyp (keyword or sequence, optional): Specifies the type of skewness measure to calculate. Defaults to :G1.\n\nAvailable typ values:\n\n:G1 (Default): Sample skewness based on the third standardized moment, as implemented by Apache Commons Math Skewness. Adjusted for sample size bias.\n:g1 or :pearson: Pearson’s moment coefficient of skewness (g1), a bias-adjusted version of the third standardized moment. Expected value 0 for symmetric distributions.\n:b1: Sample skewness coefficient (b1), related to :g1.\n:B1 or :yule: Yule’s coefficient (robust), based on quantiles. Takes an optional quantile u (default 0.25) via sequence [:B1 u] or [:yule u].\n:B3: Robust measure comparing the mean and median relative to the mean absolute deviation around the median.\n:skew: An adjusted skewness definition sometimes used in bootstrap (BCa) calculations.\n:mode: Pearson’s second skewness coefficient: (mean - mode) / stddev. Requires calculating the mode. Mode calculation method can be specified via sequence [:mode method opts], see mode.\n:median: Robust measure: 3 * (mean - median) / stddev.\n:bowley: Bowley’s coefficient (robust), based on quartiles (Q1, Q2, Q3). Also known as Yule-Bowley coefficient. Calculated as (Q3 + Q1 - 2*Q2) / (Q3 - Q1).\n:hogg: Hogg’s robust measure based on the ratio of differences between trimmed means.\n:l-skewness: L-skewness (τ₃), the ratio of the 3rd L-moment (λ₃) to the 2nd L-moment (λ₂, L-scale). Calculated directly using l-moment with the :ratio? option set to true. It’s a robust measure of asymmetry. Expected value 0 for symmetric distributions.\n\nInterpretation:\n\nPositive values generally indicate a distribution skewed to the right (tail is longer on the right).\nNegative values generally indicate a distribution skewed to the left (tail is longer on the left).\nValues near 0 suggest relative symmetry.\n\nReturns the calculated skewness value as a double.\nSee also skewness-test, normality-test, jarque-bera-test, l-moment.\nsource\n\n\n\nskewness-test\n\n(skewness-test xs)\n(skewness-test xs params)\n(skewness-test xs skew {:keys [sides type], :or {sides :two-sided, type :g1}})\n\nPerforms the D’Agostino test for normality based on sample skewness.\nThis test assesses the null hypothesis that the data comes from a normally distributed population by checking if the sample skewness significantly deviates from the zero skewness expected under normality.\nThe test works by:\n\nCalculating the sample skewness (type configurable via :type, default :g1).\nStandardizing the sample skewness relative to its expected value (0) and standard error under the null hypothesis.\nApplying a further transformation (inverse hyperbolic sine based) to this standardized score to yield a final test statistic Z that more closely follows a standard normal distribution under the null hypothesis.\n\nParameters:\n\nxs (seq of numbers): The sample data.\nskew (double, optional): A pre-calculated skewness value. If omitted, it’s calculated from xs.\nparams (map, optional): Options map:\n\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis.\n\n:two-sided (default): The population skewness is different from 0.\n:one-sided-greater: The population skewness is greater than 0 (right-skewed).\n:one-sided-less: The population skewness is less than 0 (left-skewed).\n\n:type (keyword, default :g1): The type of skewness to calculate if skew is not provided. Note that the internal normalization constants are derived based on :g1. See skewness for options.\n\n\nReturns a map containing:\n\n:Z: The final test statistic, approximately standard normal under H0.\n:stat: Alias for :Z.\n:p-value: The p-value associated with Z and the specified :sides.\n:skewness: The sample skewness value used in the test (either provided or calculated).\n\nSee also kurtosis-test, normality-test, jarque-bera-test.\nsource\n\n\n\nspan\n\n(span vs)\n\nWidth of the sample, maximum value minus minimum value\nsource\n\n\n\nspearman-correlation\n\n(spearman-correlation [vs1 vs2])\n(spearman-correlation vs1 vs2)\n\nCalculates Spearman’s rank correlation coefficient between two sequences.\nSpearman’s rank correlation is a non-parametric measure of the monotonic relationship between two datasets. It assesses how well the relationship between two variables can be described using a monotonic function. It does not require the data to be linearly related or follow a specific distribution. The coefficient is calculated on the ranks of the data rather than the raw values.\nParameters:\n\n[vs1 vs2] (sequence of two sequences): A sequence containing the two sequences of numbers.\nvs1, vs2 (sequences): The two sequences of numbers directly as arguments.\n\nBoth sequences must have the same length.\nReturns the calculated Spearman rank correlation coefficient (a value between -1.0 and 1.0) as a double. A value of 1 indicates a perfect monotonic increasing relationship, -1 a perfect monotonic decreasing relationship, and 0 no monotonic relationship.\nSee also pearson-correlation, kendall-correlation, correlation.\nsource\n\n\n\nstandardize\n\n(standardize vs)\n\nNormalize samples to have mean = 0 and stddev = 1.\nsource\n\n\n\nstats-map\n\n(stats-map vs)\n(stats-map vs estimation-strategy)\n\nCalculates a comprehensive set of descriptive statistics for a numerical dataset.\nThis function computes various summary measures and returns them as a map, providing a quick overview of the data’s central tendency, dispersion, shape, and potential outliers.\nParameters:\n\nvs (seq of numbers): The input sequence of numerical data.\nestimation-strategy (keyword, optional): Specifies the method for calculating quantiles (including median, quartiles, and values used for fences). Defaults to :legacy. See percentile or quantile for available strategies (e.g., :r1 through :r9).\n\nReturns a map where keys are statistic names (as keywords) and values are their calculated measures:\n\n:Size: The number of data points in the sequence (count).\n:Min: The minimum value (see minimum).\n:Max: The maximum value (see maximum).\n:Range: The difference between the maximum and minimum values (Max - Min).\n:Mean: The arithmetic average (see mean).\n:Median: The middle value (see median with estimation-strategy).\n:Mode: The most frequent value (see mode with default method).\n:Q1: The first quartile (25th percentile) (see percentile with estimation-strategy).\n:Q3: The third quartile (75th percentile) (see percentile with estimation-strategy).\n:Total: The sum of all values (see sum).\n:SD: The sample standard deviation (see stddev).\n:Variance: The sample variance (SD^2, see variance).\n:MAD: The Median Absolute Deviation (see median-absolute-deviation).\n:SEM: The Standard Error of the Mean (see sem).\n:LAV: The Lower Adjacent Value (smallest value within the inner fence, see adjacent-values).\n:UAV: The Upper Adjacent Value (largest value within the inner fence, see adjacent-values).\n:IQR: The Interquartile Range (Q3 - Q1).\n:LOF: The Lower Outer Fence (Q1 - 3*IQR, see outer-fence-extent).\n:UOF: The Upper Outer Fence (Q3 + 3*IQR, see outer-fence-extent).\n:LIF: The Lower Inner Fence (Q1 - 1.5*IQR, see inner-fence-extent).\n:UIF: The Upper Inner Fence (Q3 + 1.5*IQR, see inner-fence-extent).\n:Outliers: A sequence of data points falling outside the inner fences (see outliers).\n:Kurtosis: A measure of tailedness/peakedness (see kurtosis with default :G2 type).\n:Skewness: A measure of asymmetry (see skewness with default :G1 type).\n\nThis function is a convenient way to get a standard set of summary statistics for a dataset in a single call.\nsource\n\n\n\nstddev\n\n(stddev vs)\n(stddev vs mu)\n\nCalculate standard deviation of vs.\nSee population-stddev.\nsource\n\n\n\nstddev-extent\n\n(stddev-extent vs)\n\n-/+ stddev and mean\nsource\n\n\n\nsum\n\n(sum vs)\n(sum vs compensation-method)\n\nSum of all vs values.\nPossible compensated summation methods are: :kahan, :neumayer and :klein\nsource\n\n\n\nt-test-one-sample\n\n(t-test-one-sample xs)\n(t-test-one-sample xs m)\n\nPerforms a one-sample Student’s t-test to compare the sample mean against a hypothesized population mean.\nThis test assesses the null hypothesis that the true population mean is equal to mu. It is suitable when the population standard deviation is unknown and is estimated from the sample.\nParameters:\n\nxs (seq of numbers): The sample data.\nparams (map, optional): Options map:\n\n:alpha (double, default 0.05): Significance level for the confidence interval.\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis.\n\n:two-sided (default): The true mean is not equal to mu.\n:one-sided-greater: The true mean is greater than mu.\n:one-sided-less: The true mean is less than mu.\n\n:mu (double, default 0.0): The hypothesized population mean under the null hypothesis.\n\n\nReturns a map containing:\n\n:t: The calculated t-statistic.\n:stat: Alias for :t.\n:df: Degrees of freedom (n-1).\n:p-value: The p-value associated with the t-statistic and :sides.\n:confidence-interval: Confidence interval for the true population mean.\n:estimate: The calculated sample mean.\n:n: The sample size.\n:mu: The hypothesized population mean used in the test.\n:stderr: The standard error of the mean (calculated from the sample).\n:alpha: Significance level used.\n:sides: Alternative hypothesis side used.\n:test-type: Alias for :sides.\n\nAssumptions:\n\nThe data are independent observations.\nThe data are drawn from a population that is approximately normally distributed. (The t-test is relatively robust to moderate violations, especially with larger sample sizes).\n\nSee also z-test-one-sample for large samples or known population standard deviation.\nsource\n\n\n\nt-test-two-samples\n\n(t-test-two-samples xs ys)\n(t-test-two-samples xs ys {:keys [paired? equal-variances?], :or {paired? false, equal-variances? false}, :as params})\n\nPerforms a two-sample Student’s t-test to compare the means of two samples.\nThis function can perform:\n\nAn unpaired t-test (assuming independent samples) using either:\n\nWelch’s t-test (default: :equal-variances? false): Does not assume equal population variances. Uses the Satterthwaite approximation for degrees of freedom. Recommended unless variances are known to be equal.\nStudent’s t-test (:equal-variances? true): Assumes equal population variances and uses a pooled variance estimate.\n\nA paired t-test (:paired? true): Assumes observations in xs and ys are paired (e.g., before/after measurements on the same subjects). This performs a one-sample t-test on the differences between paired observations.\n\nThe test assesses the null hypothesis that the true difference between the population means (or the mean of the differences for paired test) is equal to mu.\nParameters:\n\nxs (seq of numbers): The first sample.\nys (seq of numbers): The second sample.\nparams (map, optional): Options map:\n\n:alpha (double, default 0.05): Significance level for the confidence interval.\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis.\n\n:two-sided (default): The true difference in means is not equal to mu.\n:one-sided-greater: The true difference (mean(xs) - mean(ys) or mean(diff)) is greater than mu.\n:one-sided-less: The true difference (mean(xs) - mean(ys) or mean(diff)) is less than mu.\n\n:mu (double, default 0.0): The hypothesized difference in means under the null hypothesis.\n:paired? (boolean, default false): If true, performs a paired t-test (requires xs and ys to have the same length). If false, performs an unpaired test.\n:equal-variances? (boolean, default false): Used only when paired? is false. If true, assumes equal population variances (Student’s). If false, does not assume equal variances (Welch’s).\n\n\nReturns a map containing:\n\n:t: The calculated t-statistic.\n:stat: Alias for :t.\n:df: Degrees of freedom used for the t-distribution.\n:p-value: The p-value associated with the t-statistic and :sides.\n:confidence-interval: Confidence interval for the true difference in means.\n:estimate: The observed difference between sample means (mean(xs) - mean(ys) or mean(differences)).\n:n: Sample sizes as [count xs, count ys] (or count diffs if paired).\n:nx: Sample size of xs (if unpaired).\n:ny: Sample size of ys (if unpaired).\n:estimated-mu: Observed sample means as [mean xs, mean ys] (if unpaired).\n:mu: The hypothesized difference under the null hypothesis.\n:stderr: The standard error of the difference between the means (or of the mean difference if paired).\n:alpha: Significance level used.\n:sides: Alternative hypothesis side used.\n:test-type: Alias for :sides.\n:paired?: Boolean indicating if a paired test was performed.\n:equal-variances?: Boolean indicating the variance assumption used (if unpaired).\n\nAssumptions: - Independence of observations (within and between groups for unpaired). - Normality of the underlying populations (or of the differences for paired). The t-test is relatively robust to violations of normality, especially with larger sample sizes. - Equal variances (only if :equal-variances? true).\nsource\n\n\n\ntrim\n\n(trim vs)\n(trim vs quantile)\n(trim vs quantile estimation-strategy)\n(trim vs low high nan)\n\nReturn trimmed data. Trim is done by using quantiles, by default is set to 0.2.\nsource\n\n\n\ntrim-lower\n\n(trim-lower vs)\n(trim-lower vs quantile)\n(trim-lower vs quantile estimation-strategy)\n\nTrim data below given quanitle, default: 0.2.\nsource\n\n\n\ntrim-upper\n\n(trim-upper vs)\n(trim-upper vs quantile)\n(trim-upper vs quantile estimation-strategy)\n\nTrim data above given quanitle, default: 0.2.\nsource\n\n\n\ntschuprows-t\n\n(tschuprows-t group1 group2)\n(tschuprows-t contingency-table)\n\nCalculates Tschuprow’s T, a measure of association between two nominal variables represented in a contingency table.\nTschuprow’s T is derived from the Pearson’s Chi-squared statistic and measures the strength of the association. Its value ranges from 0 to 1.\n\nA value of 0 indicates no association between the variables.\nA value of 1 indicates perfect association, but only when the number of rows (r) equals the number of columns (k) in the contingency table. If r != k, Tschuprow’s T cannot reach 1, making Cramer’s V (cramers-v) often preferred as it can reach 1 for any table size.\n\nThe function can be called in two ways:\n\nWith two sequences group1 and group2: The function will automatically construct a contingency table from the unique values in the sequences.\nWith a contingency table: The contingency table can be provided as:\n\nA map where keys are [row-index, column-index] tuples and values are counts (e.g., {[0 0] 10, [0 1] 5, [1 0] 3, [1 1] 12}). This is the output format of contingency-table with two inputs.\nA sequence of sequences representing the rows of the table (e.g., [10 5] [3 12](#LOS-10 5] [3 12)). This is equivalent to rows-&gt;contingency-table.\n\n\nParameters:\n\ngroup1 (sequence): The first sequence of categorical data.\ngroup2 (sequence): The second sequence of categorical data. Must have the same length as group1.\ncontingency-table (map or sequence of sequences): A pre-computed contingency table.\n\nReturns the calculated Tschuprow’s T coefficient as a double.\nSee also chisq-test, cramers-c, cramers-v, cohens-w, contingency-table.\nsource\n\n\n\nttest-one-sample DEPRECATED\nDeprecated: Use t-test-one-sample\nsource\n\n\n\nttest-two-samples DEPRECATED\nDeprecated: Use t-test-two-samples\nsource\n\n\n\nvariance\n\n(variance vs)\n(variance vs mu)\n\nCalculate variance of vs.\nSee population-variance.\nsource\n\n\n\nvariation\n\n(variation vs)\n\nCalculates the coefficient of variation (CV) for a sequence vs.\nThe CV is a standardized measure of dispersion of a probability distribution or frequency distribution. It is defined as the ratio of the standard deviation to the mean:\nCV = stddev(vs) / mean(vs)\nThis measure is unitless and allows for comparison of variability between datasets with different means or different units.\nParameters:\n\nvs: Sequence of numbers.\n\nReturns the calculated coefficient of variation as a double.\nNote: The CV is undefined if the mean is zero, and may be misleading if the mean is close to zero or if the data can take both positive and negative values. All values in vs should ideally be positive.\nSee also stddev, mean.\nsource\n\n\n\nweighted-kappa\n\n(weighted-kappa contingency-table)\n(weighted-kappa contingency-table weights)\n\nCalculates Cohen’s weighted Kappa coefficient (κ) for a contingency table, allowing for partial agreement between categories, typically used for ordinal data.\nWeighted Kappa measures inter-rater agreement, similar to cohens-kappa, but assigns different penalties to disagreements based on their magnitude. Disagreements between closely related categories are penalized less than disagreements between distantly related categories.\nThe function can be called in two ways:\n\nWith two sequences group1 and group2: The function will automatically construct a contingency table from the unique values in the sequences. These values are assumed to be ordinal and their position in the sorted unique value list determines their index. The mapping of values to table indices might need verification.\nWith a contingency table: The contingency table can be provided as:\n\nA map where keys are [row-index, column-index] tuples and values are counts (e.g., {[0 0] 10, [0 1] 5, [1 0] 3, [1 1] 12}). This is the output format of contingency-table with two inputs. Indices are assumed to represent the ordered categories.\nA sequence of sequences representing the rows of the table (e.g., [10 5] [3 12](#LOS-10 5] [3 12)). This is equivalent to rows-&gt;contingency-table. The row and column indices are assumed to correspond to the ordered categories.\n\n\nParameters:\n\ngroup1 (sequence): The first sequence of ordinal outcomes/categories.\ngroup2 (sequence): The second sequence of ordinal outcomes/categories. Must have the same length as group1.\ncontingency-table (map or sequence of sequences): A pre-computed contingency table where row and column indices correspond to ordered categories.\nweights (keyword, function, or map, optional): Specifies the weighting scheme to quantify the difference between categories. Defaults to :equal-spacing.\n\n:equal-spacing (default, linear weights): Penalizes disagreements linearly with the distance between categories. Weight is 1 - |i-j|/R, where i is row index, j is column index, and R is the maximum dimension of the table (max(max_row_index, max_col_index)).\n:fleiss-cohen (quadratic weights): Penalizes disagreements quadratically with the distance. Weight is 1 - (|i-j|/R)^2.\n(function (fn [R id1 id2])): A custom function that takes the maximum dimension R, row index id1, and column index id2 and returns the weight (typically between 0 and 1, where 1 is perfect agreement).\n(map {[id1 id2] weight}): A custom map providing weights for specific [row-index, column-index] pairs. Missing pairs default to a weight of 0.0.\n\n\nReturns the calculated weighted Cohen’s Kappa coefficient as a double.\nInterpretation:\n\nκ_w = 1: Perfect agreement.\nκ_w = 0: Agreement is no better than chance.\nκ_w &lt; 0: Agreement is worse than chance.\n\nSee also cohens-kappa (unweighted Kappa).\nsource\n\n\n\nwinsor\n\n(winsor vs)\n(winsor vs quantile)\n(winsor vs quantile estimation-strategy)\n(winsor vs low high nan)\n\nReturn winsorized data. Trim is done by using quantiles, by default is set to 0.2.\nsource\n\n\n\nwmean DEPRECATED\nDeprecated: Use mean\n\n(wmean vs)\n(wmean vs weights)\n\nWeighted mean\nsource\n\n\n\nwmedian\n\n(wmedian vs ws)\n(wmedian vs ws method)\n\nCalculates median of a sequence vs with corresponding weights ws.\nParameters:\n\nvs: Sequence of data values.\nws: Sequence of corresponding non-negative weights. Must have the same count as vs.\nmethod (optional keyword): Specifies the interpolation method used when qs falls between points in the weighted ECDF. Defaults to :linear.\n\n:linear: Performs linear interpolation between the data values corresponding to the cumulative weights surrounding q=0.5.\n:step: Uses a step function (specifically, step-before) based on the weighted ECDF. The result is the data value whose cumulative weight range includes q=0.5.\n:average: Computes the average of the step-before and step-after interpolation methods.\n\n\nSee also: wquantile, quantile.\nsource\n\n\n\nwmode\n\n(wmode vs)\n(wmode vs weights)\n\nReturns the primary weighted mode of a sequence vs.\nThe mode is the value that appears most often in a dataset. This function generalizes the mode concept by considering weights associated with each value. A value’s contribution to the mode calculation is proportional to its weight.\nIf multiple values share the same highest total weight (i.e., there are ties for the mode), this function returns only the first one encountered during processing. The specific mode returned in case of a tie is not guaranteed to be stable across different runs or environments. Use wmodes if you need all tied modes.\nParameters:\n\nvs: Sequence of data values. Can contain any data type (numbers, keywords, etc.).\nweights (optional): Sequence of non-negative weights corresponding to vs. Must have the same count as vs. Defaults to a sequence of 1.0s if omitted, effectively calculating the unweighted mode.\n\nReturns a single value representing the mode (or one of the modes if ties exist).\nSee also wmodes (returns all modes) and mode (for unweighted numeric data).\nsource\n\n\n\nwmodes\n\n(wmodes vs)\n(wmodes vs weights)\n\nReturns the weighted mode(s) of a sequence vs.\nThe mode is the value that appears most often in a dataset. This function generalizes the mode concept by considering weights associated with each value. A value’s contribution to the mode calculation is proportional to its weight.\nParameters:\n\nvs: Sequence of data values. Can contain any data type (numbers, keywords, etc.).\nweights (optional): Sequence of non-negative weights corresponding to vs. Must have the same count as vs. Defaults to a sequence of 1.0s if omitted, effectively calculating the unweighted modes.\n\nReturns a sequence containing all values that have the highest total weight. If there are ties (multiple values share the same maximum total weight), all tied values are included in the returned sequence. The order of modes in the returned sequence is not guaranteed.\nSee also wmode (returns only one mode in case of ties) and modes (for unweighted numeric data).\nsource\n\n\n\nwmw-odds\n\n(wmw-odds [group1 group2])\n(wmw-odds group1 group2)\n\nCalculates the Wilcoxon-Mann-Whitney odds (often denoted as ψ) for two independent samples.\nThis non-parametric effect size measure quantifies the odds that a randomly chosen observation from the first group (group1) is greater than a randomly chosen observation from the second group (group2).\nThe statistic is directly related to cliffs-delta (δ): ψ = (1 + δ) / (1 - δ).\nParameters:\n\ngroup1 (seq of numbers): The first independent sample.\ngroup2 (seq of numbers): The second independent sample.\n\nReturns the calculated WMW odds as a double.\nInterpretation:\n\nA value greater than 1 indicates that values from group1 tend to be larger than values from group2.\nA value less than 1 indicates that values from group1 tend to be smaller than values from group2.\nA value of 1 indicates stochastic equality between the distributions (50/50 odds).\n\nThis measure is robust to violations of normality and is suitable for ordinal data. It is closely related to Cliff’s Delta (δ) and the Mann-Whitney U test statistic.\nSee also cliffs-delta, ameasure.\nsource\n\n\n\nwquantile\n\n(wquantile vs ws q)\n(wquantile vs ws q method)\n\nCalculates the q-th weighted quantile of a sequence vs with corresponding weights ws.\nThe quantile q is a value between 0.0 and 1.0, inclusive.\nThe calculation involves constructing a weighted empirical cumulative distribution function (ECDF) and interpolating to find the value at quantile q.\nParameters:\n\nvs: Sequence of data values.\nws: Sequence of corresponding non-negative weights. Must have the same count as vs.\nq: The quantile level (0.0 &lt; q &lt;= 1.0).\nmethod (optional keyword): Specifies the interpolation method used when q falls between points in the weighted ECDF. Defaults to :linear.\n\n:linear: Performs linear interpolation between the data values corresponding to the cumulative weights surrounding q.\n:step: Uses a step function (specifically, step-before) based on the weighted ECDF. The result is the data value whose cumulative weight range includes q.\n:average: Computes the average of the step-before and step-after interpolation methods. Useful when q corresponds exactly to a cumulative weight boundary.\n\n\nSee also: wmedian, wquantiles, quantile.\nsource\n\n\n\nwquantiles\n\n(wquantiles vs ws)\n(wquantiles vs ws qs)\n(wquantiles vs ws qs method)\n\nCalculates the sequence of q-th weighted quantiles of a sequence vs with corresponding weights ws.\nQuantiles qs is a sequence of values between 0.0 and 1.0, inclusive.\nThe calculation involves constructing a weighted empirical cumulative distribution function (ECDF) and interpolating to find the value at quantiles qs.\nParameters:\n\nvs: Sequence of data values.\nws: Sequence of corresponding non-negative weights. Must have the same count as vs.\nqs: Sequence of quantiles level (0.0 &lt; q &lt;= 1.0).\nmethod (optional keyword): Specifies the interpolation method used when qs falls between points in the weighted ECDF. Defaults to :linear.\n\n:linear: Performs linear interpolation between the data values corresponding to the cumulative weights surrounding q.\n:step: Uses a step function (specifically, step-before) based on the weighted ECDF. The result is the data value whose cumulative weight range includes q.\n:average: Computes the average of the step-before and step-after interpolation methods. Useful when q corresponds exactly to a cumulative weight boundary.\n\n\nSee also: wquantile, quantiles.\nsource\n\n\n\nwstddev\n\n(wstddev vs freqs)\n\nCalculate weighted (unbiased) standard deviation of vs\nsource\n\n\n\nwvariance\n\n(wvariance vs freqs)\n\nCalculate weighted (unbiased) variance of vs.\nsource\n\n\n\nyeo-johnson-infer-lambda\n\n(yeo-johnson-infer-lambda xs)\n(yeo-johnson-infer-lambda xs lambda-range)\n(yeo-johnson-infer-lambda xs lambda-range {:keys [alpha], :or {alpha 0.0}})\n\nFind optimal lambda parameter for Yeo-Johnson tranformation using maximum log likelihood method.\nsource\n\n\n\nyeo-johnson-transformation\n\n(yeo-johnson-transformation xs)\n(yeo-johnson-transformation xs lambda)\n(yeo-johnson-transformation xs lambda {:keys [alpha inverse?], :or {alpha 0.0}})\n\nApplies the Yeo-Johnson transformation to a dataset.\nThis transformation is used to stabilize variance and make data more normally distributed. It extends the Box-Cox transformation to allow for zero and negative values.\nParameters:\n\nxs: The input dataset.\nlambda (default: 0.0): The power parameter controlling the transformation. If lambda is nil or a range [lambda-min, lambda-max] it will be inferred using maximum log-likelihood method.\nOptions map:\n\n:alpha (optional): A shift parameter applied before transformation.\n:inverse? (optional): Perform inverse operation, lambda should be provided (can’t be inferred).\n\n\nReturns:\n\nA transformed sequence of numbers.\n\nRelated: box-cox-tranformation\nsource\n\n\n\nz-test-one-sample\n\n(z-test-one-sample xs)\n(z-test-one-sample xs m)\n\nPerforms a one-sample Z-test to compare the sample mean against a hypothesized population mean.\nThis test assesses the null hypothesis that the true population mean is equal to mu. It typically assumes either a known population standard deviation or relies on a large sample size (e.g., n &gt; 30) where the sample standard deviation provides a reliable estimate. This implementation uses the sample standard deviation to calculate the standard error.\nParameters:\n\nxs (seq of numbers): The sample data.\nparams (map, optional): Options map:\n\n:alpha (double, default 0.05): Significance level for the confidence interval.\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis.\n\n:two-sided (default): The true mean is not equal to mu.\n:one-sided-greater: The true mean is greater than mu.\n:one-sided-less: The true mean is less than mu.\n\n:mu (double, default 0.0): The hypothesized population mean under the null hypothesis.\n\n\nReturns a map containing:\n\n:z: The calculated Z-statistic.\n:stat: Alias for :z.\n:p-value: The p-value associated with the Z-statistic and the specified :sides.\n:confidence-interval: Confidence interval for the true population mean.\n:estimate: The calculated sample mean.\n:n: The sample size.\n:mu: The hypothesized population mean used in the test.\n:stderr: The standard error of the mean (calculated using sample standard deviation).\n:alpha: Significance level used.\n:sides: Alternative hypothesis side used.\n:test-type: Alias for :sides.\n\nSee also t-test-one-sample for smaller samples or when the population standard deviation is unknown.\nsource\n\n\n\nz-test-two-samples\n\n(z-test-two-samples xs ys)\n(z-test-two-samples xs ys {:keys [paired? equal-variances?], :or {paired? false, equal-variances? false}, :as params})\n\nPerforms a two-sample Z-test to compare the means of two independent or paired samples.\nThis test assesses the null hypothesis that the difference between the population means is equal to mu (default 0). It typically assumes known population variances or relies on large sample sizes where sample variances provide good estimates. This implementation calculates the standard error using the provided sample variances.\nParameters:\n\nxs (seq of numbers): The first sample.\nys (seq of numbers): The second sample.\nparams (map, optional): Options map:\n\n:alpha (double, default 0.05): Significance level for the confidence interval.\n:sides (keyword, default :two-sided): Specifies the alternative hypothesis.\n\n:two-sided (default): The true difference in means is not equal to mu.\n:one-sided-greater: The true difference in means (mean(xs) - mean(ys)) is greater than mu.\n:one-sided-less: The true difference in means (mean(xs) - mean(ys)) is less than mu.\n\n:mu (double, default 0.0): The hypothesized difference in means under the null hypothesis.\n:paired? (boolean, default false): If true, performs a paired Z-test by applying z-test-one-sample to the differences between paired observations in xs and ys (requires xs and ys to have the same length). If false, performs a two-sample test assuming independence.\n:equal-variances? (boolean, default false): Used only when paired? is false. If true, assumes population variances are equal and calculates a pooled standard error. If false, calculates the standard error without assuming equal variances (Welch’s approach adapted for Z-test). This affects the standard error calculation but the standard normal distribution is still used for inference.\n\n\nReturns a map containing:\n\n:z: The calculated Z-statistic.\n:stat: Alias for :z.\n:p-value: The p-value associated with the Z-statistic and the specified :sides.\n:confidence-interval: Confidence interval for the true difference in means.\n:estimate: The observed difference between sample means (mean(xs) - mean(ys)).\n:n: Sample sizes as [count xs, count ys].\n:nx: Sample size of xs.\n:ny: Sample size of ys.\n:estimated-mu: The observed sample means as [mean xs, mean ys].\n:mu: The hypothesized difference under the null hypothesis.\n:stderr: The standard error of the difference between the means.\n:alpha: Significance level used.\n:sides: Alternative hypothesis side used.\n:test-type: Alias for :sides.\n:paired?: Boolean indicating if a paired test was performed.\n:equal-variances?: Boolean indicating the assumption used for standard error calculation (if unpaired).\n\nSee also t-test-two-samples for smaller samples or when population variances are unknown.\nsource\n\n\n\nfastmath.stats.bootstrap\nBootstrap methods and confidence intervals\n\n\nbootstrap\n\n(bootstrap input)\n(bootstrap input params-or-statistic)\n(bootstrap input statistic {:keys [rng samples size method antithetic? dimensions include? multi?], :or {samples 500}, :as params})\n\nGenerates bootstrap samples from a given dataset or probabilistic model for resampling purposes.\nThis function supports both nonparametric bootstrap (resampling directly from the data) and parametric bootstrap (resampling from a statistical model estimated from or provided for the data). It can optionally apply a statistic function to the original data and each sample, returning summary statistics for the bootstrap distribution.\nThe primary input can be:\n\nA sequence of data values (for nonparametric bootstrap).\nA map containing:\n\n:data: The sequence of data values.\n:model: An optional model for parametric bootstrap. If not provided, a default discrete distribution is built from the data (see :distribution and :smoothing).\n\n\nThe function offers various parameters to control the sampling process and model generation.\nParameters:\n\ninput (sequence or map): The data source. Can be a sequence of numbers or a map containing :data and optionally :model. Can be sequence of sequences for multidimensional data (when :dimensions is :multi).\nstatistic (function, optional): A function that takes a sequence of data and returns a single numerical value (e.g., fastmath.stats/mean, fastmath.stats/median). If provided, bootstrap-stats is called on the results.\nparams (map, optional): An options map to configure the bootstrap process. Keys include:\n\n:samples (long, default: 500): The number of bootstrap samples to generate.\n:size (long, optional): The size of each individual bootstrap sample. Defaults to the size of the original data.\n:method (keyword, optional): Specifies the sampling method.\n\nnil (default): Standard random sampling with replacement.\n:jackknife: Performs leave-one-out jackknife resampling (ignores :samples and :size).\n:jackknife+: Performs positive jackknife resampling (duplicates each observation once; ignores :samples).\nOther keywords are passed to fastmath.random/-&gt;seq for sampling from a distribution (only relevant if a :model is used or built).\n\n:rng (random number generator, optional): An instance of a random number generator (see fastmath.random/rng). A default JVM RNG is used if not provided.\n:smoothing (keyword, optional): Applies smoothing to the bootstrap process.\n\n:kde: Uses Kernel Density Estimation to smooth the empirical distribution before sampling. Requires specifying :kernel (default :gaussian) and optionally :bandwidth (auto-estimated by default).\n:gaussian: Adds random noise drawn from N(0, standard error) to each resampled value.\n\n:distribution (keyword, default: :real-discrete-distribution): The type of discrete distribution to build automatically from the data if no explicit :model is provided. Other options include :integer-discrete-distribution (for integer data) and :categorical-distribution (for any data type).\n:dimensions (keyword, optional): If set to :multi, treats the input :data as a sequence of sequences (multidimensional data). Models are built or used separately for each dimension, and samples are generated as sequences of vectors.\n:antithetic? (boolean, default: false): If true, uses antithetic sampling for variance reduction (paired samples are generated as x and 1-x from a uniform distribution, then transformed by the inverse CDF of the model). Requires sampling from a distribution model.\n:include? (boolean, default: false): If true, the original dataset is included as one of the samples in the output collection.\n\n\nModel for parametric bootstrap:\nThe :model parameter in the input map can be:\n\nAny fastmath.random distribution object (e.g., (r/distribution :normal {:mu 0 :sd 1})).\nAny 0-arity function that returns a random sample when called.\n\nIf :model is omitted from the input map, a default discrete distribution (:real-discrete-distribution by default, see :distribution param) is built from the :data. Smoothing options (:smoothing) apply to this automatically built model.\nWhen :dimensions is :multi, :model should be a sequence of models, one for each dimension.\nReturns:\n\nIf statistic is provided: A map containing the original input map augmented with analysis results from bootstrap-stats (e.g., :t0, :ts, :bias, :mean, :stddev).\nIf statistic is nil: A map containing the original input map augmented with the generated bootstrap samples in the :samples key. The :samples value is a collection of sequences, where each inner sequence is one bootstrap sample. If :dimensions is :multi, samples are sequences of vectors.\n\nSee also jackknife, jackknife+, bootstrap-stats, ci-normal, ci-basic, ci-percentile, ci-bc, ci-bca, ci-studentized, ci-t.\nsource\n\n\n\nbootstrap-stats\n\n(bootstrap-stats {:keys [data samples], :as input} statistic)\n\nCalculates summary statistics for bootstrap results.\nTakes bootstrap output (typically from bootstrap) and a statistic function, computes the statistic on the original data (t0) and on each bootstrap sample (ts), and derives various descriptive statistics from the distribution of ts.\nParameters:\n\nboot-data (map): A map containing:\n\n:data: The original dataset.\n:samples: A collection of bootstrap samples (e.g., from bootstrap).\n(optional) other keys like :model from bootstrap generation.\n\nstatistic (function): A function that accepts a sequence of data and returns a single numerical statistic (e.g., fastmath.stats/mean, fastmath.stats/median).\n\nReturns a map which is the input boot-data augmented with bootstrap analysis results:\n\n:statistic: The statistic function applied.\n:t0: The statistic calculated on the original :data.\n:ts: A sequence of the statistic calculated on each bootstrap sample in :samples.\n:bias: The estimated bias of the statistic: mean(:ts) - :t0.\n:mean, :median, :variance, :stddev, :sem: Descriptive statistics (mean, median, variance, standard deviation, standard error of the mean) calculated from the distribution of :ts.\n\nThis function prepares the results for calculating various bootstrap confidence intervals (e.g., ci-normal, ci-percentile, etc.).\nsource\n\n\n\nci-basic\n\n(ci-basic boot-data)\n(ci-basic boot-data alpha)\n(ci-basic {:keys [t0 ts]} alpha estimation-strategy)\n\nCalculates the Basic (or Percentile-t) bootstrap confidence interval.\nThis method is based on the assumption that the distribution of the bootstrap replicates (:ts) centered around the true statistic (t) is approximately the same as the distribution of the original statistic (:t0) centered around the mean of the bootstrap replicates (mean(:ts)).\nThe interval is constructed using the quantiles of the bootstrap replicates (:ts) relative to the original statistic (:t0). Specifically, the lower bound is 2 * :t0 - q_upper and the upper bound is 2 * :t0 - q_lower, where q_lower and q_upper are the alpha/2 and 1 - alpha/2 quantiles of :ts, respectively.\nParameters:\n\nboot-data (map): A map containing bootstrap results, typically from bootstrap-stats. Requires keys:\n\n:t0 (double): The statistic calculated on the original data.\n:ts (sequence of numbers): The statistic calculated on each bootstrap sample.\n\nalpha (double, optional): The significance level for the interval. Defaults to 0.05 (for a 95% CI). The interval is based on the alpha/2 and 1 - alpha/2 quantiles of the :ts distribution.\nestimation-strategy (keyword, optional): Specifies the quantile estimation strategy used to calculate the quantiles of :ts. Defaults to :legacy. See quantiles for available options (e.g., :r1 through :r9).\n\nReturns a vector [lower-bound, upper-bound, t0].\n\nlower-bound (double): The lower limit of the confidence interval.\nupper-bound (double): The upper limit of the confidence interval.\nt0 (double): The statistic calculated on the original data (from boot-data).\n\nSee also bootstrap-stats for input preparation and other confidence interval methods: ci-normal, ci-percentile, ci-bc, ci-bca, ci-studentized, ci-t, quantiles.\nsource\n\n\n\nci-bc\n\n(ci-bc boot-data)\n(ci-bc boot-data alpha)\n(ci-bc {:keys [t0 ts]} alpha estimation-strategy)\n\nCalculates the Bias-Corrected (BC) bootstrap confidence interval.\nThis method adjusts the standard Percentile bootstrap interval (ci-percentile) to account for potential bias in the statistic’s distribution. The correction is based on the proportion of bootstrap replicates of the statistic (:ts) that are less than the statistic calculated on the original data (:t0).\nThe procedure involves: 1. Calculating a bias correction factor (\\(z_0\\)) based on the empirical cumulative distribution function (CDF) of the bootstrap replicates at the point of the original statistic (\\(z_0 = \\Phi^{-1}(\\text{Proportion of } t^* &lt; t_0)\\), where \\(\\Phi^{-1}\\) is the inverse standard normal CDF). 2. Shifting the standard normal quantiles corresponding to the desired confidence level (\\(\\alpha/2\\) and \\(1-\\alpha/2\\)) by \\(z_0\\). 3. Finding the corresponding quantiles in the distribution of bootstrap replicates (:ts) based on these shifted probabilities.\nParameters:\n\nboot-data (map): A map containing bootstrap results, typically from bootstrap-stats. Requires keys:\n\n:t0 (double): The statistic calculated on the original data.\n:ts (sequence of numbers): The statistic calculated on each bootstrap sample.\n\nalpha (double, optional): The significance level for the interval. Defaults to 0.05 (for a 95% CI). The interval is based on quantiles of the :ts distribution, adjusted by the bias correction factor.\nestimation-strategy (keyword, optional): Specifies the quantile estimation strategy used to calculate the final interval bounds from :ts after applying corrections. Defaults to :legacy. See quantiles for available options (e.g., :r1 through :r9).\n\nReturns a vector [lower-bound, upper-bound, t0].\n\nlower-bound (double): The lower limit of the confidence interval.\nupper-bound (double): The upper limit of the confidence interval.\nt0 (double): The statistic calculated on the original data (from boot-data).\n\nSee also bootstrap-stats for input preparation and other confidence interval methods: ci-normal, ci-basic, ci-percentile, ci-bca, ci-studentized, ci-t, quantiles.\nsource\n\n\n\nci-bca\n\n(ci-bca boot-data)\n(ci-bca boot-data alpha)\n(ci-bca {:keys [t0 ts data statistic]} alpha estimation-strategy)\n\nCalculates the Bias-Corrected and Accelerated (BCa) bootstrap confidence interval.\nThe BCa interval is a sophisticated method that corrects for both bias and skewness in the distribution of the bootstrap statistic replicates. It is considered a more accurate interval, particularly when the bootstrap distribution is skewed.\nThe calculation requires two components: 1. A bias correction factor (\\(z_0\\)) based on the proportion of bootstrap replicates less than the original statistic (\\(t_0\\)). 2. An acceleration factor (\\(a\\)) which quantifies the rate of change of the standard error of the statistic with respect to the true parameter value.\nThe function uses one of two methods to calculate the acceleration factor:\n\nJackknife method: If the input boot-data map contains the original :data and the :statistic function used to compute :t0 and :ts, the acceleration factor is estimated using the jackknife method (by computing the statistic on leave-one-out jackknife samples).\nEmpirical method: If :data or :statistic are missing from boot-data, the acceleration factor is estimated empirically from the distribution of the bootstrap replicates (:ts) using its skewness.\n\nParameters:\n\nboot-data (map): A map containing bootstrap results, typically from bootstrap-stats. Requires keys:\n\n:t0 (double): The statistic calculated on the original data.\n:ts (sequence of numbers): The statistic calculated on each bootstrap sample. May optionally include:\n:data (sequence): The original dataset (required for jackknife acceleration).\n:statistic (function): The function used to calculate the statistic (required for jackknife acceleration).\n\nalpha (double, optional): The significance level for the interval. Defaults to 0.05 (for a 95% CI). The BCa method uses quantiles of the normal distribution and the bootstrap replicates, adjusted by the bias and acceleration factors.\nestimation-strategy (keyword, optional): Specifies the quantile estimation strategy used to calculate the quantiles of the bootstrap replicates (:ts) for the final interval bounds after applying corrections. Defaults to :legacy. See quantiles for available options (e.g., :r1 through :r9).\n\nReturns a vector [lower-bound, upper-bound, t0].\n\nlower-bound (double): The lower limit of the confidence interval.\nupper-bound (double): The upper limit of the confidence interval.\nt0 (double): The statistic calculated on the original data (from boot-data).\n\nSee also bootstrap-stats for input preparation and other confidence interval methods: ci-normal, ci-basic, ci-percentile, ci-bc, ci-studentized, ci-t, jackknife, quantiles.\nsource\n\n\n\nci-normal\n\n(ci-normal boot-data)\n(ci-normal {:keys [t0 ts stddev bias]} alpha)\n\nCalculates a Normal (Gaussian) approximation bias-corrected confidence interval.\nThis method assumes the distribution of the bootstrap replicates of the statistic (:ts) is approximately normal. It computes a confidence interval centered around the mean of the bootstrap statistics, adjusted by the estimated bias (mean(:ts) - :t0), and uses the standard error of the bootstrap statistics for scaling.\nParameters:\n\nboot-data (map): A map containing bootstrap results. Typically produced by bootstrap-stats. Requires keys:\n\n:t0 (double): The statistic calculated on the original data.\n:ts (sequence of numbers): The statistic calculated on each bootstrap sample. May optionally include pre-calculated :stddev (standard deviation of :ts) and :bias for efficiency.\n\nalpha (double, optional): The significance level for the interval. Defaults to 0.05 (for a 95% CI). The interval is based on the alpha/2 and 1 - alpha/2 quantiles of the standard normal distribution.\n\nReturns a vector [lower-bound, upper-bound, t0].\n\nlower-bound (double): The lower limit of the confidence interval.\nupper-bound (double): The upper limit of the confidence interval.\nt0 (double): The statistic calculated on the original data (from boot-data).\n\nSee also bootstrap-stats for input preparation and other confidence interval methods: ci-basic, ci-percentile, ci-bc, ci-bca, ci-studentized, ci-t.\nsource\n\n\n\nci-percentile\n\n(ci-percentile boot-data)\n(ci-percentile boot-data alpha)\n(ci-percentile {:keys [t0 ts]} alpha estimation-strategy)\n\nCalculates the Percentile bootstrap confidence interval.\nThis is the simplest bootstrap confidence interval method. It directly uses the quantiles of the bootstrap replicates of the statistic (:ts) as the confidence interval bounds.\nFor a confidence level of 1 - alpha, the interval is formed by taking the alpha/2 and 1 - alpha/2 quantiles of the distribution of bootstrap replicates (:ts).\nParameters:\n\nboot-data (map): A map containing bootstrap results, typically from bootstrap-stats. Requires keys:\n\n:t0 (double): The statistic calculated on the original data.\n:ts (sequence of numbers): The statistic calculated on each bootstrap sample.\n\nalpha (double, optional): The significance level for the interval. Defaults to 0.05 (for a 95% CI). The interval is based on the alpha/2 and 1 - alpha/2 quantiles of the :ts distribution.\nestimation-strategy (keyword, optional): Specifies the quantile estimation strategy used to calculate the quantiles of :ts. Defaults to :legacy. See quantiles for available options (e.g., :r1 through :r9).\n\nReturns a vector [lower-bound, upper-bound, t0].\n\nlower-bound (double): The alpha/2 quantile of :ts.\nupper-bound (double): The 1 - alpha/2 quantile of :ts.\nt0 (double): The statistic calculated on the original data (from boot-data).\n\nSee also bootstrap-stats for input preparation and other confidence interval methods: ci-normal, ci-basic, ci-bc, ci-bca, ci-studentized, ci-t, quantiles.\nsource\n\n\n\nci-studentized\n\n(ci-studentized boot-data)\n(ci-studentized boot-data alpha)\n(ci-studentized {:keys [t0 ts data samples]} alpha estimation-strategy)\n\nCalculates the Studentized (or Bootstrap-t) confidence interval.\nThis method is based on the distribution of the studentized pivotal quantity (statistic(sample) - statistic(data)) / standard_error(statistic(sample)). It estimates the quantiles of this distribution using bootstrap replicates and then uses them to construct a confidence interval around the statistic calculated on the original data (:t0), scaled by the standard error of the statistic calculated on the original data (stddev(:data)).\nParameters:\n\nboot-data (map): A map containing bootstrap results and necessary inputs. This map typically comes from bootstrap-stats and augmented with :data and :samples from the original bootstrap call if not already present. Requires the following keys:\n\n:t0 (double): The statistic calculated on the original data.\n:ts (sequence of numbers): The statistic calculated on each bootstrap sample.\n:data (sequence): The original dataset used for bootstrapping. Needed to estimate the standard error of the statistic for scaling the interval.\n:samples (collection of sequences): The collection of bootstrap samples. Needed to calculate the standard error of the statistic for each bootstrap sample.\n\nalpha (double, optional): The significance level for the interval. Defaults to 0.05 (for a 95% CI). The interval is based on the alpha/2 and 1 - alpha/2 quantiles of the studentized bootstrap replicates.\nestimation-strategy (keyword, optional): Specifies the quantile estimation strategy used to calculate the quantiles of the studentized replicates. Defaults to :legacy. See quantiles for available options (e.g., :r1 through :r9).\n\nReturns a vector [lower-bound, upper-bound, t0].\n\nlower-bound (double): The lower limit of the confidence interval.\nupper-bound (double): The upper limit of the confidence interval.\nt0 (double): The statistic calculated on the original data (from boot-data).\n\nSee also bootstrap-stats for input preparation and other confidence interval methods: ci-normal, ci-basic, ci-percentile, ci-bc, ci-bca, ci-t, stats/stddev, stats/quantiles.\nsource\n\n\n\nci-t\n\n(ci-t boot-data)\n(ci-t {:keys [t0 ts stddev]} alpha)\n\nCalculates a confidence interval based on Student’s t-distribution, centered at the original statistic value.\nThis method constructs a confidence interval centered at the statistic calculated on the original data (:t0). The width of the interval is determined by the standard deviation of the bootstrap replicates (:ts), scaled by a critical value from a Student’s t-distribution. The degrees of freedom for the t-distribution are based on the number of bootstrap replicates (count(:ts) - 1).\nThis interval does not explicitly use the Studentized bootstrap pivotal quantity. Instead, it applies a standard t-interval structure using components derived from the bootstrap results and the original data.\nParameters:\n\nboot-data (map): A map containing bootstrap results, typically from bootstrap-stats. Requires keys:\n\n:t0 (double): The statistic calculated on the original data.\n:ts (sequence of numbers): The statistic calculated on each bootstrap sample. May optionally include pre-calculated :stddev (standard deviation of :ts) for efficiency.\n\nalpha (double, optional): The significance level for the interval. Defaults to 0.05 (for a 95% CI). The interval is based on the alpha/2 and 1 - alpha/2 quantiles of the Student’s t-distribution with count(:ts) - 1 degrees of freedom.\n\nReturns a vector [lower-bound, upper-bound, t0].\n\nlower-bound (double): The lower limit of the confidence interval.\nupper-bound (double): The upper limit of the confidence interval.\nt0 (double): The statistic calculated on the original data (from boot-data).\n\nSee also bootstrap-stats for input preparation and other confidence interval methods: ci-normal, ci-basic, ci-percentile, ci-bc, ci-bca, ci-studentized.\nsource\n\n\n\njackknife\n\n(jackknife vs)\n\nGenerates a set of samples from a given sequence using the jackknife leave-one-out method.\nFor an input sequence vs of size n, this method creates n samples. Each sample is formed by removing a single observation from the original sequence.\nParameters:\n\nvs (sequence): The input data sequence.\n\nReturns a sequence of sequences. The i-th inner sequence is vs with the i-th element removed.\nThese samples are commonly used for estimating the bias and standard error of a statistic (e.g., via bootstrap-stats).\nsource\n\n\n\njackknife+\n\n(jackknife+ vs)\n\nGenerates a set of samples from a sequence using the ‘jackknife positive’ method.\nFor an input sequence vs of size n, this method creates n samples. Each sample is formed by duplicating a single observation from the original sequence and adding it back to the original sequence. Thus, each sample has size n+1.\nParameters:\n\nvs (sequence): The input data sequence.\n\nReturns a sequence of sequences. The i-th inner sequence is vs with an additional copy of the i-th element of vs.\nThis method is used in specific resampling techniques for estimating bias and variance of a statistic.\nsource\n\nsource: clay/stats.clj",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "polynomials.html",
    "href": "polynomials.html",
    "title": "Polynomials",
    "section": "",
    "text": "Reference",
    "crumbs": [
      "Polynomials"
    ]
  },
  {
    "objectID": "polynomials.html#reference",
    "href": "polynomials.html#reference",
    "title": "Polynomials",
    "section": "",
    "text": "fastmath.polynomials\n\n\n-&gt;Polynomial\n\n(-&gt;Polynomial cfs d)\n\nPositional factory function for class fastmath.polynomials.Polynomial.\nsource\n\n\n\n-&gt;PolynomialR\n\n(-&gt;PolynomialR cfs d)\n\nPositional factory function for class fastmath.polynomials.PolynomialR.\nsource\n\n\n\nadd\n\n(add poly)\n(add poly1 poly2)\n\nAdd two polynomials.\nsource\n\n\n\nbernstein\n\n(bernstein degree order)\n\nsource\n\n\n\nbessel-t\n\n(bessel-t degree)\n\nsource\n\n\n\nbessel-y\n\n(bessel-y degree)\n\nsource\n\n\n\nchebyshev-T\n\n(chebyshev-T degree)\n\nsource\n\n\n\nchebyshev-U\n\n(chebyshev-U degree)\n\nsource\n\n\n\nchebyshev-V\n\n(chebyshev-V degree)\n\nsource\n\n\n\nchebyshev-W\n\n(chebyshev-W degree)\n\nsource\n\n\n\ncoeffs\n\n(coeffs poly)\n\nCoefficients of polynomial\nsource\n\n\n\ncoeffs-&gt;polynomial\n\n(coeffs-&gt;polynomial & coeffs)\n\nCreate polynomial object for unrolled coefficients.\nsource\n\n\n\ncoeffs-&gt;ratio-polynomial\n\n(coeffs-&gt;ratio-polynomial & coeffs)\n\nCreate ratio based polynomial object for unrolled coefficients.\nsource\n\n\n\ndegree\n\n(degree poly)\n\nsource\n\n\n\nderivative\n\n(derivative poly)\n(derivative poly order)\n\nDerivative of the polynomial.\nsource\n\n\n\neval-bernstein\n\n(eval-bernstein degree order x)\n\nsource\n\n\n\neval-bessel-t\n\n(eval-bessel-t degree x)\n\nsource\n\n\n\neval-bessel-y\n\n(eval-bessel-y degree x)\n\nsource\n\n\n\neval-chebyshev-T\n\n(eval-chebyshev-T degree x)\n\nChebyshev polynomial of the first kind\nsource\n\n\n\neval-chebyshev-U\n\n(eval-chebyshev-U degree x)\n\nChebyshev polynomials of the second kind\nsource\n\n\n\neval-chebyshev-V\n\n(eval-chebyshev-V degree x)\n\nChebyshev polynomials of the third kind\nsource\n\n\n\neval-chebyshev-W\n\n(eval-chebyshev-W degree x)\n\nChebyshev polynomials of the fourth kind\nsource\n\n\n\neval-gegenbauer-C\n\n(eval-gegenbauer-C degree x)\n(eval-gegenbauer-C degree order x)\n\nGegenbauer (ultraspherical) polynomials\nsource\n\n\n\neval-hermite-H\n\n(eval-hermite-H degree x)\n\nHermite polynomials\nsource\n\n\n\neval-hermite-He\n\n(eval-hermite-He degree x)\n\nHermite polynomials\nsource\n\n\n\neval-jacobi-P\n\n(eval-jacobi-P degree alpha beta x)\n\nJacobi polynomials\nsource\n\n\n\neval-laguerre-L\n\n(eval-laguerre-L degree x)\n(eval-laguerre-L degree order x)\n\nEvaluate generalized Laguerre polynomial\nsource\n\n\n\neval-legendre-P\n\n(eval-legendre-P degree x)\n\nsource\n\n\n\neval-meixner-pollaczek-P\n\n(eval-meixner-pollaczek-P degree lambda phi x)\n\nsource\n\n\n\nevalpoly\n\n(evalpoly x & coeffs)\n\nEvaluate polynomial for given coefficients\nsource\n\n\n\nevalpoly-complex\n\n(evalpoly-complex x & coeffs)\n\nEvaluate complex polynomial\nsource\n\n\n\nevaluate\n\n(evaluate poly x)\n\nEvaluate polynomial\nsource\n\n\n\ngegenbauer-C\n\n(gegenbauer-C degree)\n(gegenbauer-C degree order)\n\nsource\n\n\n\nhermite-H\n\n(hermite-H degree)\n\nsource\n\n\n\nhermite-He\n\n(hermite-He degree)\n\nsource\n\n\n\nince-C\n\n(ince-C p m e)\n(ince-C p m e normalization)\n\nInce C polynomial of order p and degree m.\nnormalization parameter can be :none (default), :trigonometric or millers.\nsource\n\n\n\nince-C-coeffs\n\n(ince-C-coeffs p m e normalization)\n\nsource\n\n\n\nince-C-radial\n\n(ince-C-radial p m e)\n(ince-C-radial p m e normalization)\n\nInce C polynomial of order p and degree m.\nnormalization parameter can be :none (default), :trigonometric or millers.\nsource\n\n\n\nince-S\n\n(ince-S p m e)\n(ince-S p m e normalization)\n\nInce S polynomial of order p and degree m.\nnormalization parameter can be :none (default), :trigonometric or millers.\nsource\n\n\n\nince-S-coeffs\n\n(ince-S-coeffs p m e normalization)\n\nsource\n\n\n\nince-S-radial\n\n(ince-S-radial p m e)\n(ince-S-radial p m e normalization)\n\nInce S polynomial of order p and degree m.\nnormalization parameter can be :none (default), :trigonometric or millers.\nsource\n\n\n\njacobi-P\n\n(jacobi-P degree alpha beta)\n\nsource\n\n\n\nlaguerre-L\n\n(laguerre-L degree)\n(laguerre-L degree order)\n\nGeneralized Laguerre polynomials\nsource\n\n\n\nlegendre-P\n\n(legendre-P degree)\n\nsource\n\n\n\nmakepoly\n\n(makepoly coeffs)\n\nCreate polynomial function for given coefficients\nsource\n\n\n\nmakepoly-complex\n\n(makepoly-complex coeffs)\n\nCreate complex polynomial function for given coefficients\nsource\n\n\n\nmeixner-pollaczek-P\n\n(meixner-pollaczek-P degree lambda phi)\n\nsource\n\n\n\nmevalpoly MACRO\n\n(mevalpoly x & coeffs)\n\nEvaluate polynomial macro version in the form coeffs[0]+coeffs[1]x+coeffs[2]x^2+….\nsource\n\n\n\nmevalpoly-complex MACRO\n\n(mevalpoly-complex x & coeffs)\n\nEvaluate complex polynomial macro version in the form coeffs[0]+coeffs[1]x+coeffs[2]x^2+….\nsource\n\n\n\nmevalpoly-scalar-complex MACRO\n\n(mevalpoly-scalar-complex x & coeffs)\n\nEvaluate complex polynomial macro version in the form coeffs[0]+coeffs[1]x+coeffs[2]x^2+….\nCoefficients are scalars\nsource\n\n\n\nmult\n\n(mult poly)\n(mult poly1 poly2)\n\nMultiply two polynomials.\nsource\n\n\n\npolynomial\n\n(polynomial xs ys)\n(polynomial coeffs)\n\nCreate polynomial object from coeffs or fitted points.\nsource\n\n\n\nratio-polynomial\n\n(ratio-polynomial xs ys)\n(ratio-polynomial coeffs)\n\nCreate polynomial operating on ratios from coeffs or fitted points.\nsource\n\n\n\nscale\n\n(scale poly v)\n\nMultiply polynomial by scalar\nsource\n\n\n\nsub\n\n(sub poly)\n(sub poly1 poly2)\n\nSubtract two polynomials\nsource\n\nsource: clay/polynomials.clj",
    "crumbs": [
      "Polynomials"
    ]
  },
  {
    "objectID": "special.html",
    "href": "special.html",
    "title": "Special functions",
    "section": "",
    "text": "Gamma\nCollection of special functions for real arguments and real returned value. Most of the functions are implemented natively in Clojure, some are based on Apache Commons Math.\nNative implementation is based on Julia packages (SpecialFunctions.jl, Bessel, HypergeometricFunctions) or scientific papers and books (NIST, Meshfree Approximation Methods with Matlab by G. E. Fasshauer]).\nGamma and related functions",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#gamma",
    "href": "special.html#gamma",
    "title": "Special functions",
    "section": "",
    "text": "Defined functions\n\n\n\n\ngamma, log-gamma\ninv-gamma-1pm1, log-gamma-1p\nupper-incomplete-gamma, lower-incomplete-gamma\nregularized-gamma-p, regularized-gamma-q\ndigamma, trigamma, polygamma\ngamma-complex, log-gamma-complex\n\n\n\n\nGamma function\ngamma \\(\\Gamma(x)\\) function is an extension of the factorial.\n\\[\\Gamma(x) = \\int_0^\\infty t^{x-1}e^{-t}\\,dt\\]\nFor positive integer n\n\\[\\Gamma(n) = (n-1)!\\]\nGamma for negative integers is not defined.\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/gamma 1.5) ;; =&gt; 0.886226925452758\n(special/gamma -1.5) ;; =&gt; 2.3632718012073544\n(special/gamma -2.0) ;; =&gt; ##NaN\n(special/gamma 15) ;; =&gt; 8.71782912E10\n(m/factorial 14) ;; =&gt; 8.71782912E10\n\n\n\n\nAdditionally reciprocal gamma function inv-gamma-1pm1 is defined as:\n\\[\\frac{1}{\\Gamma(x+1)}-1\\text{ for } -0.5\\leq x\\leq 1.5\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/inv-gamma-1pm1 -0.5) ;; =&gt; -0.4358104164522437\n(special/inv-gamma-1pm1 0.5) ;; =&gt; 0.12837916709551256\n\n\n\n\nFor complex argument call gamma-complex.\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/gamma-complex (complex/complex 1.5 0.0)) ;; =&gt; #vec2 [0.8862269254527566, 0.0]\n(special/gamma-complex (complex/complex 0.0 1.0)) ;; =&gt; #vec2 [-0.1549498283018112, -0.49801566811835757]\n(special/gamma-complex (complex/complex -1.5 -1.0)) ;; =&gt; #vec2 [0.19071067188949734, -0.17418614261651502]\n\n\n\n\n\nLog gamma\nLogartihm of gamma log-gamma \\(\\log\\Gamma(x)\\) with derivatives: digamma \\(\\psi\\), trigamma \\(\\psi_1\\) and polygamma \\(\\psi^{(m)}\\).\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/log-gamma 1.01) ;; =&gt; -0.005690307946069651\n(special/log-gamma 0.5) ;; =&gt; 0.5723649429247001\n(m/exp (special/log-gamma 5)) ;; =&gt; 24.000000000000004\n\n\n\n\nFor complex argument call log-gamma-complex.\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/log-gamma-complex (complex/complex 1.01 0.0)) ;; =&gt; #vec2 [-0.005690307946069651, 0.0]\n(special/log-gamma-complex (complex/complex 0.0 1.0)) ;; =&gt; #vec2 [-0.6509231993018532, -1.8724366472624299]\n(special/log-gamma-complex (complex/complex -1.5 -1.0)) ;; =&gt; #vec2 [-1.3536899180323017, 5.543041710180497]\n\n\n\n\nlog-gamma-1p is more accurate function defined as \\(\\log\\Gamma(x+1)\\) for \\(-0.5\\leq x 1.5\\)\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/log-gamma-1p -0.1) ;; =&gt; 0.06637623973474298\n(special/log-gamma-1p 0.01) ;; =&gt; -0.005690307946069646\n(special/log-gamma-1p 1.01) ;; =&gt; 0.004260022907098441\n\n\n\n\nDerivatives of log-gamma function. First derivative digamma.\n\\[\\operatorname{digamma}(x)=\\psi(x)=\\psi^{(0)}(x)=\\frac{d}{dx}\\log\\Gamma(x)=\\frac{\\Gamma'(x)}{\\Gamma(x)}\\]\n\n\n\n\nSecond derivative trigamma\n\\[\\operatorname{trigamma}(x)=\\psi_1(x)=\\psi^{(1)}(x)=\\frac{d}{dx}\\psi(x)=\\frac{d^2}{dx^2}\\log\\Gamma(x)\\]\n\n\n\n\npolygamma as mth derivative of digamma\n\\[\\operatorname{polygamma}(m,x)=\\psi^{(m)}=\\frac{d^m}{dx^m}\\psi(x)=\\frac{d^{m+1}}{dx^{m+1}}\\log\\Gamma(x)\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/digamma 0.5) ;; =&gt; -1.9635100260214235\n(special/trigamma 0.5) ;; =&gt; 4.93480220054468\n(special/polygamma 0 0.5) ;; =&gt; -1.9635100260214235\n(special/polygamma 1 0.5) ;; =&gt; 4.93480220054468\n(special/polygamma 2 0.5) ;; =&gt; -16.828796644234316\n(special/polygamma 3 0.5) ;; =&gt; 97.40909103400247\n\n\n\n\n\nIncomplete and regularized\nupper-incomplete-gamma \\(\\Gamma(s,x)\\) and lower-incomplete-gamma \\(\\gamma(s,x)\\) are versions of gamma function with parametrized integral limits.\nUpper incomplete gamma is defined as:\n\\[\\Gamma(s,x) = \\int_x^\\infty t^{s-1}e^{-t}\\,dt\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/upper-incomplete-gamma 2 0.5) ;; =&gt; 0.9097959895689501\n(special/upper-incomplete-gamma -2 0.5) ;; =&gt; 0.886417457100714\n\n\n\n\nLower incomplete gamma is defined as:\n\\[\\gamma(s,x) = \\int_0^x t^{s-1}e^{-t}\\,dt\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/lower-incomplete-gamma 0.5 3) ;; =&gt; 1.7470973415820528\n(special/lower-incomplete-gamma -0.5 3) ;; =&gt; -3.5516838378128024\n\n\n\n\nregularized-gamma-p \\(P(s,x)\\) and regularized-gamma-q \\(Q(s,x)\\) are normalized incomplete gamma functions by gamma of s. s can be negative non-integer.\n\\[P(s,x)=\\frac{\\gamma(s,x)}{\\Gamma(x)}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/regularized-gamma-p 2.5 0.5) ;; =&gt; 0.03743422675270362\n(special/regularized-gamma-p -2.5 0.5) ;; =&gt; 2.134513839251947\n\n\n\n\n\\[Q(s,x)=\\frac{\\Gamma(s,x)}{\\Gamma(x)}=1-P(s,x)\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/regularized-gamma-q 2.5 0.5) ;; =&gt; 0.9625657732472964\n(special/regularized-gamma-q -2.5 0.5) ;; =&gt; -1.134513839251947",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#beta",
    "href": "special.html#beta",
    "title": "Special functions",
    "section": "Beta",
    "text": "Beta\nBeta and related functions\n\n\n\n\n\n\nDefined functions\n\n\n\n\nbeta, log-beta\nincomplete-beta, regularized-beta\n\n\n\n\nBeta function\nbeta \\(B(p,q)\\) function, defined also for negative non-integer p and q.\n\\[B(p,q) = \\int_0^1 t^{p-1}(1-t)^{q-1}\\,dt = \\frac{\\Gamma(p)\\Gamma(q)}{\\Gamma(p+q)}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/beta 2 3) ;; =&gt; 0.08333333333333334\n(special/beta -1.2 0.1) ;; =&gt; 4.750441365819471\n(special/beta -1.2 -0.1) ;; =&gt; -15.574914582341846\n\n\n\n\nlog-beta is log of \\(B(p,q)\\) for positive p and q\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/log-beta 2 3) ;; =&gt; -2.4849066497880004\n(special/log-beta 2 3) ;; =&gt; -2.4849066497880004\n(m/log (special/beta 2 3)) ;; =&gt; -2.4849066497880004\n\n\n\n\n\nIncomplete and regularized\nincomplete-beta \\(B(x,a,b)\\) and regularized-beta \\(I_x(a,b)\\). Both are defined also for negative non-integer a and b.\n\\[B(x,a,b)=\\int_0^x t^{a-1}(1-t)^{b-1}\\,dt\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/incomplete-beta 0.5 0.1 0.2) ;; =&gt; 9.789912164505285\n(special/incomplete-beta 0.5 -0.1 -0.2) ;; =&gt; -9.707992848052843\n\n\n\n\\[I_x(a,b)=\\frac{B(x,a,b)}{B(a,b)}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/regularized-beta 0.99 -0.5 -0.7) ;; =&gt; 12.086770426418143\n(special/regularized-beta 0.01 0.5 0.5) ;; =&gt; 0.06376856085851987",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#bessel",
    "href": "special.html#bessel",
    "title": "Special functions",
    "section": "Bessel",
    "text": "Bessel\n\nBessel functions of the first (\\(J_\\alpha\\)) and the second (\\(Y_\\alpha\\)) kind\nModified Bessel functions of the first (\\(I_\\alpha\\)) and the second (\\(K_\\alpha\\)) kind\nSpherical Bessel functions of the first (\\(j_\\alpha\\)) and the second (\\(y_\\alpha\\)) kind\nModified spherical Bessel functions of the first (\\(i_\\alpha^{(1)}\\), \\(i_\\alpha^{(2)}\\)) and the second (\\(k_\\alpha\\)) kind\nSombrero function jinc\n\n\n\n\n\n\n\nDefined functions\n\n\n\n\nbessel-J0, bessel-J1, bessel-J, jinc\nbessel-Y0, bessel-Y1, bessel-Y\nbessel-I0, bessel-I1, bessel-I\nbessel-K0, bessel-K1, bessel-K, bessel-K-half-odd\nspherical-bessel-j0, spherical-bessel-j1, spherical-bessel-j2,spherical-bessel-j\nspherical-bessel-y0, spherical-bessel-y1, spherical-bessel-y2,spherical-bessel-y\nspherical-bessel-1-i0, spherical-bessel-1-i1, spherical-bessel-1-i2,spherical-bessel-1-i\nspherical-bessel-2-i0, spherical-bessel-2-i1, spherical-bessel-2-i2,spherical-bessel-2-1\nspherical-bessel-k0, spherical-bessel-k1, spherical-bessel-k2,spherical-bessel-k\n\n\n\n\nBessel J, j\nBessel functions of the first kind, bessel-J \\(J_\\alpha(x)\\). bessel-J0 and bessel-J1 are functions of orders 0 and 1. An order should be integer for negative arguments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/bessel-J0 2.3) ;; =&gt; 0.055539784445602064\n(special/bessel-J1 2.3) ;; =&gt; 0.5398725326043137\n(special/bessel-J 2.1 3) ;; =&gt; 0.4761626361699597\n(special/bessel-J -3 -3.2) ;; =&gt; 0.3430663764006682\n(special/bessel-J 3 -3.2) ;; =&gt; -0.3430663764006682\n(special/bessel-J -3.1 -3.2) ;; =&gt; ##NaN\n(special/bessel-J 3.1 -3.2) ;; =&gt; ##NaN\n\n\n\n\nSpherical Bessel functions of the first kind spherical-bessel-j \\(j_\\alpha(x)\\), spherical-bessel-j0, spherical-bessel-j1 and spherical-bessel-j2 are functions of orders 0,1 and 2. Functions are defined for positive argument (only functions with orders 0, 1 and 2 accept non positive argument).\n\\[j_\\alpha(x)=\\sqrt{\\frac{\\pi}{2x}}J_{\\alpha+\\frac{1}{2}}(x)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/spherical-bessel-j0 2.3) ;; =&gt; 0.3242196574681393\n(special/spherical-bessel-j1 2.3) ;; =&gt; 0.43065029510781005\n(special/spherical-bessel-j2 2.3) ;; =&gt; 0.23749811875943916\n(special/spherical-bessel-j 3.1 3.2) ;; =&gt; 0.1579561007291703\n(special/spherical-bessel-j -3.1 3.2) ;; =&gt; 0.12796785869167607\n\n\n\n\nadditional jinc (sombrero) function is defined as:\n\\[\\operatorname{jinc}(x)=\\frac{2J_1(\\pi x)}{\\pi x}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/jinc 0.0) ;; =&gt; 1.0\n(special/jinc -2.3) ;; =&gt; 0.01707103495964295\n(special/jinc 2.3) ;; =&gt; 0.01707103495964295\n\n\n\n\n\nBessel Y, y\nBessel functions of the second kind, bessel-Y, \\(Y_\\alpha(x)\\). bessel-Y0 and bessel-Y1 are functions of orders 0 and 1. They are defined for positive argument only and any order.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/bessel-Y0 2.3) ;; =&gt; 0.5180753962076221\n(special/bessel-Y1 2.3) ;; =&gt; 0.05227731584422471\n(special/bessel-Y 2 2.3) ;; =&gt; -0.47261686069090497\n(special/bessel-Y -2.1 2.3) ;; =&gt; -0.36752629274516924\n(special/bessel-Y 3 -1) ;; =&gt; ##NaN\n\n\n\n\nSpherical Bessel functions of the second kind spherical-bessel-y \\(y_\\alpha(x)\\), spherical-bessel-y0, spherical-bessel-y1 and spherical-bessel-y2 are functions of orders 0,1 and 2. Functions are defined for positive argument.\n\\[y_\\alpha(x)=\\sqrt{\\frac{\\pi}{2x}}Y_{\\alpha+\\frac{1}{2}}(x)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/spherical-bessel-j0 2.3) ;; =&gt; 0.3242196574681393\n(special/spherical-bessel-j1 2.3) ;; =&gt; 0.43065029510781005\n(special/spherical-bessel-j2 2.3) ;; =&gt; 0.23749811875943916\n(special/spherical-bessel-j 3.1 3.2) ;; =&gt; 0.1579561007291703\n(special/spherical-bessel-j -3.1 3.2) ;; =&gt; 0.12796785869167607\n\n\n\n\n\nBessel I, i\nModified Bessel functions of the first kind, bessel-I, \\(I_\\alpha(x)\\), bessel-I0 and bessel-I1 are functions of orders 0 and 1. An order should be integer for negative arguments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/bessel-I0 2.3) ;; =&gt; 2.8296056006275854\n(special/bessel-I1 2.3) ;; =&gt; 2.097800027517421\n(special/bessel-I 2 2.3) ;; =&gt; 1.0054316636559146\n(special/bessel-I -2.1 2.3) ;; =&gt; 0.9505851207098388\n(special/bessel-I -3 -1) ;; =&gt; -0.0221684249243319\n(special/bessel-I -3.1 -1) ;; =&gt; ##NaN\n\n\n\n\nTwo modfified spherical Bessel functions of the first kind spherical-bessel-1-i \\(i_\\alpha^{(1)}(x)\\) and spherical-bessel-2-i \\(i_\\alpha^{(2)}(x)\\). spherical-bessel-1-i0, spherical-bessel-1-i1, spherical-bessel-1-i2, spherical-bessel-2-i0, spherical-bessel-2-i1 and spherical-bessel-2-i2 are functions of orders 0,1 and 2. Functions are defined for positive argument.\n\\[i_\\alpha^{(1)}(x)=\\sqrt{\\frac{\\pi}{2x}}I_{\\alpha+\\frac{1}{2}}(x)\\] \\[i_\\alpha^{(2)}(x)=\\sqrt{\\frac{\\pi}{2x}}I_{-\\alpha-\\frac{1}{2}}(x)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\\(i_\\alpha^{(1)}\\)\n\n(special/spherical-bessel-1-i0 2.3) ;; =&gt; 2.1465051328460687\n(special/spherical-bessel-1-i1 2.3) ;; =&gt; 1.256832833227258\n(special/spherical-bessel-1-i2 2.3) ;; =&gt; 0.5071579590713844\n(special/spherical-bessel-1-i 3.1 3.2) ;; =&gt; 0.48382455936204793\n(special/spherical-bessel-1-i -3.1 3.2) ;; =&gt; 1.271418391309527\n\n\\(i_\\alpha^{(2)}\\)\n\n(special/spherical-bessel-2-i0 2.3) ;; =&gt; 2.1900959344646793\n(special/spherical-bessel-2-i1 2.3) ;; =&gt; 1.1942895091657733\n(special/spherical-bessel-2-i2 2.3) ;; =&gt; 0.6323270094658442\n(special/spherical-bessel-2-i 3.1 3.2) ;; =&gt; 0.42032145323101916\n(special/spherical-bessel-2-i -3.1 3.2) ;; =&gt; 1.2425414548936577\n\n\n\n\n\nBessel K, k\nModified Bessel functions of the second kind, bessel-K, \\(K_\\alpha(x)\\), bessel-K0 and bessel-K1 are functions of orders 0 and 1. They are defined for positive argument only and any order.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/bessel-K0 2.3) ;; =&gt; 0.07913993300209364\n(special/bessel-K1 2.3) ;; =&gt; 0.09498244384536267\n(special/bessel-K 2 2.3) ;; =&gt; 0.1617333624328438\n(special/bessel-K -2.1 2.3) ;; =&gt; 0.17365527243516982\n(special/bessel-K 3 -1) ;; =&gt; ##NaN\n\n\n\n\nAdditionally bessel-K-half-odd function is optimized version for order of the half of odd integer, ie 1/2, 3/2, 5/2 and so on. First argument is an odd numerator.\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n[(special/bessel-K-half-odd 1 2.3) (special/bessel-K 0.5 2.3)] ;; =&gt; [0.0828549981836159 0.0828549981836159]\n[(special/bessel-K-half-odd 3 2.3) (special/bessel-K 1.5 2.3)] ;; =&gt; [0.11887891043736196 0.11887891043736196]\n[(special/bessel-K-half-odd 5 2.3) (special/bessel-K 2.5 2.3)] ;; =&gt; [0.23791444658017497 0.23791444658017497]\n\n\n\n\nModified spherical Bessel functions of the second kind spherical-bessel-k \\(k_\\alpha(x)\\), spherical-bessel-k0, spherical-bessel-k1 and spherical-bessel-k2 are functions of orders 0,1 and 2. Functions are defined for positive argument.\n\\[k_\\alpha(x)=\\sqrt{\\frac{\\pi}{2x}}K_{\\alpha+\\frac{1}{2}}(x)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/spherical-bessel-k0 2.3) ;; =&gt; 0.06847227106455815\n(special/spherical-bessel-k1 2.3) ;; =&gt; 0.09824282370132256\n(special/spherical-bessel-k2 2.3) ;; =&gt; 0.19661508458802238\n(special/spherical-bessel-k 3.1 3.2) ;; =&gt; 0.10488382566292166\n(special/spherical-bessel-k -3.1 3.2) ;; =&gt; 0.047694101111715716",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#hankel",
    "href": "special.html#hankel",
    "title": "Special functions",
    "section": "Hankel",
    "text": "Hankel\n\n\n\n\n\n\nDefined functions\n\n\n\n\nhankel-1, hankel-2\nspherical-hankel-1, spherical-hankel-2\n\n\n\nHankel functions of the first and second kind.\n\\[H^{(1)}_\\alpha(x)=J_\\alpha(x)+ i Y_\\alpha(x)\\] \\[H^{(2)}_\\alpha(x)=J_\\alpha(x)- i Y_\\alpha(x)\\]\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hankel-1 1 2.3) ;; =&gt; #vec2 [0.5398725326043137, 0.05227731584422471]\n(special/hankel-2 1 2.3) ;; =&gt; #vec2 [0.5398725326043137, -0.05227731584422471]\n\n\n\nSpherical functions of the first and second kind.\n\\[h^{(1)}_\\alpha(x)=j_\\alpha(x)+ i j_\\alpha(x)\\] \\[h^{(2)}_\\alpha(x)=j_\\alpha(x)- i j_\\alpha(x)\\]\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/spherical-hankel-1 1 2.3) ;; =&gt; #vec2 [0.43065029510781005, -0.19826955892752984]\n(special/spherical-hankel-2 1 2.3) ;; =&gt; #vec2 [0.43065029510781005, 0.19826955892752984]",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#erf",
    "href": "special.html#erf",
    "title": "Special functions",
    "section": "Erf",
    "text": "Erf\n\n\n\n\n\n\nDefined functions\n\n\n\n\nerf, erfc\ninv-erf, inv-erfc\n\n\n\nError functions\n\\[\\operatorname{erf}(x)=\\frac{2}{\\sqrt\\pi}\\int_0^x e^{-t^2}\\,dt\\] \\[\\operatorname{erfc}(x)=1-\\operatorname{erf}(x)\\]\n\n\n\n\n\n\n\n\n\n\n\n\nWhen two arguments are passed, difference between erf of two values is calculated \\(\\operatorname{erf}(x_2)-\\operatorname{erf}(x_1)\\)\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/erf 0.4) ;; =&gt; 0.4283923550466685\n(special/erfc 0.4) ;; =&gt; 0.5716076449533315\n[(special/erf 0.5 0.4) (- (special/erf 0.4) (special/erf 0.5))] ;; =&gt; [-0.09210752276637812 -0.09210752276637812]\n\n\n\n\nInverse of error functions.\n\ninv-erf - inverse of erf defined on \\((-1,1)\\)\ninv-erfc- inverse of erfc defined on \\((0,2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/inv-erf 0.42839235504666856) ;; =&gt; 0.4\n(special/inv-erfc (- 1 0.42839235504666856)) ;; =&gt; 0.4000000000000001",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#airy",
    "href": "special.html#airy",
    "title": "Special functions",
    "section": "Airy",
    "text": "Airy\nAiry functions and derivatives\n\n\n\n\n\n\nDefined functions\n\n\n\n\nairy-Ai, airy-Bi\nairy-Ai' airy-Bi'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/airy-Ai 2.3) ;; =&gt; 0.02183199318062265\n(special/airy-Bi 2.3) ;; =&gt; 4.885061581835644\n(special/airy-Ai' 2.3) ;; =&gt; -0.03517312272081809\n(special/airy-Bi' 2.3) ;; =&gt; 6.709740812723825",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#integrals",
    "href": "special.html#integrals",
    "title": "Special functions",
    "section": "Integrals",
    "text": "Integrals\nTrigonometric, exponential and logarithmic integrals\n\n\n\n\n\n\nDefined functions\n\n\n\n\nSi, si, Ci, Cin\nE0, E1, Ei, Ein, En\nli, Li (offset)\n\n\n\n\nTrigonometric\nSine and cosine integrals\n\\[\\operatorname{Si}(x)=\\int_0^x\\frac{\\sin t}{t}\\, dt\\] \\[\\operatorname{si}(x)=-\\int_x^\\infty\\frac{\\sin t}{t}\\, dt = \\operatorname{Si}(x)-\\frac{\\pi}{2}\\] \\[\\operatorname{Ci}(x)=-\\int_x^\\infty\\frac{\\cos t}{t}\\, dt\\] \\[\\operatorname{Cin}(x)=\\int_0^x\\frac{1-\\cos t}{t}\\, dt = \\gamma + \\ln x- \\operatorname{Ci}(x)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/Si 0.5) ;; =&gt; 0.49310741804306674\n(special/si 0.5) ;; =&gt; -1.0776889087518298\n(special/Ci 0.5) ;; =&gt; -0.17778407880661298\n(special/Cin 0.5) ;; =&gt; 0.06185256314820056\n\n\n\n\n\nExponential\nExponential integrals\n\\[E_0(x)=\\frac{e^{-x}}{x}\\] \\[E_1(x)=\\int_x^\\infty\\frac{e^{-t}}{t}\\,dt\\] \\[E_i(x)=-\\int_{-x}^\\infty\\frac{e^{-t}}{t}\\,dt\\] \\[E_{in}(x)=\\int_0^x\\frac{1-e^{-t}}{t}\\,dt\\] \\[E_n(x)=\\int_1^\\infty\\frac{e^{-xt}}{t^n}\\,dt\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/E0 0.6) ;; =&gt; 0.9146860601567108\n(special/E1 0.6) ;; =&gt; 0.4543795031894021\n(special/Ei 0.6) ;; =&gt; 0.7699269875786519\n(special/Ein 0.6) ;; =&gt; 0.5207695443249443\n(special/En 2 0.6) ;; =&gt; 0.2761839341803851\n(special/En -2 0.6) ;; =&gt; 9.04522881710525\n\n\n\n\n\nLogarithmic\nLogarithmic integrals\n\\[\\operatorname{li}(x)=\\int_0^x\\frac{dt}{\\ln t}\\] \\[\\operatorname{Li}(x)=\\int_2^x\\frac{dt}{\\ln t}=\\operatorname{li}(x)-\\operatorname{li}(2)\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/li 0.5) ;; =&gt; -0.378671043061088\n(special/Li 0.5) ;; =&gt; -1.423834823178581",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#zeta",
    "href": "special.html#zeta",
    "title": "Special functions",
    "section": "Zeta",
    "text": "Zeta\nZeta function and related\n\n\n\n\n\n\nDefined functions\n\n\n\n\nzeta - Riemann and Hurwitz zeta\nxi - Riemann (Landau) xi\neta - Dirichlet eta\ndirichlet-beta - Dirichlet beta\n\n\n\n\nRiemann zeta\n\\[\\zeta(s)=\\sum_{n=1}^\\infty\\frac{1}{n^s}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/zeta 0.0) ;; =&gt; -0.5\n(special/zeta 2.2) ;; =&gt; 1.4905432565068941\n(special/zeta -2.2) ;; =&gt; 0.0048792123593036025\n\n\n\n\n\nHurwitz zeta\n\\[\\zeta(s,z)=\\sum_{n=1}^\\infty\\frac{1}{(n+z)^s}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/zeta 0.0 3) ;; =&gt; -2.5\n(special/zeta 2.2 3) ;; =&gt; 0.27290561568286237\n(special/zeta -2.2 3) ;; =&gt; -5.589914207628901\n(special/zeta 2.2 -3) ;; =&gt; 2.7973744041931727\n(special/zeta -2.2 -3) ;; =&gt; 16.811251088887037\n(special/zeta 0.0 -3) ;; =&gt; 2.5\n\n\n\n\n\nxi\nLandau’s Xi function, symmetrical along \\(x=0.5\\)\n\\[\\xi(s)=\\frac{1}{2}s(s-1)\\pi^{-\\frac{s}{2}}\\Gamma\\left(\\frac{s}{2}\\right)\\zeta(s)\\] \\[\\xi(s)=\\xi(1-s)\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/xi 0.0) ;; =&gt; 0.5\n(special/xi 3.5) ;; =&gt; 0.6111280074951515\n(special/xi (- 1.0 3.5)) ;; =&gt; 0.6111280074951515\n\n\n\n\n\neta\nDirichlet eta function\n\\[\\eta(s)=\\sum_{n=1}^\\infty\\frac{(-1)^{n-1}}{n^s}=(1-2^{1-s})\\zeta(s)\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/eta 0.0) ;; =&gt; 0.5\n(special/eta 3.5) ;; =&gt; 0.927553577773949\n(special/eta -3.5) ;; =&gt; -0.09604760404512332\n\n\n\n\n\nbeta\nDirichlet (Catalan) beta function\n\\[\\beta(s)=\\sum_{n=0}^\\infty\\frac{(-1)^n}{(2n+1)^s}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/dirichlet-beta 0.0) ;; =&gt; 0.5\n(special/dirichlet-beta 3.5) ;; =&gt; 0.9814025112714404\n(special/dirichlet-beta -3.5) ;; =&gt; 1.0708860161983005",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#hypergeometric",
    "href": "special.html#hypergeometric",
    "title": "Special functions",
    "section": "Hypergeometric",
    "text": "Hypergeometric\nSelection of hypergeometric functions \\({}_pF_q\\)\n\\[{{}_{p}F_{q}}\\left({a_{1},\\dots,a_{p}\\atop b_{1},\\dots,b_{q}};x\\right)=\\sum_{k=0}^{\\infty}\\frac{{\\left(a_{1}\\right)_{k}}\\cdots{\\left(a_{p}\\right)_{k}}}{{\\left(b_{1}\\right)_{k}}\\cdots{\\left(b_{q}\\right)_{k}}}\\frac{x^{k}}{k!}.\\]\nwhere \\((a_p)_k\\) and \\((b_q)_k\\) are kth rising factorials\n\n\n\n\n\n\nDefined functions\n\n\n\n\nhypergeometric-pFq, hypergeometric-pFq-ratio, hypergeometric-pFq-complex\nhypergeometric-0F0, hypergeometric-0F1, hypergeometric-0F2\nhypergeometric-1F0, hypergeometric-1F1\nhypergeometric-2F0, hypergeometric-2F1\nkummers-M, tricomis-U, tricomis-U-complex\nwhittaker-M, whittaker-W\n\n\n\nFunctions are implemented using various recursive formulas, Maclaurin series and Weniger acceleration.\n\npFq, generalized\nTwo implementations of general \\({}_pF_q\\) hypergeometric functions using Maclaurin series. One implementation operates on doubles (hypergeometric-pFq), second on Clojure ratio type which is accurate but slow (hypergeometric-pFq-ratio), third on complex numbers..\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hypergeometric-2F0 0.1 0.1 0.01) ;; =&gt; 1.0001006141146784\n(special/hypergeometric-pFq [0.1 0.1] [] 0.01) ;; =&gt; 1.0001006141146787\n(special/hypergeometric-pFq-ratio [0.1 0.1] [] 0.01) ;; =&gt; 1.000100614114679\n(special/hypergeometric-pFq-complex [(complex/complex 0.2 0.2)] [1 (complex/complex -1 1)] 0.1) ;; =&gt; #vec2 [0.999390832208609, -0.020094590306120434]\n\n\n\nBoth functions accept optional max-iters parameter to control number of iterations.\nEvery implementation but ratio is unstable. Take a look at the example of \\({}_1F_1(-50;3;19.5)\\), only ratio version gives a valid result.\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hypergeometric-1F1 -50 3 19.5) ;; =&gt; 864.8061702264724\n(special/hypergeometric-pFq [-50] [3] 19.5) ;; =&gt; 1.0\n(special/hypergeometric-pFq-ratio [-50] [3] 19.5) ;; =&gt; -1.195066852171838\n\n\n\nFollowing plot shows stable (but slow) implementation hypergeometric-pFq-ratio vs unstable (but fast) hypergeometric-1F1 and Maclaurin seriers hypergeometric-pFq.\n\n\n\n\n\n0F0, exp\n\\({}_0F_0\\) is simply exponential function.\n\\[{}_0F_0(;;x)=\\sum_{k=0}^\\infty\\frac{x^k}{k!}=e^x\\]\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hypergeometric-0F0 2.3) ;; =&gt; 9.97418245481472\n(m/exp 2.3) ;; =&gt; 9.97418245481472\n\n\n\n\n\n0F1\n\\({}_0F_1\\) is called confluent hypergeometric limit function.\n\\[{}_0F_1(;a;x)=\\sum_{k=0}^\\infty\\frac{x^k}{(a)_k k!}=\n\\begin{cases}\n1.0 & x=0 \\\\\n\\frac{J_{a-1}\\left(2\\sqrt{|x|}\\right)\\Gamma(a)}{|x|^\\frac{a-1}{2}} & x&lt;0 \\\\\n\\frac{I_{a-1}\\left(2\\sqrt{|x|}\\right)\\Gamma(a)}{|x|^\\frac{a-1}{2}} & x&gt;0\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hypergeometric-pFq-ratio [] [-0.5] -2) ;; =&gt; -0.08000465565839093\n(special/hypergeometric-0F1 -0.5 -2) ;; =&gt; -0.08000465565839159\n\n\n\n\n\n0F2\n\\[{}_0F_2(;a,b;x)=\\sum_{k=0}^\\infty\\frac{x^k}{(a)_k(b)_k k!}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hypergeometric-pFq-ratio [] [-0.5 0.5] -2) ;; =&gt; 0.1239850666953995\n(special/hypergeometric-0F2 -0.5 0.5 -2) ;; =&gt; 0.12398506669539959\n\n\n\n\n\n1F0\n\\[{}_1F_0(a;;x)=\\sum_{k=0}^\\infty\\frac{(a)_k x^k}{k!}=(1-x)^{-a}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hypergeometric-pFq-ratio [0.5] [] -0.5) ;; =&gt; 0.8164965809277258\n(special/hypergeometric-1F0 0.5 -0.5) ;; =&gt; 0.816496580927726\n\n\n\n\n\n1F1, M\nConfluent hypergeometric function of the first kind, Kummer’s M.\n\\[{}_1F_1(a;b;x)=M(a,b,x)=\\sum_{k=0}^\\infty\\frac{(a)_k x^k}{(b)_k k!}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hypergeometric-pFq-ratio [0.5] [1] -0.5) ;; =&gt; 0.7910171621397194\n(special/hypergeometric-1F1 0.5 1 -0.5) ;; =&gt; 0.7910171621397188\n(special/kummers-M 0.5 1 -0.5) ;; =&gt; 0.7910171621397188\n\n\n\n\n\n2F0, U\n\\({}_2F_0\\) is related to the confluent hypergeometric function of the second kind, Tricomi’s U\n\\[{}_2F_0(a,b;;x)=\\sum_{k=0}^\\infty\\frac{(a)_k (b)_k x^k}{k!}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hypergeometric-pFq-ratio [0.1 -0.1] [] 0.01) ;; =&gt; 0.9998994982635961\n(special/hypergeometric-2F0 0.1 -0.1 0.01) ;; =&gt; 0.9998994982635958\n(special/hypergeometric-2F0 0.1 -0.1 1.2) ;; =&gt; 0.987133788261332\n\n\n\n\n\\[U(a,b,x) \\sim x^{-a}{}_2F_0(a,b;;-\\frac{1}{x})\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/tricomis-U 0.5 0.2 0.1) ;; =&gt; 1.090844545544952\n(special/tricomis-U -0.5 0.2 0.1) ;; =&gt; 0.5507462877526579\n\n\n\nComplex variant accepts both complex numbers and real numbers as arguments.\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/tricomis-U-complex 0.5 0.2 0.1) ;; =&gt; #vec2 [1.0908445455449323, 0.0]\n(special/tricomis-U-complex (complex/complex -1 1) (complex/complex 1 -1) 1) ;; =&gt; #vec2 [4.5512079583556, 4.7303232509692466]\n\n\n\n\n\n2F1, Gauss\nGauss’ hypergeometric function \\({}_2F_1\\).\n\\[{}_2F_1(a,b;c;x)=\\sum_{k=0}^\\infty\\frac{(a)_k (b)_k x^k}{(c)_k k!}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/hypergeometric-pFq-ratio [1.1 -0.1] [0.2] 0.5) ;; =&gt; 0.5254580717634016\n(special/hypergeometric-2F1 1.1 -0.1 0.2 0.5) ;; =&gt; 0.5254580717633999\n\n\n\n\n\nWhittaker M and W\nModified hypergeometric functions by Whittaker\n\\[M_{\\kappa,\\mu}\\left(x\\right)=e^{-\\frac{1}{2}x}x^{\\frac{1}{2}+\\mu}M\\left(\\tfrac{1}{2}+\\mu-\\kappa,1+2\\mu,x\\right)\\] \\[W_{\\kappa,\\mu}\\left(x\\right)=e^{-\\frac{1}{2}x}x^{\\frac{1}{2}+\\mu}U\\left(\\tfrac{1}{2}+\\mu-\\kappa,1+2\\mu,x\\right)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/whittaker-M 0.3 0.4 1.2) ;; =&gt; 1.0250053521045672\n(special/whittaker-W 0.3 0.4 1.2) ;; =&gt; 0.6219095691834272",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#other",
    "href": "special.html#other",
    "title": "Special functions",
    "section": "Other",
    "text": "Other\n\n\n\n\n\n\nDefined functions\n\n\n\n\nlambert-W (\\(W_0\\)), lambert-W-1 (\\(W_{-1}\\))\nharmonic-number\nminkowski - \\(?(x)\\)\nowens-t\n\n\n\n\nLambert W\nLambert W is a function for which \\(W(xe^x)=x\\). There are two branches \\(W_0\\) (lambert-W) and \\(W_{-1}\\) (lambert-W-1).\n\\[\n\\begin{align}\nW_0(xe^x)=x & \\text{ for } x\\ge -1 \\\\\nW_{-1}(xe^x)=x & \\text{ for } x\\le -1\n\\end{align}\n\\]\ndomain of functions\n\\[\n\\begin{align}\nW_0(t) & \\text{ for } t\\in(-1/e,\\infty) \\\\\nW_{-1}(t) & \\text{ for } t\\in(-1/e,0)\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/lambert-W m/E) ;; =&gt; 1.0\n(special/lambert-W (* 2.3 (m/exp 2.3))) ;; =&gt; 2.3\n(special/lambert-W-1 (* -2 (m/exp -2))) ;; =&gt; -2.0\n\n\n\n\n\nHarmonic H\nHarmonic numbers\n\\[H_n=\\int_0^1\\frac{1-x^n}{1-x}\\,dx=\\operatorname{digamma}(x+1)+\\gamma\\]\nFor non-negative integers\n\\[H_n=\\sum_{k=1}^n\\frac{1}{k}\\]\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/harmonic-number 2) ;; =&gt; 1.5\n(special/harmonic-number 2.5) ;; =&gt; 1.680372305546776\n(special/harmonic-number 3) ;; =&gt; 1.8333333333333335\n(special/harmonic-number -0.5) ;; =&gt; -1.3862943611198908\n\n\n\n\nGeneralized harmonic numbers for \\(m\\neq0\\) or \\(m\\neq1\\)\n\\[H_{n,m}=\\zeta(m)-\\zeta(m,n+1)\\] \\[H_{n,0}=n\\text{, }H_{n,1}=H_n\\]\nFor non-negative integer n\n\\[H_{n,m}=\\sum_{k=1}^n\\frac{1}{k^m}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/harmonic-number 2.2 -0.5) ;; =&gt; 2.737173754376224\n(special/harmonic-number 2.2 0.5) ;; =&gt; 1.8306098144389147\n\n\n\n\n\nMinowski\nMinkowski’s question mark \\(?(x)\\) function.\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/minkowski 0.5) ;; =&gt; 0.5\n[(special/minkowski 0.2) (- 1.0 (special/minkowski 0.8))] ;; =&gt; [0.0625 0.0625]\n[(special/minkowski (/ 0.5 1.5)) (/ (special/minkowski 0.5) 2)] ;; =&gt; [0.25 0.25]\n\n\n\n\n\nOwen’s T\nOwen’s T function\n\\[T(h,a) = \\frac{1}{2\\pi } \\int_{0}^{a} \\frac{e^{-\\frac{1}{2}h^2(1+x^2)}}{1+x^2}dx\\quad(-\\infty &lt; h,a &lt; +\\infty)\\]\n\n\n\n\n\n\nExamples\n\n\n\n\n(special/owens-t 1 0) ;; =&gt; 0.0\n(special/owens-t 0 1) ;; =&gt; 0.125\n(special/owens-t 1 1) ;; =&gt; 0.06674188216570096\n(special/owens-t -1 1) ;; =&gt; 0.06674188216570096\n(special/owens-t 1 -1) ;; =&gt; -0.06674188216570096\n(special/owens-t -1 -1) ;; =&gt; -0.06674188216570096",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "special.html#reference",
    "href": "special.html#reference",
    "title": "Special functions",
    "section": "Reference",
    "text": "Reference\n\nfastmath.special\nSpecial functions for real arguments and value.\n\nBessel J, Y, jinc\nModified Bessel I, K\nSpherical Bessel j, y\nModified spherical Bessel i1, i2, k\nGamma, log, digamma, trigamma, polygamma, regularized, lower/upper incomplete\nBeta, log, regularized, incomplete\nErf, inverse\nAiry A, B with derivatives\nZeta (Riemann, Hurwitz), Eta (Dirichlet), Xi (Landau), Beta (Dirichlet)\nIntegrals: Si, Ci, li/Li, Ei, En, Ein\nHypergeometric 0F0, 0F1, 1F0, 1F1, 2F1, 2F0, 0F2, pFq, Kummers M, Tricomis U, Whittaker M and W\nLambert W (0 and -1)\nMinkowski\nHarmonic H\nOwen’s T\nComplex: (log)Gamma, Hypergeometric pFq, Tricomis U\n\n\n\nCi\n\n(Ci x)\n\nCosine integral\nsource\n\n\n\nCin\n\n(Cin x)\n\nCosine integral, alternative definition\nsource\n\n\n\nE0\n\n(E0 x)\n\nExponential integral E0\nsource\n\n\n\nE1\n\n(E1 x)\n\nExponential integral E1 for positive real numbers\nsource\n\n\n\nEi\n\n(Ei x)\n\nExponential integral\nsource\n\n\n\nEin\n\n(Ein x)\n\nExponential integral, alternative definition\nsource\n\n\n\nEn\n\n(En n x)\n\nGeneralized exponential integral En\nsource\n\n\n\nLi\n\n(Li x)\n\nOffset logarythmic integral\nsource\n\n\n\nSi\n\n(Si x)\n\nSine integral\nsource\n\n\n\nairy-Ai\n\n(airy-Ai x)\n\nAiry Ai function\nsource\n\n\n\nairy-Ai’\n\n(airy-Ai' x)\n\nFirst derivative of the Airy Ai function\nsource\n\n\n\nairy-Bi\n\n(airy-Bi x)\n\nAiry Bi function\nsource\n\n\n\nairy-Bi’\n\n(airy-Bi' x)\n\nFirst derivative of the Airy Bi function\nsource\n\n\n\nbessel-I\n\n(bessel-I order x)\n\nModified Bessel function of the first kind of order v, I_v(x)\nsource\n\n\n\nbessel-I0\n\n(bessel-I0 x)\n\nModified Bessel function of the first kind of order 0, I_0(x)\nsource\n\n\n\nbessel-I1\n\n(bessel-I1 x)\n\nModified Bessel function of the first kind of order 1, I_0(x)\nsource\n\n\n\nbessel-J\n\n(bessel-J order x)\n\nBessel function of the first kind of order v, J_v(x)\nsource\n\n\n\nbessel-J0\n\n(bessel-J0 x)\n\nBessel function of the first kind of order 0, J_0(x)\nsource\n\n\n\nbessel-J1\n\n(bessel-J1 x)\n\nBessel function of the first kind of order 1, J_1(x)\nsource\n\n\n\nbessel-K\n\n(bessel-K order x)\n\nModified Bessel function of the second kind and real order v, K_v(x)\nsource\n\n\n\nbessel-K-half-odd\n\n(bessel-K-half-odd odd-numerator x)\n\nBessel K_a function for a = order/2\nFunction accepts only odd integers for order\nsource\n\n\n\nbessel-K0\n\n(bessel-K0 x)\n\nModified Bessel function of the second kind of order 0, K_0(x)\nsource\n\n\n\nbessel-K1\n\n(bessel-K1 x)\n\nModified Bessel function of the second kind of order 1, K_1(x)\nsource\n\n\n\nbessel-Y\n\n(bessel-Y order x)\n\nBessel function of the second kind of order v, Y_v(x)\nsource\n\n\n\nbessel-Y0\n\n(bessel-Y0 x)\n\nBessel function of the second kind of order 0, Y_0(x)\nsource\n\n\n\nbessel-Y1\n\n(bessel-Y1 x)\n\nBessel function of the second kind of order 1, Y_1(x)\nsource\n\n\n\nbeta\n\n(beta p q)\n\nBeta function\nsource\n\n\n\ndigamma\n\n(digamma x)\n\nFirst derivative of log of Gamma function.\nsource\n\n\n\ndirichlet-beta\n\n(dirichlet-beta x)\n\nDirichlet Beta function\nsource\n\n\n\nerf\n\n(erf x)\n(erf x1 x2)\n\nError function.\nFor two arguments returns a difference between (erf x2) and (erf x1).\nsource\n\n\n\nerfc\n\n(erfc x)\n\nComplementary error function.\nsource\n\n\n\neta\n\n(eta x)\n\nDirichlet Eta function\nsource\n\n\n\ngamma\n\n(gamma x)\n\nGamma function \\(\\Gamma(x)\\). Extension of the factorial.\nsource\n\n\n\ngamma-complex\n\n(gamma-complex z)\n\nComplex version of gamma function.\nsource\n\n\n\nhankel-1\n\n(hankel-1 order x)\n\nHankel function of the first kind, returns complex number.\nsource\n\n\n\nhankel-2\n\n(hankel-2 order x)\n\nHankel function of the second kind, returns complex number.\nsource\n\n\n\nharmonic-number\n\n(harmonic-number n)\n(harmonic-number n m)\n\nHarmonic number H_n or generalized harmonic number H_n,m\nsource\n\n\n\nhypergeometric-0F0\n\n(hypergeometric-0F0 x)\n\nHypergeometric ₀F₀ function, exp(x)\nsource\n\n\n\nhypergeometric-0F1\n\n(hypergeometric-0F1 a x)\n\nConfluent hypergeometric ₀F₁ limit function.\nsource\n\n\n\nhypergeometric-0F2\n\n(hypergeometric-0F2 a b x)\n\nGeneralized hypergeometric ₀F₂ function.\nsource\n\n\n\nhypergeometric-1F0\n\n(hypergeometric-1F0 a x)\n\nHypergeometric ₁F₀ function.\nsource\n\n\n\nhypergeometric-1F1\n\n(hypergeometric-1F1 a b x)\n\nConfluent hypergeometric ₁F₁ function of the first kind, Kummer’s M.\nsource\n\n\n\nhypergeometric-2F0\n\n(hypergeometric-2F0 a b x)\n\nGeneralized hypergeometric ₂F₀ function.\nsource\n\n\n\nhypergeometric-2F1\n\n(hypergeometric-2F1 a b c x)\n\nGauss’s hypergeometric ₂F₁ function.\nsource\n\n\n\nhypergeometric-pFq\n\n(hypergeometric-pFq ps qs x)\n(hypergeometric-pFq ps qs x max-iters)\n\nHypergeometric-pFq using MacLaurin series or Weniger acceleration.\nmax-iters is set to 10000 by default.\nsource\n\n\n\nhypergeometric-pFq-complex\n\n(hypergeometric-pFq-complex ps qs z)\n(hypergeometric-pFq-complex ps qs z max-iters)\n\nHypergeometric-pFq using MacLaurin series or Weniger acceleration on complex numbers.\nmax-iters is set to 10000 by default.\nsource\n\n\n\nhypergeometric-pFq-ratio\n\n(hypergeometric-pFq-ratio ps qs z)\n(hypergeometric-pFq-ratio ps qs z max-iters)\n\nHypergeometric-pFq using MacLaurin series on ratios. Can be very slow.\nmax-iters is set to 10000 by default.\nsource\n\n\n\nincomplete-beta\n\n(incomplete-beta x a b)\n\nIncomplete Beta B(x,a,b)\nsource\n\n\n\ninv-erf\n\n(inv-erf x)\n\nInverse of erf function.\nsource\n\n\n\ninv-erfc\n\n(inv-erfc x)\n\nInverse of erfc function.\nsource\n\n\n\ninv-gamma-1pm1\n\n(inv-gamma-1pm1 x)\n\n\\(\\frac{1}{\\Gamma(1+x)}-1\\) for \\(-0.5≤x≤1.5\\).\nsource\n\n\n\njinc\n\n(jinc x)\n\nBesselj1 devided by x\nsource\n\n\n\nkummers-M\n\n(kummers-M a b x)\n\nKummer’s (confluent hypergeometric, 1F1) function for real arguments.\nsource\n\n\n\nlambert-W\n\n(lambert-W x)\n\nLambert W_0 function. W(xe^x)=x for x&gt;=-1.0.\nsource\n\n\n\nlambert-W-1\n\n(lambert-W-1 x)\n\nLambert W_1 function. W_1(xe^x)=x for x&lt;=-1.0.\nsource\n\n\n\nli\n\n(li x)\n\nLogarythmic integral\nsource\n\n\n\nlog-beta\n\n(log-beta p q)\n\nLogarithm of Beta function.\nsource\n\n\n\nlog-gamma\n\n(log-gamma x)\n\nLog of Gamma function \\(\\log\\Gamma(x)\\).\nsource\n\n\n\nlog-gamma-1p\n\n(log-gamma-1p x)\n\n\\(\\ln\\Gamma(1+x)\\) for \\(-0.5≤x≤1.5\\).\nsource\n\n\n\nlog-gamma-complex\n\n(log-gamma-complex z)\n\nLogarithm of complex gamma function.\nsource\n\n\n\nlower-incomplete-gamma\n\n(lower-incomplete-gamma s x)\n\nLower incomplete gamma function\nsource\n\n\n\nminkowski\n\n(minkowski x)\n\nMinkowski’s question mark function ?(x)\nsource\n\n\n\nowens-t\n\n(owens-t h a)\n\nOwens’ T function\nsource\n\n\n\npolygamma\n\n(polygamma m x)\n\nPolygamma function of order m and real argument.\nsource\n\n\n\nregularized-beta\n\n(regularized-beta x a b)\n\nRegularized Beta I_x(a,b)\nsource\n\n\n\nregularized-gamma-p\n\n(regularized-gamma-p a x)\n\nRegularized gamma P(a,x)\nsource\n\n\n\nregularized-gamma-q\n\n(regularized-gamma-q a x)\n\nRegularized gamma Q(a,x)\nsource\n\n\n\nsi\n\n(si x)\n\nSine integral, Si shifted by -pi/2\nsource\n\n\n\nspherical-bessel-1-i\n\n(spherical-bessel-1-i order x)\n\nFirst modified spherical Bessel function of the first kind.\nsource\n\n\n\nspherical-bessel-1-i0\n\n(spherical-bessel-1-i0 x)\n\nFirst modified spherical Bessel function of the first kind and order 0.\nsource\n\n\n\nspherical-bessel-1-i1\n\n(spherical-bessel-1-i1 x)\n\nFirst modified spherical Bessel function of the first kind and order 1.\nsource\n\n\n\nspherical-bessel-1-i2\n\n(spherical-bessel-1-i2 x)\n\nFirst modified spherical Bessel function of the first kind and order 2.\nsource\n\n\n\nspherical-bessel-2-i\n\n(spherical-bessel-2-i order x)\n\nSecond modified spherical Bessel function of the first kind.\nsource\n\n\n\nspherical-bessel-2-i0\n\n(spherical-bessel-2-i0 x)\n\nSecond modified spherical Bessel function of the first kind and order 0.\nsource\n\n\n\nspherical-bessel-2-i1\n\n(spherical-bessel-2-i1 x)\n\nSecond modified spherical Bessel function of the first kind and order 1.\nsource\n\n\n\nspherical-bessel-2-i2\n\n(spherical-bessel-2-i2 x)\n\nSecond modified spherical Bessel function of the first kind and order 2.\nsource\n\n\n\nspherical-bessel-j\n\n(spherical-bessel-j order x)\n\nSpherical Bessel function of the first kind.\nsource\n\n\n\nspherical-bessel-j0\n\n(spherical-bessel-j0 x)\n\nSpherical Bessel function of the first kind and order 0.\nsource\n\n\n\nspherical-bessel-j1\n\n(spherical-bessel-j1 x)\n\nSpherical Bessel function of the first kind and order 1.\nsource\n\n\n\nspherical-bessel-j2\n\n(spherical-bessel-j2 x)\n\nSpherical Bessel function of the first kind and order 2.\nsource\n\n\n\nspherical-bessel-k\n\n(spherical-bessel-k order x)\n\nModified spherical Bessel function of the second kind.\nsource\n\n\n\nspherical-bessel-k0\n\n(spherical-bessel-k0 x)\n\nModified spherical Bessel function of the second kind and order 0.\nsource\n\n\n\nspherical-bessel-k1\n\n(spherical-bessel-k1 x)\n\nModified spherical Bessel function of the second kind and order 1.\nsource\n\n\n\nspherical-bessel-k2\n\n(spherical-bessel-k2 x)\n\nModified spherical Bessel function of the second kind and order 2.\nsource\n\n\n\nspherical-bessel-y\n\n(spherical-bessel-y order x)\n\nSpherical Bessel function of the second kind.\nsource\n\n\n\nspherical-bessel-y0\n\n(spherical-bessel-y0 x)\n\nSpherical Bessel function of the second kind and order 0.\nsource\n\n\n\nspherical-bessel-y1\n\n(spherical-bessel-y1 x)\n\nSpherical Bessel function of the second kind and order 1.\nsource\n\n\n\nspherical-bessel-y2\n\n(spherical-bessel-y2 x)\n\nSpherical Bessel function of the second kind and order 2.\nsource\n\n\n\nspherical-hankel-1\n\n(spherical-hankel-1 order x)\n\nSpherical Hankel function of the first kind, returns complex number.\nsource\n\n\n\nspherical-hankel-2\n\n(spherical-hankel-2 order x)\n\nSpherical Hankel function of the second kind, returns complex number.\nsource\n\n\n\ntricomis-U\n\n(tricomis-U a b x)\n\nConfluent hypergeometric function U of the second kind.\nsource\n\n\n\ntricomis-U-complex\n\n(tricomis-U-complex a b z)\n\nsource\n\n\n\ntrigamma\n\n(trigamma x)\n\nSecond derivative of log of Gamma function.\nsource\n\n\n\nupper-incomplete-gamma\n\n(upper-incomplete-gamma s x)\n\nUpper incomplete gamma function\nsource\n\n\n\nwhittaker-M\n\n(whittaker-M kappa mu x)\n\nWhittaker’s M\nsource\n\n\n\nwhittaker-W\n\n(whittaker-W kappa mu x)\n\nWhittaker’s W\nsource\n\n\n\nxi\n\n(xi s)\n\nRiemann (Landau’s) Xi function\nsource\n\n\n\nzeta\n\n(zeta s)\n(zeta s z)\n\nRiemann and Hurwitz zeta functions for real arguments\nsource\n\nsource: clay/special.clj",
    "crumbs": [
      "Special functions"
    ]
  },
  {
    "objectID": "calculus.html",
    "href": "calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "Integration",
    "crumbs": [
      "Calculus"
    ]
  },
  {
    "objectID": "calculus.html#differentiation",
    "href": "calculus.html#differentiation",
    "title": "Calculus",
    "section": "Differentiation",
    "text": "Differentiation",
    "crumbs": [
      "Calculus"
    ]
  },
  {
    "objectID": "calculus.html#solvers",
    "href": "calculus.html#solvers",
    "title": "Calculus",
    "section": "Solvers",
    "text": "Solvers",
    "crumbs": [
      "Calculus"
    ]
  },
  {
    "objectID": "calculus.html#reference",
    "href": "calculus.html#reference",
    "title": "Calculus",
    "section": "Reference",
    "text": "Reference\n\nfastmath.calculus\nIntegration and derivatives\nIntegrate univariate and multivariate functions.\n\nVEGAS / VEGAS+ - Monte Carlo integration of multivariate function\nh-Cubature - h-adaptive integration of multivariate function\nGuass-Kronrod and Gauss-Legendre - quadrature integration of univariate functions\nRomberg, Simpson, MidPoint and Trapezoid\n\nIntegrant is substituted in case of improper integration bounds.\nDerivatives (finite differences method):\n\nderivatives of any degree and any order of accuracy\ngradient and hessian for multivariate functions\n\n\n\ncubature\n\n(cubature f lower upper)\n(cubature f lower upper options)\n\nCubature - h-adaptive integration of multivariate function, n&gt;1 dimensions.\nAlgorithm uses Genz Malik method.\nIn each iteration a box with biggest error is subdivided and reevaluated.\nImproper integrals with infinite bounds are handled by a substitution.\nArguments:\n\nf - integrant\nlower - seq of lower bounds\nupper - seq of upper bounds\n\nOptions:\n\n:initvid - initial subdivision per dimension, default: 2.\n:max-evals - maximum number of evaluations, default: max integer value.\n:max-iters - maximum number of iterations, default: 64.\n:rel - relative error, 1.0e-7\n:abs - absolute error, 1.0e-7\n:info? - return full information about integration, default: false\n\nFunction returns a map containing (if info? is true, returns result otherwise):\n\n:result - integration value\n:error - integration error\n:iterations - number of iterations\n:evaluations - number of evaluations\n:subdivisions - final number of boxes\n:fail? - set to :max-evals or :max-iters when one of the limits has been reached without the convergence.\n\nsource\n\n\n\nderivative\n\n(derivative f)\n(derivative f n)\n(derivative f n options)\n\nCreate nth derivative of f using finite difference method for given accuracy :acc and step :h.\nReturns function.\nArguments:\n\nn - derivative\n:acc - order of accuracy (default: 2)\n:h - step, (default: 0.0, automatic)\n:method - :central (default), :forward or :backward\n:extrapolate? - creates extrapolated derivative if set to true or a map with extrapolate function options\n\nsource\n\n\n\nextrapolate\n\n(extrapolate g)\n(extrapolate g options)\n\nRichardson extrapolation for given function g=g(x,h). Returns extrapolated function f(x).\nOptions:\n\n:contract - shrinkage factor, default=1/2\n:power - set to 2.0 for even functions around x0, default 1.0\n:init-h - initial step h, default=1/2\n:abs - absolute error, default: machine epsilon\n:rel - relative error, default: ulp for init-h\n:tol - tolerance for error, default: 2.0\n:max-evals - maximum evaluations, default: maximum integer\n\nsource\n\n\n\nf’\n\n(f' f)\n\nFirst central derivative with order of accuracy 2.\nsource\n\n\n\nf’’\n\n(f'' f)\n\nSecond central derivative with order of accuracy 2.\nsource\n\n\n\nf’’’\n\n(f''' f)\n\nThird central derivative with order of accuracy 2.\nsource\n\n\n\nfx-&gt;gx+h\n\n(fx-&gt;gx+h f)\n\nConvert f(x) to g(x,h)=f(x+h)\nsource\n\n\n\nfx-&gt;gx-h\n\n(fx-&gt;gx-h f)\n\nConvert f(x) to g(x,h)=f(x-h)\nsource\n\n\n\ngradient\n\n(gradient f)\n(gradient f options)\n\nCreate first partial derivatives of multivariate function for given accuracy :acc and step :h.\nReturns function.\nOptions:\n\n:acc - order of accuracy, 2 (default) or 4.\n:h - step, default 1.0e-6\n\nsource\n\n\n\nhessian\n\n(hessian f)\n(hessian f options)\n\nCreates function returning Hessian matrix for mulitvariate function f and given :h step (default: 5.0e-3).\nsource\n\n\n\nintegrate\n\n(integrate f)\n(integrate f lower upper)\n(integrate f lower upper {:keys [rel abs max-iters min-iters max-evals info? integrator integration-points], :or {rel BaseAbstractUnivariateIntegrator/DEFAULT_RELATIVE_ACCURACY, abs BaseAbstractUnivariateIntegrator/DEFAULT_ABSOLUTE_ACCURACY, min-iters BaseAbstractUnivariateIntegrator/DEFAULT_MIN_ITERATIONS_COUNT, max-evals Integer/MAX_VALUE, integration-points 7, integrator :gauss-kronrod, info? false}, :as options})\n\nUnivariate integration.\nImproper integrals with infinite bounds are handled by a substitution.\nArguments:\n\nf - integrant\nlower - lower bound\nupper - upper bound\n\nOptions:\n\n:integrator - integration algorithm, one of: :romberg, :trapezoid, :midpoint, :simpson, :gauss-legendre and :gauss-kronrod (default).\n:min-iters - minimum number of iterations (default: 3), not used in :gauss-kronrod\n:max-iters - maximum number of iterations (default: 32 or 64)\n:max-evals - maximum number of evaluations, (default: maximum integer)\n:rel - relative error\n:abs - absolute error\n:integration-points - number of integration (quadrature) points for :gauss-legendre and :gauss-kronrod, default 7\n:initdiv - initial number of subdivisions for :gauss-kronrod, default: 1\n:info? - return full information about integration, default: false\n\n:gauss-kronrod is h-adaptive implementation\nFunction returns a map containing (if info? is true, returns result otherwise):\n\n:result - integration value\n:error - integration error (:gauss-kronrod only)\n:iterations - number of iterations\n:evaluations - number of evaluations\n:subdivisions - final number of boxes (:gauss-kronrod only)\n:fail? - set to :max-evals or :max-iters when one of the limits has been reached without the convergence.\n\nsource\n\n\n\nvegas\n\n(vegas f lower upper)\n(vegas f lower upper options)\n\nVEGAS+ - Monte Carlo integration of multivariate function, n&gt;1 dimensions.\nImproper integrals with infinite bounds are handled by a substitution.\nArguments:\n\nf - integrant\nlower - seq of lower bounds\nupper - seq of upper bounds\n\nAdditional options:\n\n:max-iters - maximum number of iterations, default: 10\n:nevals - number of evaluations per iteration, default: 10000\n:nintervals - number of grid intervals per dimension (default: 1000)\n:nstrats - number of stratifications per dimension (calculated)\n:warmup - number of warmup iterations (results are used to train stratification and grid spacings, default: 0\n:alpha - grid refinement parameter, 0.5 slow (default for vegas+), 1.5 moderate/fast (defatult for vegas)\n:beta - stratification damping parameter for startification adaptation, default: 0.75\n:rel - relative accuracy, default: 5.0e-4\n:abs - absolute accuracy, default: 5.0e-4\n:random-sequence - random sequence used for generating samples: :uniform (default), low-discrepancy sequences: :r2, :sobol and :halton.\n:jitter - jittering factor for low-discrepancy random sequence, default: 0.75\n:info? - return full information about integration, default: false\n:record-data? - stores samples, number of strata, x and dx, default: false (requires, :info? to be set to true)\n\nFor original VEGAS algorithm set :nstrats to 1.\n:nstrats can be also a list, then each dimension is divided independently according to a given number. If list is lower then number of dimensions, then it’s cycled.\nFunction returns a map with following keys (if info? is true, returns result otherwise):\n\n:result - value of integral\n:iterations - number of iterations (excluding warmup)\n:sd - standard deviation of results\n:nintervals - actual grid size\n:nstrats - number of stratitfications per dimension\n:nhcubes - number of hypercubes\n:evaluations - number of function calls\n:chi2-avg - average of chi2\n:dof - degrees of freedom\n:Q - goodness of fit indicator, 1 - very good, &lt;0.25 very poor\n:data - recorded data (if available)\n\nsource\n\n\n\nfastmath.solver\n\n\ncubic\n\n(cubic a b c d)\n\nReal solution of cubic formula ax3+bx2+cx+d=0. Returns NaNs where no real solutions are found.\nsource\n\n\n\nfind-root\n\n(find-root f lower-bound upper-bound)\n(find-root f lower-bound upper-bound {:keys [absolute-accuracy relative-accuracy max-iters initial-value solver], :or {max-iters 100, solver :brent}})\n\nFind zero (root) of a function f in given range [lower-bound, upper-bound].\nOptional parameters:\n\n:absolute-accuracy - default 1.0e-8\n:relative-accuracy\n:max-iters - maximum iterations (default: 100)\n:initial-value - algorithm starting value\n:solver - one of: :brent (default), :bisection, :illinois, :muller, :muller2, :pegasus, :regula-falsi, :ridders and :secant.\n\nsource\n\n\n\nquadratic\n\n(quadratic a b c)\n\nReal solutions of quadratic formula ax^2+bx+c=0. Returns NaNs where no real solutions are found.\nsource\n\nsource: clay/calculus.clj",
    "crumbs": [
      "Calculus"
    ]
  },
  {
    "objectID": "interpolation.html",
    "href": "interpolation.html",
    "title": "Interpolation",
    "section": "",
    "text": "1d target\nInterpolation namespace defines the unified API for various interpolation methods. Most of them also extrapolates. Methods include:\nAll methods are accessible from fastmath.interpolation namespace via a multimethod interpolation. Additionally each method is implemented as a regular function in the dedicated namespace. interpolation returns an interpolant function\nBoth examples below are equivalent:\nList of all possible methods:\nThe following functions and samples will be used as a target to illustrate usage of described method.\n\\[f(x)=\\sin\\left(\\frac{x\\cos(x+1)}{2}\\right)\\]\nPoints used in interpolation",
    "crumbs": [
      "Interpolation"
    ]
  },
  {
    "objectID": "interpolation.html#d-target",
    "href": "interpolation.html#d-target",
    "title": "Interpolation",
    "section": "",
    "text": "(defn target-1d [x] (m/sin (* 0.5 x (m/cos (inc x)))))\n\n\n(target-1d 4.0)\n\n\n0.5373775050861961\n\n\n\n(def xs1 [0.5 0.69 1.73 2.0 2.28 3.46 3.5 4.18 4.84 5.18 5.53 5.87 6.22 6.5])\n\n\n(def ys1 (map target-1d xs1))\n\n\n\n\n\n\n\n\nMonotone\n\n(def xxss (range 0 400 5))\n\n\n(def data (mapv (fn [^long v] (m/+ v (m/+ (m/* 30 (m/sin (m// v 60.0)))\n                                       (r/grand (m// v -2.0) (m// v 10.0))))) xxss))\n\n\n(ggplot/-&gt;file\n (ggplot/function+scatter (i/interpolation :cir xxss data) xxss data {:dot-size 2\n                                                                      :dot-alpha 0.6\n                                                                      :title \"CIR\"}))\n\n\n\n\n\n\n(ggplot/-&gt;file\n (ggplot/function+scatter (i/interpolation :cir xxss data {:method :monotone})\n                          xxss data {:dot-size 2\n                                     :dot-alpha 0.6\n                                     :title \"CIR smoothed\"}))",
    "crumbs": [
      "Interpolation"
    ]
  },
  {
    "objectID": "interpolation.html#d-target-1",
    "href": "interpolation.html#d-target-1",
    "title": "Interpolation",
    "section": "2d target",
    "text": "2d target\n\\[f(x,y)=\\sin\\left(\\frac{x-100}{10}\\cos\\left(\\frac{y}{20}\\right)\\right)+\\frac{x}{100}+\\left(\\frac{y-100}{100}\\right)^2+1\\]\n\n(defn target-2d [[x y]] (m/+ 1.0 (m/sin (* (/ (- x 100.0) 10.0) (m/cos (/ y 20.0))))\n                          (m// x 100.0)\n                          (m/sq (m// (m/- y 100.0) 100.0))))\n\n\n(target-2d [20 20])\n\n\n2.7649202623006808\n\n\nGrid\nPoints for grid interpolation\n\n(def xs2 [20 25 30 35 40 50 58 66 100 121 140 150 160 170 180])\n\n\n(def ys2 [20 30 58 66 90  121 140 152 170     180])\n\n\n(def zss (for [x xs2]\n         (for [y ys2]\n           (target-2d [x y]))))\n\n\n(def xss (repeatedly 300 #(vector (r/drandom uniform-seed-44 20 180)\n                                (r/drandom uniform-seed-44 20 180))))\n\n\n(def ys3 (map target-2d xss))\n\n\n(defn error-1d\n  [interpolant]\n  (m/sqrt (calc/integrate (fn [^double x] (m/sq (m/- (target-1d x) (interpolant x)))) 0.5 6.5)))\n\n\n(error-1d (linear/linear xs1 ys1))\n\n\n0.2110302144467739\n\nFor 2d case the following formula will be used:\n\\[error_{2d}(f,g)=\\|f-g\\|=\\sqrt{\\int_{20}^{180}\\int_{20}^{180}|f(x,y)-g(x,y)|^2\\,dx dy}\\]\n\n(defn error-2d\n  [interpolant]\n  (m/sqrt (calc/cubature (fn [xy] (m/sq (m/- (target-2d xy) (interpolant xy))))\n                         [20.0 20.0]\n                         [180.0 180.0])))\n\n\n(error-2d (linear/bilinear xs2 ys2 zss))\n\n\n102.03678750452109",
    "crumbs": [
      "Interpolation"
    ]
  },
  {
    "objectID": "interpolation.html#d",
    "href": "interpolation.html#d",
    "title": "Interpolation",
    "section": "1d",
    "text": "1d\n\nLinear\nLinear piecewise interpolation and extrapolation. Extrapolation uses a slope from the boundaries. See more on Wikipedia\n\n(require '[fastmath.interpolation.linear :as linear])\n\n\n(def linear (linear/linear xs1 ys1))\n\n\n(linear 4.0)\n\n\n0.49924424111385607\n\n\n(error-1d linear)\n\n\n0.2110302144467739\n\n\n\n\n\n\nCubic\nNatural cubic spline (second derivatives at boundary points have value \\(0\\)) interpolation and extrapolation. See more on Wikipedia\n\n(require '[fastmath.interpolation.cubic :as cubic])\n\n\n(def cubic (cubic/cubic xs1 ys1))\n\n\n(cubic 4.0)\n\n\n0.5516054931803801\n\n\n(error-1d cubic)\n\n\n0.0275840592896124\n\n\n\n\n\n\nAkima\nSee more on Wikipedia\n\n(require '[fastmath.interpolation.acm :as acm])\n\n\n(def akima (acm/akima xs1 ys1))\n\n\n(akima 4.0)\n\n\n0.5335842087231077\n\n\n(error-1d akima)\n\n\n0.03487751999898592\n\n\n\n\n\n\nNeville\nSee more on Wikipedia\n\n(require '[fastmath.interpolation.acm :as acm])\n\n\n(def neville (acm/neville xs1 ys1))\n\n\n(neville 4.0)\n\n\n0.5432043004304535\n\n\n(error-1d neville)\n\n\n0.8675392877418397\n\n\n\n\n\n\nBarycentric\nRational interpolation as described in Numerical Recipes ch. 3.4. The order (default \\(1\\)) parameter contols number of points used to calculate weights. Higher order means better accuracy.\n\n(require '[fastmath.interpolation.barycentric :as barycentric])\n\n\n(defn barycentric\n  ([] (barycentric/barycentric xs1 ys1))\n  ([order] (barycentric/barycentric xs1 ys1 {:order order})))\n\n\n((barycentric) 4.0)\n\n\n0.5492673111356233\n\n\n\n\n\n\norder\nerror\nbarrycentric(4.0)\nerror at 4.0\n\n\n\n\n0\n0.5193270391333753\n0.6329368698778738\n0.0955593647916777\n\n\n1\n0.03176373180495161\n0.5492673111356233\n0.011889806049427243\n\n\n2\n0.05019164899125852\n0.5160607443493412\n0.021316760736854845\n\n\n3\n0.028650229888319802\n0.5232915410624766\n0.014085964023719533\n\n\n4\n0.00351102181650211\n0.5349629695697342\n0.0024145355164618687\n\n\n5\n0.009022181871044352\n0.5387189359388596\n0.0013414308526634722\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB-spline\n\n(require '[fastmath.interpolation.ssj :as ssj])\n\n\n(defn b-spline\n  ([] (ssj/b-spline xs1 ys1))\n  ([degree] (b-spline degree nil))\n  ([degree hp1] (ssj/b-spline xs1 ys1 {:degree degree :hp1 hp1})))\n\n\n((b-spline) 4.0)\n\n\n0.1610170071559863\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDivided difference\n\n(require '[fastmath.interpolation.acm :as acm])\n\n\n(def divided-difference (acm/divided-difference xs1 ys1))\n\n\n(divided-difference 4.0)\n\n\n0.5432043004304531\n\n\n(error-1d divided-difference)\n\n\n0.8675392877418397\n\n\n\n\n\n\nPolynomial\n\n(require '[fastmath.interpolation.ssj :as ssj])\n\n\n(def polynomial (ssj/polynomial xs1 ys1))\n\n\n(polynomial 4.0)\n\n\n0.5432043380309324\n\n\n(error-1d polynomial)\n\n\n0.8675392846805364\n\n\n\n\n\n\nMonotone\n\n(require '[fastmath.interpolation.monotone :as monotone])\n\n\n(def monotone (monotone/monotone xs1 ys1))\n\n\n(monotone 4.0)\n\n\n0.6588206176299103\n\n\n(error-1d monotone)\n\n\n0.1517488499630331\n\n\n\n\n\n\nStep\n\n(require '[fastmath.interpolation.step :as step])\n\n\n(defn step\n  ([] (step/step xs1 ys1))\n  ([point] (step/step xs1 ys1 {:point point})))\n\n\n(def step-before (step/step-before xs1 ys1))\n\n\n(def step-after (step/step-after xs1 ys1))\n\n\n\n\n\n\nmethod\nerror\nvalue at 4.0\n\n\n\n\nstep-before\n0.849159325039357\n0.8087819747808206\n\n\nstep-after\n0.7429959099336633\n-0.3605827968499356\n\n\nstep\n0.4328611328633974\n0.8087819747808206\n\n\nstep (point=0.55)\n0.42287483285733546\n0.8087819747808206\n\n\nstep (point=0.25)\n0.5962341092433667\n0.8087819747808206\n\n\nstep (point=0.75)\n0.49446591280052093\n-0.3605827968499356\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoess\n\n(require '[fastmath.interpolation.acm :as acm])\n\n\n(defn loess\n  ([] (acm/loess xs1 ys1))\n  ([bandwidth] (acm/loess xs1 ys1 {:bandwidth bandwidth})))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCubic smoothing\n\n(require '[fastmath.interpolation.ssj :as ssj])\n\n\n(defn cubic-smoothing\n  ([] (ssj/cubic-smoothing xs1 ys1))\n  ([rho] (ssj/cubic-smoothing xs1 ys1 {:rho rho})))",
    "crumbs": [
      "Interpolation"
    ]
  },
  {
    "objectID": "interpolation.html#d-grid",
    "href": "interpolation.html#d-grid",
    "title": "Interpolation",
    "section": "2d grid",
    "text": "2d grid\n\nBilinear\n\n(require '[fastmath.interpolation.linear :as linear])\n\n\n(def bilinear (linear/bilinear xs2 ys2 zss))\n\n\n(error-2d bilinear)\n\n\n102.03678750452109\n\n\n\nBicubic\n\n(require '[fastmath.interpolation.acm :as acm])\n\n\n(def bicubic (acm/bicubic xs2 ys2 zss))\n\n\n(error-2d bicubic)\n\n\n103.97025992767536\n\n\n\nCubic 2d\n\n(require '[fastmath.interpolation.cubic :as cubic])\n\n\n(def cubic-2d (cubic/cubic-2d xs2 ys2 zss))\n\n\n(error-2d cubic-2d)\n\n\n103.23387133898065",
    "crumbs": [
      "Interpolation"
    ]
  },
  {
    "objectID": "interpolation.html#multivariate-and-kernel-based",
    "href": "interpolation.html#multivariate-and-kernel-based",
    "title": "Interpolation",
    "section": "Multivariate and kernel based",
    "text": "Multivariate and kernel based\n\nMicrosphere projection\n\n(require '[fastmath.interpolation.acm :as acm])\n\n\n\n\n\n\n\n\n\n\n\n\n\n(error-1d (acm/microsphere-projection xs1 ys1))\n\n\n0.20201293127226447\n\n\n(error-2d (acm/microsphere-projection xss ys3))\n\n\n54.40226214886831\n\n\n\nShepard\n\n(require '[fastmath.interpolation.shepard :as shepard])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRadial Basis Function\n\n(require '[fastmath.interpolation.rbf :as rbf])\n\n\n(defn chart-f [f title] (-&gt; (ggplot/function f {:x [-5 5] :title title})\n                         (ggplot/-&gt;image)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolynomial term\n\n(defn polynomial-terms-1d [^double x]\n  [1.0 x (m/sq x)])\n\n\n(defn polynomial-terms-2d [[^double x ^double y]]\n  [1.0 x y (m/* x y) (m/sq x) (m/sq y)])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(error-2d (rbf/rbf xss ys3 (kernel/rbf :gaussian {:shape 0.1})))\n\n\n365.96788014805696\n\n\n(error-2d (rbf/rbf xss ys3 (kernel/rbf :matern-c2 {:shape 0.15})))\n\n\n316.10296877225454\n\n\n(error-2d (rbf/rbf xss ys3 (kernel/rbf :gaussians-laguerre-22 {:shape 0.07})))\n\n\n376.435234072405\n\n\n(error-2d (rbf/rbf xss ys3 (kernel/rbf :thin-plate)))\n\n\n49.070725801843054\n\n\n\nSmoothing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKriging\n\nVariograms\n\n(require '[fastmath.kernel.variogram :as variogram]\n         '[fastmath.interpolation.kriging :as kriging])\n\n\n(defn svar-image [f emp title]\n  (let [x (map :h emp)\n        y (map :gamma emp)]\n    (-&gt; (ggplot/function+scatter f x y {:title title :ylim [0 nil]})\n        (ggplot/-&gt;image))))\n\n\n(def empirical-matheron-1d (variogram/empirical xs1 ys1))\n\n\n(def empirical-matheron (variogram/empirical xss ys3 {:size 20}))\n\n\nempirical-matheron\n\n\n[{:n 116, :h 3.3604788552002978, :gamma 0.05610650116225876} {:n 385, :h 7.841219583707931, :gamma 0.18995728933979483} {:n 606, :h 12.72578480586009, :gamma 0.399844655716175} {:n 856, :h 17.611233442491304, :gamma 0.4194793467635297} {:n 1019, :h 22.694649851980536, :gamma 0.4993167205111838} {:n 1173, :h 27.74503808347392, :gamma 0.5232922610403302} {:n 1322, :h 32.696030602647625, :gamma 0.5223125852571721} {:n 1527, :h 37.65818295299809, :gamma 0.5426056881417575} {:n 1594, :h 42.65227807229347, :gamma 0.5227361157844413} {:n 1803, :h 47.67443166086058, :gamma 0.5243286183079909} {:n 1784, :h 52.675449304556054, :gamma 0.5272068497879318} {:n 1922, :h 57.731016242599935, :gamma 0.5285755419763869} {:n 1909, :h 62.70585327047225, :gamma 0.5652895296636227} {:n 1966, :h 67.77992830214916, :gamma 0.5265816327093601} {:n 1977, :h 72.71580210020514, :gamma 0.49759061919470493}]\n\n\n\n\n\n(def empirical-cressie (variogram/empirical xss ys3 {:estimator :cressie :size 20}))\n\n\n\n\n\n(def empirical-highly-robust (variogram/empirical xss ys3 {:estimator :highly-robust :size 20\n                                                         :remove-outliers? true}))\n\n\nempirical-highly-robust\n\n\n[{:n 116, :h 3.3604788552002978, :gamma 0.02913016400873318} {:n 385, :h 7.841219583707931, :gamma 0.14056514492191244} {:n 606, :h 12.72578480586009, :gamma 0.42633847448010354} {:n 856, :h 17.611233442491304, :gamma 0.44483727296978925} {:n 1019, :h 22.694649851980536, :gamma 0.5434941912574424} {:n 1173, :h 27.74503808347392, :gamma 0.571757191071463} {:n 1322, :h 32.696030602647625, :gamma 0.5504904163482222} {:n 1527, :h 37.65818295299809, :gamma 0.5905951431086924} {:n 1594, :h 42.65227807229347, :gamma 0.5577607587471852} {:n 1803, :h 47.67443166086058, :gamma 0.555261258787647} {:n 1784, :h 52.675449304556054, :gamma 0.561444986664588} {:n 1922, :h 57.731016242599935, :gamma 0.5631892305752904} {:n 1909, :h 62.70585327047225, :gamma 0.6128688264262536} {:n 1966, :h 67.77992830214916, :gamma 0.5591266023267967} {:n 1977, :h 72.71580210020514, :gamma 0.5142798048326319}]\n\n\n(def empirical-quantile (variogram/empirical xss ys3 {:estimator :quantile :size 50\n                                                    :quantile 0.92}))\n\n\n(def empirical-M-robust (variogram/empirical xss ys3 {:estimator :m-robust :size 50}))\n\n\n\n\n\n\nSemi-variograms\n\n(def variogram-linear  (variogram/fit empirical-quantile :linear))\n\n\n(def variogram-gaussian (variogram/fit empirical-highly-robust :gaussian))\n\n\n(def variogram-pentaspherical (variogram/fit empirical-highly-robust :pentaspherical))\n\n\n(def variogram-rbf-wendland-2-3 (variogram/fit empirical-highly-robust (kernel/rbf :wendland {:s 2 :k 3})))\n\n\n(def variogram-superspherical-1d (variogram/fit empirical-matheron-1d :tplstable {:order 1.9 :defaults {:beta 14.0}}))\n\n\n(((variogram/-&gt;superspherical 1.0) {:nugget 0.1 :psill 0.5 :range 1.0}) 0.4)\n\n\n0.384\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(def kriging-linear (kriging/kriging xss ys3 variogram-linear))\n\n\n(def kriging-gaussian (kriging/kriging xss ys3 variogram-gaussian))\n\n\n(def kriging-pentaspherical (kriging/kriging xss ys3 variogram-pentaspherical))\n\n\n(def kriging-rbf-wendland-2-3 (kriging/kriging xss ys3 variogram-rbf-wendland-2-3))\n\n\n(error-2d kriging-linear)\n\n\n56.63634949359084\n\n\n(def vl (variogram/linear {:nugget 0.03 :sill 0.5 :range 14.0}))\n\n\n\nSmoothing\n\n\n\nGaussian processes\n\nsource: clay/interpolation.clj",
    "crumbs": [
      "Interpolation"
    ]
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Optimization",
    "section": "",
    "text": "Bayesian optimization",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "optimization.html#reference",
    "href": "optimization.html#reference",
    "title": "Optimization",
    "section": "Reference",
    "text": "Reference\n\nfastmath.optimization\nOptimization.\nNamespace provides various optimization methods.\n\nBrent (1d functions)\nBobyqa (2d+ functions)\nPowell\nNelder-Mead\nMultidirectional simplex\nCMAES\nGradient\nL-BFGS-B\nBayesian Optimization (see below)\nLinear optimization\n\nAll optimizers require bounds.\n## Optimizers\nTo optimize functions call one of the following functions:\n\nminimize or maximize - to perform actual optimization\nscan-and-minimize or scan-and-maximize - functions find initial point using brute force and then perform optimization paralelly for best initialization points. Brute force scan is done using jitter low discrepancy sequence generator.\n\nYou can also create optimizer (function which performs optimization) by calling minimizer or maximizer. Optimizer accepts initial point.\nAll above accept:\n\none of the optimization method, ie: :brent, :bobyqa, :nelder-mead, :multidirectional-simplex, :cmaes, :gradient, :bfgs and :lbfgsb\nfunction to optimize\nparameters as a map\n\nFor parameters meaning refer Optim package\n### Common parameters\n\n:bounds (obligatory) - search ranges for each dimensions as a seqence of [low high] pairs\n:initial - initial point other then mid of the bounds as vector\n:max-evals - maximum number of function evaluations\n:max-iters - maximum number of algorithm interations\n:bounded? - should optimizer force to keep search within bounds (some algorithms go outside desired ranges)\n:stats? - return number of iterations and evaluations along with result\n:rel and :abs - relative and absolute accepted errors\n\nFor scan-and-... functions additionally you can provide:\n\n:N - number of brute force iterations\n:n - fraction of N which are used as initial points to parallel optimization\n:jitter - jitter factor for sequence generator (for scanning domain)\n\n### Specific parameters\n\nBOBYQA - :number-of-points, :initial-radius, :stopping-radius\nNelder-Mead - :rho, :khi, :gamma, :sigma, :side-length\nMultidirectional simples - :khi, :gamma, :side-length\nCMAES - :check-feasable-count, :diagonal-only, :stop-fitness, :active-cma?, :population-size\nGradient - :bracketing-range, :formula (:polak-ribiere or :fletcher-reeves), :gradient-h (finite differentiation step, default: 0.01)\n\n## Bayesian Optimization\nBayesian optimizer can be used for optimizing expensive to evaluate black box functions. Refer this article or this article\n## Linear optimization\n\n\nbayesian-optimization\n\n(bayesian-optimization f {:keys [warm-up init-points bounds utility-function-type utility-param kernel kscale jitter noise optimizer optimizer-params normalize?], :or {utility-function-type :ucb, init-points 3, jitter 0.25, noise 1.0E-8, utility-param (if (#{:ei :poi} utility-function-type) 0.001 2.576), warm-up (* (count bounds) 1000), normalize? true, kernel :matern-52, kscale 1.0}})\n\nBayesian optimizer\nParameters are:\n\n:warm-up - number of brute force iterations to find maximum of utility function\n:init-points - number of initial evaluation before bayesian optimization starts. Points are selected using jittered low discrepancy sequence generator (see: jittered-sequence-generator\n:bounds - bounds for each dimension\n:utility-function-type - one of :ei, :poi or :ucb\n:utility-param - parameter for utility function (kappa for ucb and xi for ei and poi)\n:kernel - kernel, default :matern-52, see fastmath.kernel\n:kscale - scaling factor for kernel\n:jitter - jitter factor for sequence generator (used to find initial points)\n:noise - noise (lambda) factor for gaussian process\n:optimizer - name of optimizer (used to optimized utility function)\n:optimizer-params - optional parameters for optimizer\n:normalize? - normalize data in gaussian process?\n\nReturns lazy sequence with consecutive executions. Each step consist:\n\n:x - maximum x\n:y - value\n:xs - list of all visited x’s\n:ys - list of values for every visited x\n:gp - current gaussian process regression instance\n:util-fn - current utility function\n:util-best - best x in utility function\n\nsource\n\n\n\nlinear-optimization\n\n(linear-optimization target constraints)\n(linear-optimization target constraints {:keys [goal epsilon max-ulps cut-off rule non-negative? max-iter stats?], :or {goal :minimize, epsilon 1.0E-6, max-ulps 10, cut-off 1.0E-10, rule :dantzig, non-negative? false, max-iter Integer/MAX_VALUE}})\n\nSolves a linear problem.\nTarget is defined as a vector of coefficients and constant as the last value: [a1 a2 a3 ... c] means f(x1,x2,x3) = a1*x1 + a2*x2 + a3*x3 + ... +  c\nConstraints are defined as a sequence of one of the following triplets:\n\n[a1 a2 a3 ...] R n - which means a1*x1+a2*x2+a3*x3+... R n\n[a1 a2 a3 ... ca] R [b1 b2 b3 ... cb] - which means a1*x1+a2*x2+a3*x3+...+ca R b1*x1+b2*x2+b3*x3+...+cb\n\nR is a relationship and can be one of &lt;=, &gt;= or = as symbol or keyword. Also :leq, :geq and :eq are valid.\nFunction returns pair of optimal point and function value. If stat? option is set to true, returns also information about number of iterations.\nPossible options:\n\n:goal - :minimize (default) or :maximize\n:rule - pivot selection rule, :dantzig (default) or :bland\n:max-iter - maximum number of iterations, maximum integer by default\n:non-negative? - allow non-negative variables only, default: false\n:epsilon - convergence value, default: 1.0e-6:\n:max-ulps - floating point comparisons, default: 10 ulp\n:cut-off - pivot elements smaller than cut-off are treated as zero, default: 1.0e-10\n\n(linear-optimization [-1 4 0] [[-3 1] :&lt;= 6\n                               [-1 -2] :&gt;= -4\n                               [0 1] :&gt;= -3])\n;; =&gt; [(9.999999999999995 -3.0) -21.999999999999993]\nsource\n\n\n\nmaximize\n\n(maximize method f config)\n\nMaximize given function.\nParameters: optimization method, function and configuration.\nsource\n\n\n\nmaximizer\n\n(maximizer method f config)\n\nCreate optimizer which maximizes function.\nReturns function which performs optimization for optionally given initial point.\nsource\n\n\n\nminimize\n\n(minimize method f config)\n\nMinimize given function.\nParameters: optimization method, function and configuration.\nsource\n\n\n\nminimizer\n\n(minimizer method f config)\n\nCreate optimizer which minimizes function.\nReturns function which performs optimization for optionally given initial point.\nsource\n\n\n\nscan-and-maximize\nsource\n\n\n\nscan-and-minimize\nsource\n\nsource: clay/optimization.clj",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "transform.html",
    "href": "transform.html",
    "title": "Transforms",
    "section": "",
    "text": "FFT\nGeneral description of the topic\nDetails about FFT and use-cases\nSome examples:",
    "crumbs": [
      "Transforms"
    ]
  },
  {
    "objectID": "transform.html#fft",
    "href": "transform.html#fft",
    "title": "Transforms",
    "section": "",
    "text": "(def fft-real (t/transformer :real :fft ))\n\n\n\n\n\n\n\nExamples\n\n\n\n\n(seq (t/forward-1d fft-real [1 2 -10 1])) ;; =&gt; (-6.0 -12.0 11.0 -1.0)\n(seq (t/reverse-1d fft-real [-6 -12 11 -1])) ;; =&gt; (1.0 2.0 -10.0 1.0)",
    "crumbs": [
      "Transforms"
    ]
  },
  {
    "objectID": "transform.html#wavelets",
    "href": "transform.html#wavelets",
    "title": "Transforms",
    "section": "Wavelets",
    "text": "Wavelets",
    "crumbs": [
      "Transforms"
    ]
  },
  {
    "objectID": "transform.html#compression-and-denoising",
    "href": "transform.html#compression-and-denoising",
    "title": "Transforms",
    "section": "Compression and denoising",
    "text": "Compression and denoising\nAn use case with charts\n\n(def domain (m/slice-range 0 10 512))\n\n\n(def signal (map (fn [x] (+ (Math/sin x)\n                         (* 0.1 (- (rand) 0.5)))) ;; add some noise\n               domain))\n\n\n(def denoised-signal (t/denoise fft-real signal {:method :hard}))",
    "crumbs": [
      "Transforms"
    ]
  },
  {
    "objectID": "transform.html#reference",
    "href": "transform.html#reference",
    "title": "Transforms",
    "section": "Reference",
    "text": "Reference\n\nfastmath.transform\nTransforms.\nSee transformer and TransformProto for details.\n### Wavelet\nBased on JWave library.\nBe aware that some of the wavelet types doesn’t work properly. :battle-23, :cdf-53, :cdf-97.\n### Cos/Sin/Hadamard\nOrthogonal or standard fast sine/cosine/hadamard 1d transforms.\n### Fourier\nDFT, FFT, DHT.\n\n\ncompress\n\n(compress trans xs mag)\n(compress xs mag)\n\nCompress transformed signal xs with given magnitude mag.\nsource\n\n\n\ncompress-peaks-average\n\n(compress-peaks-average trans xs)\n(compress-peaks-average xs)\n\nCompress transformed signal xs with peaks average as a magnitude\nsource\n\n\n\ndenoise\n\n(denoise xs {:keys [method threshold skip], :or {method :hard, threshold :universal, skip 0}})\n(denoise trans xs method)\n(denoise xs)\n\nWavelet shrinkage with some threshold.\nMethods can be: * :hard (default)\n* :soft * :garrote * :hyperbole\n:threshold can be a number of one of the denoise-threshold methods (default: :visu)\n:skip can be used to leave :skip number of coefficients unaffected (default: 0)\nUse on transformed sequences or call with transformer object.\nsource\n\n\n\ndenoise-threshold\n\n(denoise-threshold xs threshold)\n\nCalculate optimal denoise threshold.\nthreshold is one of the following\n\n:visu - based on median absolute deviation estimate (default)\n:universal - based on standard deviation estimate\n:sure or :rigrsure - based on SURE estimator\n:hybrid or :heursure - hybrid SURE estimator\n\nsource\n\n\n\nforward-1d\n\n(forward-1d t xs)\n\nForward transform of sequence or array.\nsource\n\n\n\nforward-2d\n\n(forward-2d t xss)\n\nForward transform of sequence or array.\nsource\n\n\n\nreverse-1d\n\n(reverse-1d t xs)\n\nForward transform of sequence or array.\nsource\n\n\n\nreverse-2d\n\n(reverse-2d t xss)\n\nForward transform of sequence or array.\nsource\n\n\n\ntransformer\nCreate transform object for given wavelet.\n#### Wavelets\n\n:fast for 1d or 2d Fast Wavelet Transform. Size of data should be power of 2.\n:packet for 1d or 2d Wavelet Packet Transform. Size of data should be power of 2.\n:decomposed-fast for 1d Fast Wavelet Transform. Data can have any size (Ancient Egyptian Decomposition is used).\n:decomposed-packet for 1d Wavelet Packet Transform. Data can have any size (Ancient Egyptian Decomposition is used).\n\nSecond argument is wavelet name as key. See wavelets-list for all supported names.\n#### Sine/Cosine/Hadamard\n\n:standard for 1d :sine, :cosine, :hadamard.\n:orthogonal for 1d :sine, :cosine.\n\nNote that :sine and :cosine require first element to be equal 0. Size of data should be power of 2.\n#### Fourier\n\n:standard :dft - 1d Discrete Fourier Transform - returns double-array where even elements are real part, odd elements are imaginary part.\n\nsource\n\n\n\nwavelets-list\nList of all possible wavelets.\nsource\n\nsource: clay/transform.clj",
    "crumbs": [
      "Transforms"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Regression",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#clustering",
    "href": "ml.html#clustering",
    "title": "Machine Learning",
    "section": "Clustering",
    "text": "Clustering\n\nfastmath.ml.regression\nOLS, WLS and GLM regression models with analysis.\n\n\n-&gt;Family\n\n(-&gt;Family default-link variance initialize residual-deviance aic quantile-residuals-fun dispersion)\n\nPositional factory function for class fastmath.ml.regression.Family.\nsource\n\n\n\n-&gt;GLMData\n\n(-&gt;GLMData model transformer xtxinv ys intercept? offset? intercept beta coefficients observations residuals fitted weights offset names deviance df dispersion dispersions estimated-dispersion? & overage)\n\nPositional factory function for class fastmath.ml.regression.GLMData.\nsource\n\n\n\n-&gt;LMData\n\n(-&gt;LMData model intercept? offset? transformer xtxinv intercept beta coefficients offset weights residuals fitted df observations names r-squared adjusted-r-squared sigma2 sigma tss & overage)\n\nPositional factory function for class fastmath.ml.regression.LMData.\nsource\n\n\n\n-&gt;Link\n\n(-&gt;Link g mean derivative)\n\nPositional factory function for class fastmath.ml.regression.Link.\nsource\n\n\n\n-&gt;family\n\n(-&gt;family family-map)\n(-&gt;family default-link variance initialize residual-deviance aic quantile-residuals-fun dispersion)\n(-&gt;family variance residual-deviance)\n\nCreate Family record.\nArguments:\n\ndefault-link - canonical link function, default: :identity\nvariance - variance function in terms of mean\ninitialize - initialization of glm, default: the same as in :gaussian\nresidual-deviance - calculates residual deviance\naic - calculates AIC, default (constantly ##NaN)\nquantile-residuals-fun - calculates quantile residuals, default as in :gaussian\ndisperation - value or :estimate (default), :pearson or :mean-deviance\n\nInitialization will be called with ys and weights and should return:\n\nys, possibly changed if any adjustment is necessary\ninit-mu, starting point\nweights, possibly changes or orignal\n(optional) any other data used to calculate AIC\n\nAIC function should accept: ys, fitted, weights, deviance, observation, rank (fitted parameters) and additional data created by initialization\nMinimum version should define variance and residual-deviance.\nsource\n\n\n\n-&gt;link\n\n(-&gt;link link-map)\n(-&gt;link g mean mean-derivative)\n\nCreates link record.\nArgs:\n\ng - link function\nmean - mean, inverse link function\nmean-derivative - derivative of mean\n\nsource\n\n\n\n-&gt;string\nsource\n\n\n\nanalysis\n\n(analysis model)\n\nInfluence analysis, laverage, standardized and studentized residuals, correlation.\nsource\n\n\n\ncir\n\n(cir ys)\n(cir xs ys)\n(cir xs ys order)\n(cir xs ys ws order)\n\nCentered Isotonic Regression.\nReturns shrinked [xs,ys] pair.\nArguments: - xs - regressor variable - ys - response variable - ws - weights (optional) - order - :asc or :increasing (default), :desc or :decreasing, :non-decreasing and :non-increasing.\nsource\n\n\n\ndose\n\n(dose glm-model)\n(dose glm-model p)\n(dose glm-model p coeff-id)\n(dose {:keys [link-fun xtxinv coefficients]} p intercept-id coeff-id)\n\nPredict Lethal/Effective dose for given p (default: p=0.5, median).\n\nintercept-id - id of intercept, default: 0\ncoeff-id is the coefficient used for calculating dose, default: 1\n\nsource\n\n\n\nfamilies\nsource\n\n\n\nfamily-with-link\n\n(family-with-link family)\n(family-with-link family params)\n(family-with-link family link params)\n\nReturns family with a link as single map.\nsource\n\n\n\nglm\n\n(glm ys xss)\n(glm ys xss {:keys [max-iters tol epsilon family link weights alpha offset dispersion-estimator intercept? init-mu simple? transformer names], :or {max-iters 25, tol 1.0E-8, epsilon 1.0E-8, family :gaussian, alpha 0.05, intercept? true, simple? false}, :as params})\n\nFit a generalized linear model using IRLS method.\nArguments:\n\nys - response vector\nxss - terms of systematic component\noptional parameters\n\nParameters:\n\n:tol - tolerance for matrix decomposition (SVD and Cholesky), default: 1.0e-8\n:epsilon - tolerance for IRLS (stopping condition), default: 1.0e-8\n:max-iters - maximum numbers of iterations, default: 25\n:weights - optional weights\n:offset - optional offset\n:alpha - significance level, default: 0.05\n:intercept? - should intercept term be included, default: true\n:init-mu - initial response vector for IRLS\n:simple? - returns simplified result\n:dispersion-estimator - :pearson, :mean-deviance or any number, replaces default one.\n:family - family, default: :gaussian\n:link - link\n:nbinomial-theta - theta for :nbinomial family, default: 1.0.\n:transformer - an optional function which will be used to transform systematic component xs before fitting and prediction\n:names - an optional vector of names to use when printing the model\n\nFamily is one of the: :gaussian (default), :binomial, :quasi-binomial, :poisson, :quasi-poisson, :gamma, :inverse-gaussian, :nbinomial, custom Family record (see -&gt;family) or a function returning Family (accepting a map as an argument)\nLink is one of the: :probit, :identity, :loglog, :sqrt, :inverse, :logit, :power, :nbinomial, :cauchit, :distribution, :cloglog, :inversesq, :log, :clog, custom Link record (see -&gt;link) or a function returning Link (accepting a map as an argument)\nNotes:\n\nSVD decomposition is used instead of more common QR\nintercept term is added implicitely if intercept? is set to true (by default)\n:nbinomial family requires :nbinomial-theta parameter\nEach family has its own default (canonical) link.\n\nReturned record implementes IFn protocol and contains:\n\n:model - set to :glm\n:intercept? - whether intercept term is included or not\n:xtxinv - (X^T X)^-1\n:intercept - intercept term value\n:beta - vector of model coefficients (without intercept)\n:coefficients - coefficient analysis, a list of maps containing :estimate, :stderr, :t-value, :p-value and :confidence-interval\n:weights - weights, :weights (working) and :initial\n:residuals - a map containing :raw, :working, :pearsons and :deviance residuals\n:fitted - fitted values for xss\n:df - degrees of freedom: :residual, :null and :intercept\n:observations - number of observations\n:deviance - deviances: :residual and :null\n:dispersion - default or calculated, used in a model\n:dispersions - :pearson and :mean-deviance\n:family - family used\n:link - link used\n:link-fun - link function, g\n:mean-fun - mean function, g^-1\n:q - (1-alpha/2) quantile of T or Normal distribution for residual degrees of freedom\n:chi2 and :p-value - Chi-squared statistic and respective p-value\n:ll - a map containing log-likelihood and AIC/BIC\n:analysis - laverage, residual and influence analysis - a delay\n:iters and :converged? - number of iterations and convergence indicator\n\nAnalysis, delay containing a map:\n\n:residuals - :standardized and :studentized residuals (pearsons and deviance)\n:laverage - :hat, :sigmas and laveraged :coefficients (leave-one-out)\n:influence - :cooks-distance, :dffits, :dfbetas and :covratio\n:influential - list of influential observations (ids) for influence measures\n:correlation - correlation matrix of estimated parameters\n\nsource\n\n\n\nglm-nbinomial\n\n(glm-nbinomial ys xss)\n(glm-nbinomial ys xss {:keys [nbinomial-theta max-iters epsilon], :or {max-iters 25, epsilon 1.0E-8}, :as params})\n\nFits theta for negative binomial glm in iterative process.\nReturns fitted model with :nbinomial-theta key.\nArguments and parameters are the same as for glm.\nAdditional parameters:\n\n:nbinomial-theta - initial theta used as a starting point for optimization.\n\nsource\n\n\n\nlinks\nsource\n\n\n\nlm\n\n(lm ys xss)\n(lm ys xss {:keys [tol weights alpha intercept? offset transformer names], :or {tol 1.0E-8, alpha 0.05, intercept? true}})\n\nFit a linear model using ordinary (OLS) or weighted (WLS) least squares.\nArguments:\n\nys - response vector\nxss - terms of systematic component\noptional parameters\n\nParameters:\n\n:tol - tolerance for matrix decomposition (SVD and Cholesky), default: 1.0e-8\n:weights - optional weights for WLS\n:offset - optional offset\n:alpha - significance level, default: 0.05\n:intercept? - should intercept term be included, default: true\n:transformer - an optional function which will be used to transform systematic component xs before fitting and prediction\n:names - sequence or string, used as name for coefficient when pretty-printing model, default 'X'\n\nNotes:\n\nSVD decomposition is used instead of more common QR\nintercept term is added implicitely if intercept? is set to true (by default)\nTwo variants of AIC/BIC are calculated, one based on log-likelihood, second on RSS/n\n\nReturned record implementes IFn protocol and contains:\n\n:model - :ols or :wls\n:intercept? - whether intercept term is included or not\n:xtxinv - (X^T X)^-1\n:intercept - intercept term value\n:beta - vector of model coefficients (without intercept)\n:coefficients - coefficient analysis, a list of maps containing :estimate, :stderr, :t-value, :p-value and :confidence-interval\n:weights - initial weights\n:residuals - a map containing :raw and :weighted residuals\n:fitted - fitted values for xss\n:df - degrees of freedom: :residual, :model and :intercept\n:observations - number of observations\n:r-squared and :adjusted-r-squared\n:sigma and :sigma2 - deviance and variance\n:msreg - regression mean squared\n:rss, :regss, :tss - residual, regression and total sum of squares\n:qt - (1-alpha/2) quantile of T distribution for residual degrees of freedom\n:f-statistic and :p-value - F statistic and respective p-value\n:ll - a map containing log-likelihood and AIC/BIC in two variants: based on log-likelihood and RSS\n:analysis - laverage, residual and influence analysis - a delay\n\nAnalysis, delay containing a map:\n\n:residuals - :standardized and :studentized weighted residuals\n:laverage - :hat, :sigmas and laveraged :coefficients (leave-one-out)\n:influence - :cooks-distance, :dffits, :dfbetas and :covratio\n:influential - list of influential observations (ids) for influence measures\n:correlation - correlation matrix of estimated parameters\n:normality - residuals normality tests: :skewness, :kurtosis, :durbin-watson (for raw and weighted), :jarque-berra and :omnibus (normality)\n\nsource\n\n\n\nmap-&gt;Family\n\n(map-&gt;Family m__7997__auto__)\n\nFactory function for class fastmath.ml.regression.Family, taking a map of keywords to field values.\nsource\n\n\n\nmap-&gt;GLMData\n\n(map-&gt;GLMData m__7997__auto__)\n\nFactory function for class fastmath.ml.regression.GLMData, taking a map of keywords to field values.\nsource\n\n\n\nmap-&gt;LMData\n\n(map-&gt;LMData m__7997__auto__)\n\nFactory function for class fastmath.ml.regression.LMData, taking a map of keywords to field values.\nsource\n\n\n\nmap-&gt;Link\n\n(map-&gt;Link m__7997__auto__)\n\nFactory function for class fastmath.ml.regression.Link, taking a map of keywords to field values.\nsource\n\n\n\npava\n\n(pava ys)\n(pava ys order)\n(pava ys ws order)\n\nIsotonic regression, pool-adjacent-violators algorithm with up-and-down-blocks variant.\nIsotonic regression minimizes the (weighted) L2 loss function with a constraint that result should be monotonic (ascending or descending).\nArguments:\n\nys - response variable data\nws - weights (optional)\norder - :asc or :increasing (default), :desc or :decreasing, :non-decreasing and :non-increasing\n\nReturns monotonic predicted values.\nsource\n\n\n\npredict\n\n(predict model xs)\n(predict model xs stderr?)\n\nPredict from the given model and data point.\nIf stderr? is true, standard error and confidence interval is added. If model is fitted with offset, first element of data point should contain provided offset.\nExpected data point:\n\n[x1,x2,...,xn] - when model was trained without offset\n[offset,x1,x2,...,xn] - when offset was used for training\n[] or nil - when model was trained with intercept only\n[offset] - when model was trained with intercept and offset\n\nsource\n\n\n\nquantile-residuals\n\n(quantile-residuals {:keys [quantile-residuals-fun residuals dispersion], :as model})\n\nQuantile residuals for a model, possibly randomized.\nsource\n\n\n\nfastmath.ml.clustering\n\n\ndbscan\n\n(dbscan xss)\n(dbscan xss {:keys [eps points distance add-data?], :or {distance :euclidean, add-data? true}})\n\nsource\n\n\n\nfuzzy-kmeans\n\n(fuzzy-kmeans xss)\n(fuzzy-kmeans xss {:keys [clusters fuzziness max-iters distance epsilon rng add-data?], :or {clusters 1, fuzziness 2, max-iters -1, distance :euclidean, epsilon 0.001, rng (r/rng :default), add-data? true}})\n\nsource\n\n\n\ninfer-dbscan-radius\n\n(infer-dbscan-radius xss dist)\n\nsource\n\n\n\nkmeans++\n\n(kmeans++ xss)\n(kmeans++ xss {:keys [clusters max-iters distance rng empty-cluster-strategy trials add-data?], :or {clusters 1, max-iters -1, distance :euclidean, empty-cluster-strategy :largest-variance, trials 1, rng (r/rng :default), add-data? true}})\n\nsource\n\nsource: clay/ml.clj",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "easings.html",
    "href": "easings.html",
    "title": "Easings",
    "section": "",
    "text": "Reference",
    "crumbs": [
      "Easings"
    ]
  },
  {
    "objectID": "easings.html#reference",
    "href": "easings.html#reference",
    "title": "Easings",
    "section": "",
    "text": "fastmath.easings\nEasing functions.\nList of all are in easings-list.\n\n\nback-in\n\n(back-in t)\n(back-in s t)\n\nBackIn easing.\nParameter s (default: 1.70158) defines overshoot.\nsource\n\n\n\nback-in-out\n\n(back-in-out t)\n(back-in-out s t)\n\nBackInOut easing.\nParameter s (default: 1.70158) defines overshoot.\nsource\n\n\n\nback-out\n\n(back-out t)\n(back-out s t)\n\nBackOut easing.\nParameter s (default: 1.70158) defines overshoot.\nsource\n\n\n\nbounce-in\n\n(bounce-in t)\n\nBounceIn easing\nsource\n\n\n\nbounce-in-out\n\n(bounce-in-out t)\n\nBounceInOut easing\nsource\n\n\n\nbounce-out\n\n(bounce-out t)\n\nBounceOut easing\nsource\n\n\n\ncircle-in\n\n(circle-in t)\n\nCircleIn easing\nsource\n\n\n\ncircle-in-out\n\n(circle-in-out t)\n\nCircleInOut easing\nsource\n\n\n\ncircle-out\n\n(circle-out t)\n\nCircleIn easing\nsource\n\n\n\ncubic-in\n\n(cubic-in t)\n\nCubicIn easing\nsource\n\n\n\ncubic-in-out\n\n(cubic-in-out t)\n\nCubicInOut easing\nsource\n\n\n\ncubic-out\n\n(cubic-out t)\n\nCubicOut easing\nsource\n\n\n\neasings-list\nMap of easing names (as keywords) and functions.\nsource\n\n\n\nelastic-in\n\n(elastic-in t)\n(elastic-in amplitude period)\n\nElasticIn.\nWhen called with t parameter, returns easing value (for amplitude=1.0 and period=0.3). When called with amplitude and period returns custom easing function.\nsource\n\n\n\nelastic-in-out\n\n(elastic-in-out t)\n(elastic-in-out amplitude period)\n\nElasticInOut.\nWhen called with t parameter, returns easing value (for amplitude=1.0 and period=0.3). When called with amplitude and period returns custom easing function.\nsource\n\n\n\nelastic-out\n\n(elastic-out t)\n(elastic-out amplitude period)\n\nElasticOut.\nWhen called with t parameter, returns easing value (for amplitude=1.0 and period=0.3). When called with amplitude and period returns custom easing function.\nsource\n\n\n\nexp-in\n\n(exp-in t)\n\nExpIn easing\nsource\n\n\n\nexp-in-out\n\n(exp-in-out t)\n\nExpInOut easing\nsource\n\n\n\nexp-out\n\n(exp-out t)\n\nExpOut easing\nsource\n\n\n\nin-out\n\n(in-out easeing)\n\nCreate in-out easing for given easing function.\nsource\n\n\n\nlinear\n\n(linear t)\n\nLinear easing (identity)\nsource\n\n\n\nout\n\n(out easeing)\n\nCreate out easing for given easing function.\nsource\n\n\n\npoly-in\n\n(poly-in t)\n(poly-in e t)\n\nPolyIn easing.\nOptional exponent e defaults to 3.0.\nsource\n\n\n\npoly-in-out\n\n(poly-in-out t)\n(poly-in-out e t)\n\nPolyInOut easing.\nOptional exponent e defaults to 3.0.\nsource\n\n\n\npoly-out\n\n(poly-out t)\n(poly-out e t)\n\nPolyOut easing.\nOptional exponent e defaults to 3.0.\nsource\n\n\n\nquad-in\n\n(quad-in t)\n\nQuadIn easing\nsource\n\n\n\nquad-in-out\n\n(quad-in-out t)\n\nQuadInOut easing\nsource\n\n\n\nquad-out\n\n(quad-out t)\n\nQuadOut easing\nsource\n\n\n\nreflect\n\n(reflect easing center)\n\nCreate in-out easing for given easing function and center.\nsource\n\n\n\nsin-in\n\n(sin-in t)\n\nSinIn easing\nsource\n\n\n\nsin-in-out\n\n(sin-in-out t)\n\nSinInOut easing\nsource\n\n\n\nsin-out\n\n(sin-out t)\n\nSinOut easing\nsource\n\nsource: clay/easings.clj",
    "crumbs": [
      "Easings"
    ]
  },
  {
    "objectID": "fields.html",
    "href": "fields.html",
    "title": "Vector fields",
    "section": "",
    "text": "Reference",
    "crumbs": [
      "Vector fields"
    ]
  },
  {
    "objectID": "fields.html#reference",
    "href": "fields.html#reference",
    "title": "Vector fields",
    "section": "",
    "text": "fastmath.fields\nVector field functions.\nVector fields are functions R2-&gt;R2.\nNames are taken from fractal flames world where such fields are call variations. Most implementations are taken from JWildfire software.\n### Creation\nTo create vector field call field multimethod with name of the field as keyword.\nSome of the vector fields require additional configuration as a map of parameters as keywords and values. Call parametrization to create random one or to merge with provided.\nAdditionally you can provide amount parameter which is scaling factor for vector field (default: 1.0).\n### Derived fields\nYou can use several method to derive new vector field from the other one(s). Possible options are:\n\nderivative, grad-x, grad-y - directional derivative of the field\nsum - sum of two fields\nmultiply - multiplication of the fields\ncomposition - composition of the fields\nangles - angles of the field vectors\n\n### Scalar fields\nYou can derive scalar fields from given vector field(s):\n\njacobian - determinant of jacobian matrix\ndivergence - divergence of the field\ncross - cross product of the fields (as a determinant of the 2x2 matrix of vectors)\ndot - dot product\nangle-between - angle between vectors from fields.\n\n### Combinations\nThe other option is to create vector field using some of the above possibilities. Combination is a tree of field operations with parametrizations. Functions:\n\ncombine - create vector field randomly of from given parametrization.\nrandom-configuration - returns random configuration as a map\nrandomize-configuration - change parametrization for given configuration.\n\n\n\nskip-random-fields\nWhen random configuration for combine is used. Skip vector fields which are random.\nsource\n\n\n\nangle-between\nAngle between input vector and result of the vector field.\nIn case when two vector fields are given, cross product is taken from result of vector fields.\nResulting value is from range [-PI,PI].\nsource\n\n\n\ncombine\n\n(combine {:keys [type name amount config var step var1 var2]})\n(combine)\n\nCreate composite vector field function based on configuration\nCall without argument to get random vector field.\nConfiguration is a tree structure where nodes are one of the following\n\n{:type :variation :name NAME :amount AMOUNT :config CONFIG} where\n\nNAME is variation name (keyword)\nAMOUNT is scaling factor\nCONFIG is variation parametrization\n\n{:type :operation :name OPERATION :amount AMOUNT :var1 VAR1 :var2 VAR2} where\n\nOPERATION is one of the operations (see below)\nAMOUNT is scaling factor\nVAR1 and VAR2 two variations to combine\n\n{:type :operation :name :derivative :amount AMOUNT :var VAR :step STEP} where\n\nAMOUNT is scaling factor\nVAR variation, subject to calculate derivative\nSTEP dx and dy value\n\n\nPossible OPERATIONs are:\n\n:add - sum of two variations\n:mult - multiplication\n:comp - composition\n:angles - vector field from angles\n\nSee random-configuration for example.\nsource\n\n\n\ncomposition\n\n(composition f1 f2 amount)\n(composition f1 f2)\n\nCompose two vector fields.\nsource\n\n\n\ncross\n2d cross product (det of the 2x2 matrix) of the input vector and result of the vector field.\nIn case when two vector fields are given, cross product is taken from results of vector fields.\nsource\n\n\n\ncurl\n\n(curl f)\n(curl f h)\n\nCurl (2d version) of the field.\nSee: https://youtu.be/rB83DpBJQsE?t=855\nsource\n\n\n\nderivative\n\n(derivative f amount h)\n(derivative f h)\n(derivative f)\n\nCalculate directional derivative of fn. Derivative is calculated along [1,1] vector with h as a step (default 1.0e-6).\nsource\n\n\n\ndivergence\n\n(divergence f)\n(divergence f h)\n\nDivergence of the field.\nSee: https://youtu.be/rB83DpBJQsE?t=855\nsource\n\n\n\ndot\nDot product of the input vector and result of the vector field.\nIn case when two vector fields are given, cross product is taken from result of vector fields.\nsource\n\n\n\nfield\nReturn vector field for given name and options: amount (scaling factor) and parametrization.\nDefault scaling factor is 1.0, default parametrization is random.\nResulting function operates on Vec2 type.\nsource\n\n\n\nfields-list\nsource\n\n\n\nfields-list-not-random\nsource\n\n\n\nfields-list-random\nsource\n\n\n\nfields-map\nsource\n\n\n\ngrad-x\n\n(grad-x f amount h)\n(grad-x f h)\n(grad-x f)\n\nCalculate gradient along x axis.\nsource\n\n\n\ngrad-y\n\n(grad-y f amount h)\n(grad-y f h)\n(grad-y f)\n\nCalculate gradient along y axis.\nsource\n\n\n\nheading\n\n(heading f)\n\nAngle of the vectors from field.\nsource\n\n\n\njacobian\n\n(jacobian f)\n(jacobian f h)\n\nDet of Jacobian of the field\nsource\n\n\n\nmagnitude\n\n(magnitude f)\n\nMagnitude of the vectors from field.\nsource\n\n\n\nmultiplication\n\n(multiplication f1 f2 amount)\n(multiplication f1 f2)\n\nMultiply two vector fields (as a element-wise multiplication of results).\nsource\n\n\n\nparametrization\nReturn random parametrization map for given field.\nOptinally you can pass part of the parametrization. In this case function will add remaining keys with randomly generated values.\nIf field doesn’t have parametrization, empty map will be returned.\nSee field.\nsource\n\n\n\nrandom-configuration\n\n(random-configuration)\n(random-configuration depth)\n(random-configuration depth f)\n\nCreate random configuration for combine function. Optionally with depth (0 = only root is created).\nSee combine for structure.\nBind *skip-random-fields* to true to exclude fields which are random.\nsource\n\n\n\nrandom-field\n\n(random-field)\n(random-field depth)\n\nCreate randomized field (optional depth can be provided).\nsource\n\n\n\nrandomize-configuration\n\n(randomize-configuration f)\n\nRandomize values for given configuration. Keeps structure untouched.\nsource\n\n\n\nscalar-&gt;vector-field\n\n(scalar-&gt;vector-field scalar f)\n(scalar-&gt;vector-field scalar f1 f2)\n\nReturns vector field build from scalar fields of the input vector and result of the vector field.\nsource\n\n\n\nsum\n\n(sum f1 f2 amount)\n(sum f1 f2)\n\nAdd two vector fields.\nsource\n\nsource: clay/fields.clj",
    "crumbs": [
      "Vector fields"
    ]
  },
  {
    "objectID": "efloat.html",
    "href": "efloat.html",
    "title": "EFloat",
    "section": "",
    "text": "Reference",
    "crumbs": [
      "EFloat"
    ]
  },
  {
    "objectID": "efloat.html#reference",
    "href": "efloat.html#reference",
    "title": "EFloat",
    "section": "",
    "text": "fastmath.efloat\n(re)Implementation of EFloat/Interval from pbrt-v3/pbrt-v4.\nA floating point number structure which keeps a track of error caused by operations.\n\n\n-&gt;EFloat\n\n(-&gt;EFloat v low high)\n\nPositional factory function for class fastmath.efloat.EFloat.\nsource\n\n\n\n-&gt;Pair\n\n(-&gt;Pair a b)\n\nPositional factory function for class fastmath.efloat.Pair.\nsource\n\n\n\n-&gt;double\n\n(-&gt;double ev)\n\nsource\n\n\n\nabs\n\n(abs ev)\n\nsource\n\n\n\nabsolute-error\n\n(absolute-error ev)\n\nsource\n\n\n\nacos\n\n(acos ev)\n\nsource\n\n\n\nadd\n\n(add ev1 ev2)\n\nsource\n\n\n\naddf\n\n(addf ev v)\n\nsource\n\n\n\nceil\n\n(ceil ev)\n\nsource\n\n\n\ncos\n\n(cos ev)\n\nsource\n\n\n\ndifference-of-products\n\n(difference-of-products a b c d)\n\nsource\n\n\n\ndiv\n\n(div ev1 ev2)\n\nsource\n\n\n\ndivf\n\n(divf ev v)\n\nsource\n\n\n\nefloat\n\n(efloat v)\n(efloat v err)\n(efloat v low high)\n\nCreate EFloat object from a single value or low and high values.\nsource\n\n\n\nequals?\n\n(equals? ev v)\n\nsource\n\n\n\nfloor\n\n(floor ev)\n\nsource\n\n\n\nfma\n\n(fma ev1 ev2 ev3)\n\nsource\n\n\n\nin-range?\n\n(in-range? ev v)\n\nsource\n\n\n\nlower-bound\n\n(lower-bound ev)\n\nsource\n\n\n\nmap-&gt;EFloat\n\n(map-&gt;EFloat m__7997__auto__)\n\nFactory function for class fastmath.efloat.EFloat, taking a map of keywords to field values.\nsource\n\n\n\nmap-&gt;Pair\n\n(map-&gt;Pair m__7997__auto__)\n\nFactory function for class fastmath.efloat.Pair, taking a map of keywords to field values.\nsource\n\n\n\nmax\n\n(max ev1 ev2)\n\nsource\n\n\n\nmid-point\n\n(mid-point ev)\n\nsource\n\n\n\nmin\n\n(min ev1 ev2)\n\nsource\n\n\n\nmul\n\n(mul ev1 ev2)\n\nsource\n\n\n\nmulf\n\n(mulf ev v)\n\nsource\n\n\n\nneg\n\n(neg ev)\n\nsource\n\n\n\nquadratic\n\n(quadratic a b c)\n\nsource\n\n\n\nrelative-error\n\n(relative-error ev)\n\nsource\n\n\n\nsin\n\n(sin ev)\n\nsource\n\n\n\nsq\n\n(sq ev)\n\nsource\n\n\n\nsqrt\n\n(sqrt ev)\n\nsource\n\n\n\nsub\n\n(sub ev1 ev2)\n\nsource\n\n\n\nsubf\n\n(subf ev v)\n\nsource\n\n\n\nsum-of-products\n\n(sum-of-products a b c d)\n\nsource\n\n\n\nupper-bound\n\n(upper-bound ev)\n\nsource\n\n\n\nwidth\n\n(width ev)\n\nsource\n\nsource: clay/efloat.clj",
    "crumbs": [
      "EFloat"
    ]
  }
]